[
  {
    "objectID": "questions/R1-quiz8932.html",
    "href": "questions/R1-quiz8932.html",
    "title": "Rossling Chapter 1 Quiz",
    "section": "",
    "text": "Which answer best represents the misconception Rosling calls “The Gap Instinct?”\n\n\n\n the misconception that “the world is divided in two”\n\n\n the misconception that the knowledge gap between experts and non-experts is large\n\n\n the misconception that the education gap between males and females is large\n\n\n the misconception that the gaps in our own personal knowledge are large\n\n\n\nquestion id: R1-quiz-1\n\n\n\nWhat is the role of the chimpanzee in Rosling’s Intro & Chapter 1?\n\n\n\n It is an example of a species whose population is usually underestimated.\n\n\n It is an example of a “random guesser” to which human responses might be compared.\n\n\n It represents the baseline to which evolutionary improvement might be compared.\n\n\n It represents a population negatively affected by climate change.\n\n\n\nquestion id: R1-quiz-2\n\n\n\nThe Rosling picture below refers to what year?\n\n\n 1965        1980        1995        2010       \n\nquestion id: R1-quiz-3\n\n\n\n\n\nFigure from Rosling Ch 1\n\n\n\nR1-quiz8932.rmarkdown Collect your answers\n\nNo answers yet collected\n\n\n\n\n\n\n\nSubmit collected answers here"
  },
  {
    "objectID": "questions/R3-before-AK.html",
    "href": "questions/R3-before-AK.html",
    "title": "Rosling Chapter 3: The Straight-line Instinct",
    "section": "",
    "text": "Question 1:\n\n\n\nWhat is meant by the “straight line instinct” in Rosling? Explain in simple words.\n  question id: R03-QB6 \n\n\n\n\n\n\n\n\nQuestion 2:\n\n\n\nCan you come up with a concrete example of how this cognitive bias can be detrimental?\n  question id: R03-QB7 \n\n\n\n\n\n\n\n\nQuestion 3:\n\n\n\nGiven the “straight line instinct” leads to wrong conclusions, how would you balance it with the fact that we use linear regression?\n  question id: R03-QB8 \n\n\n\nR3-before-AK.rmarkdown Collect your answers\n\nNo answers yet collected\n\n\n\n\n\n\n\nSubmit collected answers here"
  },
  {
    "objectID": "questions/R1-before.html",
    "href": "questions/R1-before.html",
    "title": "Rossling Chapter 1: The Gap Instinct",
    "section": "",
    "text": "Question 1:\n\n\n\nWhat does Rosling mean by “the west and the rest.”Name a historian on campus who wrote a book subtitled, “The West and the Rest.” That book is about the most recent 500 years of global history. What period of time is encompass by the positive changes Rosling writes about?\n  question id: S01B1-1 \n\n\n\n\n\n\n\n\nQuestion 2:\n\n\n\nExplain what Rosling means when he says that “averages mislead.” If you had a high-school stats course, did they emphasize this?\n  question id: R01B2 \n\n\n\n\n\n\n\n\nQuestion 3:\n\n\n\nLook at the horizontal scale of the bottom figure on p. 43. Are the labeled values equally spaced on paper? Are they equally spaced in value? What is this kind of scale called?\n  question id: R01B3 \n\n\n\n\n\n\n\n\nQuestion 4:\n\n\n\nWhich of Rosling’s bullet points on p. 46 has something to say about an object of political debate called “the 1 percent?” In Brazil (to judge from the p.43 graph), how does the income of the top 1% compare proportionately to the “typical” income of Brazilians?\n  question id: R01B4 \n\n\n\n\n\n\n\n\nQuestion 5:\n\n\n\nWhat would happen if everyone in Category 4 gave $3/day to the people in Category 1?\n  question id: R01B5 \n\n\n\n\n\n\n\n\nQuestion 6:\n\n\n\nRossling uses four aspects of daily to illustrate the different income Categories 1-4:\n\nWater\nTransport\nCooking\nPlate of food\n\nWhat would, in your opinion, be a plausible and meaningful way of measuring quantitatively the level of each of these aspects?\n  question id: R01B6 \n\n\n\nR1-before.rmarkdown Collect your answers\n\nNo answers yet collected\n\n\n\n\n\n\n\nSubmit collected answers here"
  },
  {
    "objectID": "questions/S6-after.html",
    "href": "questions/S6-after.html",
    "title": "Spiegelhalter Chapter 6",
    "section": "",
    "text": "Question 1:\n\n\n\nAs best you can, explain why a sensitivity of 100% can always be achieved at the cost of lowering specificity to 0, and a specificity of 100% can always be had in exchange for a sensitivity of 0.\n  question id: S06-QA1 \n\n\n\n\n\n\n\n\nQuestion 2:\n\n\n\nWhat is overfitting? Explain what the division of historical data (that is, data with the outcome already recorded) into training and testing datasets can help avoid overfitting.\n  question id: S06-QA2-overfitting \n\n\n\n\n\n\n\n\nQuestion 3:\n\n\n\nSpiegelhalter characterizes the classification tree in Fig 6.6 (“Over-fitted classification tree for the Titanic data”) as having a “rather strange set of questions.” What do you think motivates this? (Hint: Look at the role of “fare” in determining the predicted outcome.)\n  question id: S06-QA3 \n\n\n\nS6-after.rmarkdown Collect your answers\n\nNo answers yet collected\n\n\n\n\n\n\n\nSubmit collected answers here"
  },
  {
    "objectID": "questions/S8-before.html",
    "href": "questions/S8-before.html",
    "title": "Spiegelhalter Chapter 8",
    "section": "",
    "text": "Question 1:\n\n\n\nList Spiegelhalter’s four “simple” probability rules.\n  question id: S08-QB1-prob-rules \n\n\n\n\n\n\n\n\nQuestion 2:\n\n\n\nWhich of the characterizations of probability on pp. 217-218 most closely corresponds to how you think about probability?\n  question id: S08-QB2-prob-thinking \n\n\n\n\n\n\n\n\nQuestion 3:\n\n\n\nWhat is the “prosecutor’s fallacy?”\n  question id: S08-QB3-prosecutor \n\n\n\n\n\n\n\n\nQuestion 4:\n\n\n\nGive an example of a relationship modeled by the Poisson distribution in this chapter.\n  question id: S08-QB4-poisson \n\n\n\n\n\n\n\n\nQuestion 5:\n\n\n\nBONUS. Consider Game 3 (similar to Game 1 and Game 2 in the reading): Throw 3 fair dice at most 144 times, and win if you get triple 6. Is this game a better or worse bet than Game 2? (If you’d like a hint, see endnote 1 to the chapter).\n  question id: S08-QB5-bonus \n\n\n\nS8-before.rmarkdown Collect your answers\n\nNo answers yet collected\n\n\n\n\n\n\n\nSubmit collected answers here"
  },
  {
    "objectID": "questions/S6-before.html",
    "href": "questions/S6-before.html",
    "title": "Spiegelhalter Chapter 6",
    "section": "",
    "text": "This is a challenging but important chapter. The techniques Spiegelhalter mentions (“random forests,” “logistic regression,” “support vector machines”, among others) seem like wizards’ incantations. The book was published in 2019. Even in the few years since then, other techniques have found their way into popular culture: —e.g. “deep learning,” “generative pre-trained transformers” (GPT).\nSpiegelhalter chooses to exemplify all this very recent, ongoing development with one of the very oldest methods: “classification trees” from the 1960s. This is a good choice in many ways. It requires little training to calculate the output from a classification tree given the explanatory variable inputs. And, although Spiegelhalter doesn’t show how such trees are created, the process is readily understandable if presented step by step.\nThe chapter title is “Algorithms, Analytics, and Prediction.” Let’s take some of the mystery out of “algorithm” and “analytics.” To analyze means simply “taking apart.” It is the opposite of synthesize: “putting together.” The “classification tree” is a matter to taking a data frame apart into separate sets of rows, then aggregating each set to make a prediction (e.g. a 16% change of survival for the Misters)\nAn “algorithm” is a recipe or set of instructions for a procedure. Spiegelhalter doesn’t go over any algorithms; he shows the results produced by the particular algorithm for generating classification trees.\n\n\n\n\n\n\nQuestion 1:\n\n\n\nGenome-wide association studies involve measuring many (say, 500,000) features of a person’s DNA. By comparing this huge number of features across dozens of people, some with and others without a disease, researchers try to figure out which genes might be involved in the disease. Put this in the context of \\(p\\) and \\(n\\) in the paragraphs on p. 145-145. (locator phrase: “Data can also be ‘big’”)\n  question id: S06-QB1 \n\n\n\n\n\n\n\n\nQuestion 2:\n\n\n\nWhat is a confusion matrix?\n  question id: S06-QB2 \n\n\n\n\n\n\n\n\nQuestion 3:\n\n\n\nWhen it comes to prediction of a binary outcome variable, what are sensitivity and specificity?\n  question id: S06-QB3 \n\n\n\nS6-before.rmarkdown Collect your answers\n\nNo answers yet collected\n\n\n\n\n\n\n\nSubmit collected answers here"
  },
  {
    "objectID": "questions/R2-before.html",
    "href": "questions/R2-before.html",
    "title": "Rossling Chapter 2: The Negativity Instinct",
    "section": "",
    "text": "Question 1:\n\n\n\nWhat does Rosling mean about distinguishing between “better” and “bad?” Did you know that there is a component of the standard high-school curriculum that provides a framework for dealing with such distinctions? What do you think it is called?\n  question id: R02B1 \n\n\n\n\n\n\n\n\nQuestion 2:\n\n\n\nWhat is “selective reporting?”\n  question id: R02B2 \n\n\n\n\n\n\n\n\nQuestion 3:\n\n\n\nWhat is the problem Rosling identifies as “feeling, no thinking?”\n  question id: R02B3 \n\n\n\nR2-before.rmarkdown Collect your answers\n\nNo answers yet collected\n\n\n\n\n\n\n\nSubmit collected answers here"
  },
  {
    "objectID": "questions/S1-quiz-ysle.html",
    "href": "questions/S1-quiz-ysle.html",
    "title": "Spiegelhalter Chapter 1 Quiz",
    "section": "",
    "text": "Choose the best answer: Categorical variables are measures that …\n\n\n\n are unambiguously explicit and direct.\n\n\n are employed in the study of category theory in mathematics.\n\n\n can be categorized as continuous or discrete.\n\n\n can take on two or more categories.\n\n\n\nquestion id: S1-quiz-1\n\n\n\nSpiegelhalter shows a 3D pie chart as an example of presenting categorical data. His purpose for showing this example is to demonstrate that …\n\n\n\n 3D pie charts generally ought not be used as they distort areas.\n\n\n 3D pie charts generally ought to be used as they are straightforward to interpret.\n\n\n 3D plots are generally less informative than 2D plots.\n\n\n 3D plots are generally more informative than 2D plots.\n\n\n\nquestion id: S1-quiz-2\n\n\n\nSpiegelhalter refers to a study about risks of taking statins, and poses the question, “How can a rise from 85% to 87% be called a ______% increase?” What number was in the blank in the reading?\n\n\n 1        10        20        40       \n\nquestion id: S1-quiz-3\n\n\n\nS1-quiz-ysle.rmarkdown Collect your answers\n\nNo answers yet collected\n\n\n\n\n\n\n\nSubmit collected answers here"
  },
  {
    "objectID": "questions/R5-before.html",
    "href": "questions/R5-before.html",
    "title": "Rossling Chapter 5: The Size Instinct",
    "section": "",
    "text": "Question 1:\n\n\n\nBriefly: What is the “size instinct” as described by Rosling in this chapter?\n  question id: R05-QB1-size-instinct \n\n\n\n\n\n\n\n\nQuestion 2:\n\n\n\nAccording to Rosling, what is a key way to avoid being misled by large numbers?\n  question id: R05-QB2-avoid-misled \n\n\n\n\n\n\n\n\nQuestion 3:\n\n\n\nWhat is Rosling’s “PIN code” for the world, and what does it mean?\n  question id: R05-QB3-PIN \n\n\n\n\n\n\n\n\nQuestion 4:\n\n\n\nProvide three examples for which a per capita measurement is more meaningful than an absolute measurement.\n  question id: R05-QB4-per-capita \n\n\n\nR5-before.rmarkdown Collect your answers\n\nNo answers yet collected\n\n\n\n\n\n\n\nSubmit collected answers here"
  },
  {
    "objectID": "questions/S1-before.html",
    "href": "questions/S1-before.html",
    "title": "Spiegelhalter Chapter 1",
    "section": "",
    "text": "Question 1:\n\n\n\nWhat is binary data?\n  question id: S01-B1 \n\n\n\n\n\n\n\n\nQuestion 2:\n\n\n\nThere are at least two examples in the chapter of ways to make graphics more informative. What are they?\n  question id: S01-B2 \n\n\n\nS1-before.rmarkdown Collect your answers\n\nNo answers yet collected\n\n\n\n\n\n\n\nSubmit collected answers here"
  },
  {
    "objectID": "demonstrations/Gender-neutral-names.html",
    "href": "demonstrations/Gender-neutral-names.html",
    "title": "Gender-neutral names",
    "section": "",
    "text": "NOTE IN DRAFT\n\n\n\nPoint out that this will be just a demonstration. Use babynames as an inclass example of wrangling?\n\nHow can we figure out what are the most gender neutral names.\nfilter and summarize babynames: proportion of males and females with each name, select out those where the ratio is 0.25 to 0.75. Calculate the information (p ln(p))"
  },
  {
    "objectID": "Kaplan_posts.html",
    "href": "Kaplan_posts.html",
    "title": "Class notes and posts for Prof. Kaplan’s section",
    "section": "",
    "text": "Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\nDate\n\n\nTitle\n\n\nAuthor\n\n\n\n\n\n\nMar 20, 2025\n\n\nKaplan’s in-class notes for 2025-03-20\n\n\n \n\n\n\n\nMar 18, 2025\n\n\nKaplan’s in-class notes for 2025-03-18\n\n\n \n\n\n\n\nMar 14, 2025\n\n\nKaplan’s in-class notes for 2025-03-14\n\n\n \n\n\n\n\nMar 11, 2025\n\n\nKaplan’s in-class notes for 2025-03-11\n\n\n \n\n\n\n\nMar 7, 2025\n\n\nKaplan’s in-class notes for 2025-03-07\n\n\n \n\n\n\n\nMar 4, 2025\n\n\nKaplan’s in-class notes for 2025-03-04\n\n\n \n\n\n\n\nFeb 28, 2025\n\n\nKaplan’s in-class notes for 2025-02-28\n\n\n \n\n\n\n\nFeb 25, 2025\n\n\nKaplan’s in-class notes for 2025-02-25\n\n\n \n\n\n\n\nFeb 21, 2025\n\n\nKaplan’s in-class notes for 2025-02-21\n\n\n \n\n\n\n\nFeb 18, 2025\n\n\nKaplan’s in-class notes for 2025-02-18\n\n\n \n\n\n\n\nFeb 7, 2025\n\n\nKaplan’s in-class notes for 2025-02-07\n\n\n \n\n\n\n\nFeb 4, 2025\n\n\nKaplan’s in-class notes for 2025-02-04\n\n\n \n\n\n\n\nJan 31, 2025\n\n\nKaplan’s in-class notes for 2025-01-31\n\n\n \n\n\n\n\nJan 28, 2025\n\n\nKaplan’s in-class notes for 2025-01-28\n\n\n \n\n\n\n\nJan 24, 2025\n\n\nKaplan’s in-class notes for 2025-01-24\n\n\n \n\n\n\n\nJan 21, 2025\n\n\nKaplan’s in-class notes for 2025-01-21\n\n\n \n\n\n\n\nJan 17, 2025\n\n\nKaplan’s in-class notes for 2025-01-17\n\n\n \n\n\n\n\nJan 14, 2025\n\n\nKaplan’s in-class notes for 2025-01-14\n\n\n \n\n\n\n\nJan 10, 2025\n\n\nKaplan’s in-class notes for 2025-01-10\n\n\n \n\n\n\n\nJan 7, 2025\n\n\nKaplan’s notes for 2025-01-07\n\n\n \n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "day-by-day/Ruth-schedule.html",
    "href": "day-by-day/Ruth-schedule.html",
    "title": "QR2 Prof. Ruth’s Section",
    "section": "",
    "text": "3. Monday 13 January, 2025\n\n\n\nBlock 1 (cont.)\n\nNew readings:\n\nComputing tutorial 1. Read before class session.\nRosling Chap 2 questions\n\nAssignments due before class time:\n\nRemember that the “New readings” section has links to the relevant reading questions.\nFor the computing tutorials, there are no “questions” as such, but be sure to run and, better, play around with the Active R Chunks. Then go through the familiar “Collect answers” and submission process. Your “answers” will be the contents of the Active R Chunks that you have run.\nThese principles apply to all class sessions, but won’t be restated explicitly in other day’s itemizations.\n\nDiscussions and Activities\n\nRossling reading\nReview of Computing Tutorial 1\nIn-class group work on computing activities (TBA)"
  },
  {
    "objectID": "day-by-day/Ruth-schedule.html#past-class-sessions",
    "href": "day-by-day/Ruth-schedule.html#past-class-sessions",
    "title": "QR2 Prof. Ruth’s Section",
    "section": "Past class sessions",
    "text": "Past class sessions\n\n\n\n\n\n\n1. Monday 6 January, 2025\n\n\n\nOrientation to QR2\n\nReadings:\n\nSpiegelhalter Introduction\nRosling Introduction\nNote: As a rule, you should have completed a first pass of the day’s readings before the class meets. This being the first day, I can only ask to to try to do so.\n\nDiscussion:\n\nLearning about data and statistics\nSpiegelhalter Introduction\nRosling Introduction\nNote: The items under the “Discussion” header in this document are just FYI. The readings and the reading questions are the appropriate preparation.\n\n\n\n\n\n\n\n\n\n\n2. Thursday 9 January\n\n\n\nBlock 1: Data, visualization, trends\n\nNew readings:\n\nSpiegelhalter Chap 1 questions\nRosling Chap 1 questions\n\nAssignments due before class time:\n\nBefore-class reading questions as linked above.\nSpiegelhalter introduction follow-up reading questions\n\nDiscussions:\n\nSorting out any start-up problems\nSpiegelhalter and Rosling readings\nIn-class orientation to computing\n\n\n\n\n\n\n\n\n\n\n6. Thursday 23 January\n\n\n\n\n\n\n\n\n\n\n\n\n7. Monday 27 January, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n8. Thursday 30 January\n\n\n\n\n\n\n\n\n\n\n\n\n9. Monday 3 February\n\n\n\n\n\n\n\n\n\n\n\n\n10. Thursday 6 February\n\n\n\n\n\n\nMid-term break\n\n\n\n\n\n\n11. Monday 10 February\n\n\n\n\n\n\n\n\n\n\n\n\n12. Thursday 13 February\n\n\n\n\n\n\n\n\n\n\n\n\n13. Monday 17 February\n\n\n\n\n\n\n\n\n\n\n\n\n14. Thursday 20 February\n\n\n\n\n\n\n\n\n\n\n\n\n15. Monday 24 February\n\n\n\n\n\n\n\n\n\n\n\n\n16. Thursday 27 February\n\n\n\n\n\n\n\n\n\n\n\n\n17. Monday 3 March\n\n\n\n\n\n\n\n\n\n\n\n\n18. Thursday 6 March\n\n\n\n\n\n\n\n\n\n\n\n\n19. Monday 11 March\n\n\n\n\n\n\n\n\n\n\n\n\n20. Thursday 13 March"
  },
  {
    "objectID": "day-by-day/Ruth-schedule.html#already-past",
    "href": "day-by-day/Ruth-schedule.html#already-past",
    "title": "QR2 Prof. Ruth’s Section",
    "section": "Already past",
    "text": "Already past\nSchedule items will be moved here (in reverse chronological order) as an archive."
  },
  {
    "objectID": "day-by-day/Overbey-schedule.html",
    "href": "day-by-day/Overbey-schedule.html",
    "title": "QR2 Prof. Overbey’s Section",
    "section": "",
    "text": "1. Monday 6 January, 2025\n\n\n\nOrientation to QR2\n\nReadings:\n\nSpiegelhalter Introduction\nRosling Introduction\nNote: As a rule, you should have completed a first pass of the day’s readings before the class meets. This being the first day, I can only ask to to try to do so.\n\nDiscussion:\n\nSyllabus Review (syllabus in Populi).\nRosling Introduction\nSpiegelhalter Introduction\nNote: The items under the “Discussion” header in this document are just FYI. The readings and the reading questions are the appropriate preparation.\n\n\n\n\n\n\n\n\n\n\n2. Tuesday 7 January\n\n\n\nBlock 1: Data, visualization, trends\n\nNew readings:\n\nSpiegelhalter Chap 1 questions\nRosling Chap 1 questions\n\nAssignments due before class time:\n\nBefore-class reading questions as linked above.\nSpiegelhalter introduction follow-up reading questions\n\nDiscussions:\n\nSorting out any start-up problems\nSpiegelhalter and Rosling readings\nIn-class orientation to computing"
  },
  {
    "objectID": "day-by-day/Overbey-schedule.html#near-future",
    "href": "day-by-day/Overbey-schedule.html#near-future",
    "title": "QR2 Prof. Overbey’s Section",
    "section": "",
    "text": "1. Monday 6 January, 2025\n\n\n\nOrientation to QR2\n\nReadings:\n\nSpiegelhalter Introduction\nRosling Introduction\nNote: As a rule, you should have completed a first pass of the day’s readings before the class meets. This being the first day, I can only ask to to try to do so.\n\nDiscussion:\n\nSyllabus Review (syllabus in Populi).\nRosling Introduction\nSpiegelhalter Introduction\nNote: The items under the “Discussion” header in this document are just FYI. The readings and the reading questions are the appropriate preparation.\n\n\n\n\n\n\n\n\n\n\n2. Tuesday 7 January\n\n\n\nBlock 1: Data, visualization, trends\n\nNew readings:\n\nSpiegelhalter Chap 1 questions\nRosling Chap 1 questions\n\nAssignments due before class time:\n\nBefore-class reading questions as linked above.\nSpiegelhalter introduction follow-up reading questions\n\nDiscussions:\n\nSorting out any start-up problems\nSpiegelhalter and Rosling readings\nIn-class orientation to computing"
  },
  {
    "objectID": "day-by-day/Overbey-schedule.html#further-out-i.e.-tentative",
    "href": "day-by-day/Overbey-schedule.html#further-out-i.e.-tentative",
    "title": "QR2 Prof. Overbey’s Section",
    "section": "Further out (i.e. tentative)",
    "text": "Further out (i.e. tentative)\n\n\n\n\n\n\n3. Monday 13 January, 2025\n\n\n\nBlock 1 (cont.)\n\nNew readings:\n\nComputing tutorial 1. Read before class session.\nRosling Chap 2 questions\n\nAssignments due before class time:\n\nRemember that the “New readings” section has links to the relevant reading questions.\nFor the computing tutorials, there are no “questions” as such, but be sure to run and, better, play around with the Active R Chunks. Then go through the familiar “Collect answers” and submission process. Your “answers” will be the contents of the Active R Chunks that you have run.\nThese principles apply to all class sessions, but won’t be restated explicitly in other day’s itemizations.\n\nDiscussions and Activities\n\nRossling reading\nReview of Computing Tutorial 1\nIn-class group work on computing activities (TBA)\n\n\n\n\n\n\n\n\n\n\n4. Tuesday 16 January\n\n\n\nBlock 2: Prediction\n\nNew readings\n\nfirst\nsecond\n\n\n\n\n\n\n\n\n\n\n5. Monday 20 January, 2025\n\n\n\nNew Readings\nAssignments due before class time:\n\nComputing Tutorial 1\n\n\n\n\n\n\n\n\n\n6. Tuesday 21 January\n\n\n\n\n\n\n\n\n\n\n\n\n7. Monday 27 January, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n8. Tuesday 28 January\n\n\n\n\n\n\n\n\n\n\n\n\n9. Monday 3 February\n\n\n\n\n\n\n\n\n\n\n\n\n10. Tuesday 4 February\n\n\n\n\n\n\nMid-term break\n\n\n\n\n\n\n11. Monday 10 February\n\n\n\n\n\n\n\n\n\n\n\n\n12. Tuesday 11 February\n\n\n\n\n\n\n\n\n\n\n\n\n13. Monday 17 February\n\n\n\n\n\n\n\n\n\n\n\n\n14. Tuesday 18 February\n\n\n\n\n\n\n\n\n\n\n\n\n15. Monday 24 February\n\n\n\n\n\n\n\n\n\n\n\n\n16. Tuesday 25 February\n\n\n\n\n\n\n\n\n\n\n\n\n17. Monday 3 March\n\n\n\n\n\n\n\n\n\n\n\n\n18. Tuesday 4 March\n\n\n\n\n\n\n\n\n\n\n\n\n19. Monday 11 March\n\n\n\n\n\n\n\n\n\n\n\n\n20. Tuesday 12 March"
  },
  {
    "objectID": "day-by-day/Overbey-schedule.html#already-past",
    "href": "day-by-day/Overbey-schedule.html#already-past",
    "title": "QR2 Prof. Overbey’s Section",
    "section": "Already past",
    "text": "Already past\nSchedule items will be moved here (in reverse chronological order) as an archive."
  },
  {
    "objectID": "day-by-day/Kolpakov-schedule.html",
    "href": "day-by-day/Kolpakov-schedule.html",
    "title": "QR2 Prof. Kolpakov’s Section",
    "section": "",
    "text": "9. Monday 3 February\n\n\n\n\nWe shall discuss confidence intervals (see notes on Populi)\nWe shall talk about regression coefficients Computing tutorial 4. Read before class session.\n\n\n\n\n\n\n\n\n\n10. Thursday 6 February\n\n\n\n\nWe shall start some Course Project preparations: accessing the dataset, merging its parts together, trying to get some basic information from it (over the reading break)\nAssignments due:\n\nComputing Activities for Tutorial 3"
  },
  {
    "objectID": "day-by-day/Kolpakov-schedule.html#near-future",
    "href": "day-by-day/Kolpakov-schedule.html#near-future",
    "title": "QR2 Prof. Kolpakov’s Section",
    "section": "",
    "text": "9. Monday 3 February\n\n\n\n\nWe shall discuss confidence intervals (see notes on Populi)\nWe shall talk about regression coefficients Computing tutorial 4. Read before class session.\n\n\n\n\n\n\n\n\n\n10. Thursday 6 February\n\n\n\n\nWe shall start some Course Project preparations: accessing the dataset, merging its parts together, trying to get some basic information from it (over the reading break)\nAssignments due:\n\nComputing Activities for Tutorial 3"
  },
  {
    "objectID": "day-by-day/Kolpakov-schedule.html#further-out-i.e.-tentative",
    "href": "day-by-day/Kolpakov-schedule.html#further-out-i.e.-tentative",
    "title": "QR2 Prof. Kolpakov’s Section",
    "section": "Further out (i.e. tentative)",
    "text": "Further out (i.e. tentative)\n\n\n\n\n\n\n11. Monday 10 February\n\n\n\nReading break (no class)\n\n\n\n\n\n\n\n\n12. Thursday 13 February\n\n\n\nReading break (no class)\n\n\n\n\n\n\n\n\nMonday 17 February\n\n\n\nPresident’s Day (will hold class on Wednesday, February 19)\n\n\n\n\n\n\n\n\n13. Wednesday 19 February\n\n\n\n\n\n\n\n\n\n\n\n\n14. Thursday 20 February\n\n\n\n\n\n\n\n\n\n\n\n\n15. Monday 24 February\n\n\n\n\n\n\n\n\n\n\n\n\n16. Thursday 27 February\n\n\n\n\n\n\n\n\n\n\n\n\n17. Monday 3 March\n\n\n\n\n\n\n\n\n\n\n\n\n18. Thursday 6 March\n\n\n\n\n\n\n\n\n\n\n\n\n19. Monday 11 March\n\n\n\n\n\n\n\n\n\n\n\n\n20. Thursday 13 March\n\n\n\nFinal project presentation"
  },
  {
    "objectID": "day-by-day/Kolpakov-schedule.html#already-past",
    "href": "day-by-day/Kolpakov-schedule.html#already-past",
    "title": "QR2 Prof. Kolpakov’s Section",
    "section": "Already past",
    "text": "Already past\n\n\n\n\n\n\n8. Thursday 30 January\n\n\n\nBlock 3 Regression & Adjustment (continued)\n\nNew readings\n\nStart Computing tutorial 4. Read before class session.\n\n\n\n\n\n\n\n\n\n\n7. Monday 27 January, 2025\n\n\n\nBlock 3: Regression & Adjustment\n\nNew readings\n\nSpiegelhalter Chap 5 & reading questions\nRosling Chap 3 & reading questions\nStart Computing tutorial 3. Read before class session.\n\n\n\n\n\n\n\n\n\n\n6. Thursday 23 January\n\n\n\nBlock 2: Prediction (continued)\n\nNew readings:\n\nQuantifying Uncertainty of Populi (Sections 3 and 4)\n\nAssignments due:\n\nPlease be sure to complete Computing Activities for Tutorial 2.\n\n\n\n\n\n\n\n\n\n\n5. Wednesday 22 January, 2025\n\n\n\nBlock 2: Prediction (continued)\n\nNew readings:\n\nQuantifying Uncertainty of Populi (Sections 1 and 2)\nStart Computing tutorial 2. Read before class session.\n\n\n\n\n\n\n\n\n\n\nMonday 20 January, 2025\n\n\n\nMLK Observance (will hold class on Wednesday, January 22)\n\n\n\n\n\n\n\n\n4. Thursday 16 January\n\n\n\nBlock 2: Prediction\n\nNew readings\n\nRosling Chap 4 & reading questions. We’re skipping Rosling 3, saving that for another day.\nSpiegelhalter Chap 3 & reading questions\n\nAssignments due:\n\nPlease be sure to complete Computing Activities for Tutorial 1.\n\n\n\n\n\n\n\n\n\n\n3. Monday 13 January, 2025\n\n\n\nBlock 1 (continued)\n\nNew readings:\n\nComputing tutorial 1. Read before class session.\nRosling Chap 2 questions\n\nAssignments due before class time:\n\nRemember that the “New readings” section has links to the relevant reading questions.\nFor the computing tutorials, there are no “questions” as such, but be sure to run and, better, play around with the Active R Chunks. Then go through the familiar “Collect answers” and submission process. Your “answers” will be the contents of the Active R Chunks that you have run.\nThese principles apply to all class sessions, but won’t be restated explicitly in other day’s itemizations.\n\nDiscussions and Activities\n\nRosling reading\nReview of Computing Tutorial 1\nIn-class group work on computing activities\n\n\n\n\n\n\n\n\n\n\n2. Thursday 9 January\n\n\n\nBlock 1: Data, visualization, trends\n\nNew readings:\n\nSpiegelhalter Chap 1 questions\nRosling Chap 1 questions\n\nAssignments due before class time:\n\nBefore-class reading questions as linked above.\nSpiegelhalter introduction follow-up reading questions\n\nDiscussions:\n\nSorting out any start-up problems\nSpiegelhalter and Rosling readings\nIn-class orientation to computing\n\n\n\n\n\n\n\n\n\n\n1. Monday 6 January, 2025\n\n\n\nOrientation to QR2\n\nReadings:\n\nSpiegelhalter Introduction\nRosling Introduction\nNote: As a rule, you should do preparatory reading before class. For the first days of class, we shall make an exception, and probably discuss a few things on the tangent, too.\n\nDiscussion:\n\nLearning about data and statistics\nSpiegelhalter Introduction\nRosling Introduction\nNote: The items under the “Discussion” header in this document are just FYI. The readings and the reading questions are the appropriate preparation.\n\nComputing:\n\nPlease have your laptops with you! There will be a few in-class computing exercises.\nThe main computing tool for this class is R. You do not need to install it on your computer, although you definitely may."
  },
  {
    "objectID": "projects/Census/intro.html",
    "href": "projects/Census/intro.html",
    "title": "QR 2 Term Project: Census Data",
    "section": "",
    "text": "Note: The dates when the different stages of the project are due will be given by your instructor, and will likely differ somewhat from one instructor to another. Some instructors may use a different course project. Look to your instructor’s syllabus for guidance."
  },
  {
    "objectID": "projects/Census/intro.html#introduction",
    "href": "projects/Census/intro.html#introduction",
    "title": "QR 2 Term Project: Census Data",
    "section": "Introduction",
    "text": "Introduction\nThe US Census is a decenial (that is, every ten years) enumeration of people living in the US and associated territories that is mandated by Article I section 2 of the US Constitution. Census data is published in widely available summary forms, e.g. the population of each state broken down by age and sex groups.\nThis project is about census data on the level of an individual person. For instance, in 2020, the census recorded information from about 330 million persons. This person-by-person data is held confidential for 70 years after the census is conducted. After that, it become available to any interested party. The most recent census with such microdata available is 1950. This project concerns the 1940 census.\nYour work on this project will be conducted in several phases. Instructions for each will be given and a (short) report will be required about your progress after each phase. The first phase, “initial transcription,” has several steps, the goal being to put the data pertaining to one “population schedule” into machine readable form\n\nPhase 1: Initial transcription\n\nSTEP 1:\nYou will select a single document called a “Population Schedule” which gives the data collected on each of roughly 30 to 40 people. The 1940 Census involved about 4 million such schedules, but we will focus on the hundreds from Travis County, TX. A large collection of images of the schedules is available, as well as the codebook for the schedules.\n\nThis site contains an archive of the Travis County population schedules. The archive is organized into a list of about 100 distinct “enumeration districts,” each with a title like “1940 Census Population Schedules - Texas - Travis County - ED 227-84.”\nPick one of the hundred at random.[^NOTE: If you are from Travis County or have a location of interest to you personally such as your family’s or your grandparents’ home, you might want to pick the corresponding population schedule. But avoid landmarks such as the Capitol that many students would be tempted to pick.] One possibility, select a district whose number ends with the day of them month you were born, plus 4 times the month you were born. (For instance, if your birthday is Feb 28, your “random” number is 28 + 4 \\(\\times\\) 2, pointing to ED 227-36.\nNavigate to the ED you picked in (b). For instance, our Feb 28 student will go to ED 227-36 where she will find 38 population schedules. Pick one at random, but try to avoid schedules that don’t have almost all rows filled in.\nDownload the JPG image for your population schedule to your laptop or another convenient location.\n\n\n\nSTEP 2\nRegister your population schedule on this spreadsheet. We will all share this Population Schedule Registrar, each of us being responsible for one row.\n\nThe unit of observation is one population schedule. (Typically covering about 40 people.)\nThere is an example entry on line 2. Follow that pattern when entering your own sheet.\n\nWe’ll find the latitude and longitude at the heart of your schedule later. Following professional practice, enter the latitude and longitude as “NA,” a common signifier for “missing data.”\nThe ED_sheet_count is not about your particular sheet, but about the ED from which you selected your particular sheet. It is the number of sheets (indicated on the ED page) from which you selected one.\nThe person_sheet and household_sheet will be filled in as part of STEP 3.\n\n\n\n\nSTEP 3\n\nCreate TWO google sheet documents where you will enter the data from your population schedule.\n\nOne sheet will have “a person” as the unit of observation.\nThe second sheet will have “a household” as the unit of observation.\n\n\nIf you are not sure how to do this, ask a friend or search online.\n\nArrange via Google sheets to “share” your sheets with anyone with the links. Copy the links into the person_sheet and household_sheet columns in the Population Schedule Register (STEP 2).\n\n\n\nSTEP 4\nFigure out how you can display your population schedule so that you can readily read from it. If you have a large external monitor, you might find that sufficient. If not, consider printing out the schedule at a sufficient resolution that you can read the entries. Unless you have very sharp eyes, or a magnifying glass, you will need more than one page to print the whole image.\nIn STEP 6 you will start to transcribe data from the schedule into your STEP 3 Google spreadsheets. A laptop display may be sufficient for this. Arrange your desk/displays so that you can easily read from the image while typing in the spreadsheet.\n\n\nSTEP 5\nTHINK about what you want to call your variables and how you want to encode them. Feel free to talk to your classmates or even work together in doing this. It’s a good idea to create yet another sharable document that you use as your codebook giving the meaning of the variable names and specifying the levels of any categorical variables and the units of any quantitative ones. (Some of the schedule entries don’t have a ixed set of levels, for instance, the person’s name. You don’t have to specify those in your codebook, but you might want to set the format, e.g., “surname, first middle, with CAPITALS as written on the schedule.”)\nThe “person” and “household” spreadsheets will presumably have different variables. Also, since every person is a member of a “household” (even if it is of size 1), you will add a household ID for each household and enter the corresponding ID as a variable in the person spreadsheet.\n\n\nSTEP 6\nTranscribe your schedule into your person and household spreadsheets. Add a new variable to each of these spreadsheets: the ID of the population schedule from which you transcribed the data. This will be the same for all rows (that is, no variation). In Phase 2 you will see why this has been added.\nIn addition, you will figure out an reasonable latitude and longitude for your population schedule and enter this into our shared Population Schedule Registry\n\n\n\nPhase 2: Combining and Cleaning\nIn this phase, you will work with a team of two or three classmates. Your team will combine your spreadsheets into new, team documents that contain the entries for your three or four population schedules. (Keep your original, individual spreadsheets!) The team sheets will have roughly 100 persons and about 20 households.\nIn the process of combining your spreadsheets you are likely to encounter errors in your initial data entry or situations where data was entered in incompatible ways by different people in the team. Fixing such problems is an example of “data cleaning.”\nTo facilitate identifying problems, your instructor will show you how to read your data into R so that you can wrangle summaries that point to inconsistencies. You don’t need to do this on your own; your instructor will help.\nOnce all teams have completed combining and cleaning their team sheets, we will as a class discuss inconsistencies between teams and reconcile them. Be prepared to do additional cleaning of your team sheets.\n\n\nPhase 3: Data summarization\nUsing grit, hard work, computing savvy, and magic, a heroic instructor will combine the various team spreadsheets into new, comprehensive sheets that encompass several hundred 1940 persons and a hundred or so households.\nYou may well get feedback from the hero-instructor about edits you should make to your team’s sheets. This is why you added a population schedule ID to every row of your individual spreadsheet, so that the instructor can figure out who is responsible for making the change. Please respond in a timely way so that all of us can move on.\nYour primary task—you, as an individual student—in Phase 3 is to create simple summaries of the combined data in tabular and/or graphical form. For instance, how many 1940 persons fall into the various household “roles” of head, spouse, child, lodger, etc. You will make several such summaries for your Phase 3 report. Be creative. Your instructor can help you to write the data wrangling commands needed.\n\n\nPhase 4: Modeling\nYou will choose a demographic/statistical/etc question of interest to you and carry out the data wrangling and modeling needed to make definite statements about the question. As an example, a person interested in housing affordability might model whether owning or renting a house is related to explanatory variables such as farm-vs-city, size of household, income of household, and so on.\nYour report will include not just the statistical analysis but also interpretation and supporting summaries. For the housing affordability example, it would be appropriate for instance to make a map showing housing costs as a function of geographic position to identify well- and poorly-housed areas of Travis County in 1940.\nYou will submit your written report and also give a short (3- to 5-minute) presentation in class. You are welcome to work with your student colleagues, but your model, report, and presentation will be your individual product."
  },
  {
    "objectID": "Kaplan_posts/2025-01-07-notes/2025-01-07-notes.html",
    "href": "Kaplan_posts/2025-01-07-notes/2025-01-07-notes.html",
    "title": "Kaplan’s notes for 2025-01-07",
    "section": "",
    "text": "The course is called “QR2.” This is not very descriptive. It would have been fair to name it “Data Science.” Data science is an appropriate core course for college students because modern society is data-driven to a great extent. Knowing even the basics of how to work with data, and how to draw valid conclusions from data (even if it’s just to be able to argue with others!) is tremendously empowering."
  },
  {
    "objectID": "Kaplan_posts/2025-01-07-notes/2025-01-07-notes.html#debrief",
    "href": "Kaplan_posts/2025-01-07-notes/2025-01-07-notes.html#debrief",
    "title": "Kaplan’s notes for 2025-01-07",
    "section": "Debrief",
    "text": "Debrief\nWhat do you know about data?\nWhat do you know about statistics?\n\nDid you have a previous stats course? (Or a data-science course, but these aren’t yet so common at the high-school level?)\n\nPossibilities:\n\nCorrelation is not causation."
  },
  {
    "objectID": "Kaplan_posts/2025-01-07-notes/2025-01-07-notes.html#rossling",
    "href": "Kaplan_posts/2025-01-07-notes/2025-01-07-notes.html#rossling",
    "title": "Kaplan’s notes for 2025-01-07",
    "section": "Rossling",
    "text": "Rossling\nA book about development and public health, what’s technically called “global health.”\nI selected this because\n\nIt involves data.\nEpidemiology provides a good reference client for useful statistics. What is epidemiology?\n\nMaking decisions with crappy data to guide actions.\n\nIt’s organized around 10 misleading heuristics that are widely used to interpret the world and that give a very misleading impression. Heuristics, e.g. “Us” and “them”; splitting things into two categories e.g. rich and poor; ….\n\nHave the class take the quiz."
  },
  {
    "objectID": "Kaplan_posts/2025-01-07-notes/2025-01-07-notes.html#spiegelhalter",
    "href": "Kaplan_posts/2025-01-07-notes/2025-01-07-notes.html#spiegelhalter",
    "title": "Kaplan’s notes for 2025-01-07",
    "section": "Spiegelhalter",
    "text": "Spiegelhalter\nDID NOT DO THIS IN CLASS\nBag of tricks. Some simple graphics. Short case study about drawing information out of graphics.\nBeing able to see patterns in data.\nExample: Gapminder data\n\nMurders per 100,000 people per year from this page\n\nWhat’s going on in Albania? Is this a large or small murder rate? Speculation about why. (Balkan wars post Yugoslavia break up.)\nWhat happened in Argentina?"
  },
  {
    "objectID": "Kaplan_posts/2025-01-07-notes/2025-01-07-notes.html#syllabus-daily-assignments",
    "href": "Kaplan_posts/2025-01-07-notes/2025-01-07-notes.html#syllabus-daily-assignments",
    "title": "Kaplan’s notes for 2025-01-07",
    "section": "Syllabus, daily assignments",
    "text": "Syllabus, daily assignments\n\nReadings from the two books (not always in chapter order).\n\nMost readings will have reading questions.\nSome of these are to be handed in before the class when we will discuss the readings, some are due by the following class. The reading questions are typically non-technical.\n\nComputing tutorials. These are somewhat interactive and you should interact with them."
  },
  {
    "objectID": "Kaplan_posts/2025-01-07-notes/2025-01-07-notes.html#data-organization-of",
    "href": "Kaplan_posts/2025-01-07-notes/2025-01-07-notes.html#data-organization-of",
    "title": "Kaplan’s notes for 2025-01-07",
    "section": "Data (organization of …)",
    "text": "Data (organization of …)\nWhat words do you associate with “data?”\nWhat do you know about how data is organized?\nGOT TO HERE IN CLASS\n\nWhat is a database?\nWhat is a “relational” database.\n\nWhat names/words might you have heard that relate to relational databases.\nWhen were they invented.\nHow big a sector of the economy? How does one measure “how big” when talking about the economy. Introduce data frame:\n\n\n\nColumns and rows.\nVariables and “specimens”\nQuantitative and categorical.\nIn R, data frames can be given names. We’ll work with a handful a lot so that we can become familiar with them and use them to illustrate general concepts.\nGalton\npoint_plot() and pipes\nTilde notation"
  },
  {
    "objectID": "Kaplan_posts/2025-02-28-notes/2025-02-28-notes.html",
    "href": "Kaplan_posts/2025-02-28-notes/2025-02-28-notes.html",
    "title": "Kaplan’s in-class notes for 2025-02-28",
    "section": "",
    "text": "Interpolation and extrapolation. Why extrapolation is hard. Why linear interpolation is good."
  },
  {
    "objectID": "Kaplan_posts/2025-02-28-notes/2025-02-28-notes.html#rosling-ch.-3-the-straight-line-instinct",
    "href": "Kaplan_posts/2025-02-28-notes/2025-02-28-notes.html#rosling-ch.-3-the-straight-line-instinct",
    "title": "Kaplan’s in-class notes for 2025-02-28",
    "section": "",
    "text": "Interpolation and extrapolation. Why extrapolation is hard. Why linear interpolation is good."
  },
  {
    "objectID": "Kaplan_posts/2025-02-28-notes/2025-02-28-notes.html#regression-techniques",
    "href": "Kaplan_posts/2025-02-28-notes/2025-02-28-notes.html#regression-techniques",
    "title": "Kaplan’s in-class notes for 2025-02-28",
    "section": "Regression techniques",
    "text": "Regression techniques"
  },
  {
    "objectID": "Kaplan_posts/2025-02-28-notes/2025-02-28-notes.html#computing",
    "href": "Kaplan_posts/2025-02-28-notes/2025-02-28-notes.html#computing",
    "title": "Kaplan’s in-class notes for 2025-02-28",
    "section": "Computing",
    "text": "Computing\nContrast prediction with regression: Same model training, but different goal: to understand what the connections are in a system.\n\nDifferent criteria apply to the selection of explanatory variables.\nWe are interested in details of the model, not just the prediction. But the form will look much like he prediction interval for quantitative response variables.\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nShow also “confidence band”\n\n\nHmod1 &lt;- Galton |&gt;\n  model_train(height ~ mother + father + sex) \nHmod1 |&gt; conf_interval()\n\n# A tibble: 4 × 4\n  term         .lwr  .coef   .upr\n  &lt;chr&gt;       &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 (Intercept) 9.95  15.3   20.7  \n2 mother      0.260  0.321  0.383\n3 father      0.349  0.406  0.463\n4 sexM        4.94   5.23   5.51 \n\nHmod1 |&gt; \n  model_eval(Galton) |&gt;\n  summarize(sd(.resid))\n\n  sd(.resid)\n1   2.150721\n\n\n\nEvaluation of model formula\nCompare by-hand evaluation at given inputs\n\n\nWhy do statistics book defer prediction intervals?\nWhy statistics books teach confidence intervals before prediction intervals: Prediction intervals look bad.\n\nThe residuals set the size (mainly) of the prediction intervals.\nThe prediction bands cover about 95% of the data. Drawing them by hand from a point plot.\n\nConfidence intervals get smaller/tighter the more data you have.\nSimulation: We’ll simulate the height data so that we can generate as much (made-up) data as we like.\n\nGalton |&gt; \n  summarize(mean(mother), sd(mother), \n            mean(father), sd(father))\n\n  mean(mother) sd(mother) mean(father) sd(father)\n1     64.08441   2.307025     69.23285   2.470256\n\n\n\nHsim &lt;- datasim_make(\n  mom &lt;- rnorm(n, mean=64, sd = 2.3),\n  dad &lt;- rnorm(n, mean=69, sd = 2.5),\n  sex &lt;- categorical(n, \"b\", \"g\"),\n  height &lt;- 15.34 + 0.32*mom + 0.41*dad + \n    cat2value(sex, b=5.22, g = 0) +\n    2.15 * rnorm(n)\n)\nHsim |&gt; take_sample(n = 5)\n\n# A tibble: 5 × 4\n    mom   dad sex   height\n  &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;  &lt;dbl&gt;\n1  66.0  71.5 b       72.6\n2  66.1  70.3 b       68.1\n3  63.2  66.0 g       63.1\n4  62.8  67.1 b       66.6\n5  66.3  71.0 g       65.7\n\n\nPrediction intervals don’t change so much with sample size:\n\nHsim |&gt; \n  take_sample(n = 100) |&gt;\n  point_plot(height ~ mom, annot = \"model\", interval = \"prediction\")\n\n\n\n\n\n\n\nHsim |&gt; \n  take_sample(n = 10000) |&gt;\n  point_plot(height ~ mom, annot = \"model\", interval = \"prediction\")\n\n\n\n\n\n\n\n\nNow change the interval to \"confidence\" and see how nice the graph looks."
  },
  {
    "objectID": "Kaplan_posts/2025-02-28-notes/2025-02-28-notes.html#r2",
    "href": "Kaplan_posts/2025-02-28-notes/2025-02-28-notes.html#r2",
    "title": "Kaplan’s in-class notes for 2025-02-28",
    "section": "R2",
    "text": "R2\nAnother model summary: How much of the variation in the response variable has been explained.\n\nHmod1 |&gt; R2()\n\n    n k  Rsquared        F     adjR2 p df.num df.denom\n1 898 3 0.6396752 529.0317 0.6384661 0      3      894"
  },
  {
    "objectID": "Kaplan_posts/2025-02-28-notes/2025-02-28-notes.html#covariates",
    "href": "Kaplan_posts/2025-02-28-notes/2025-02-28-notes.html#covariates",
    "title": "Kaplan’s in-class notes for 2025-02-28",
    "section": "Covariates",
    "text": "Covariates\nShow coefficients change as we add in covariates:\n\nGalton |&gt; \n  model_train(height ~ mother) |&gt;\n  conf_interval()\n\n# A tibble: 2 × 4\n  term          .lwr  .coef   .upr\n  &lt;chr&gt;        &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 (Intercept) 40.3   46.7   53.1  \n2 mother       0.213  0.313  0.413\n\nGalton |&gt; \n  model_train(height ~ mother + father + sex) |&gt;\n  conf_interval()\n\n# A tibble: 4 × 4\n  term         .lwr  .coef   .upr\n  &lt;chr&gt;       &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 (Intercept) 9.95  15.3   20.7  \n2 mother      0.260  0.321  0.383\n3 father      0.349  0.406  0.463\n4 sexM        4.94   5.23   5.51 \n\n\nThe coefficient tells the contribution of its variable in the context of the other explanatory variable.\nA more dramatic example:\nIn class I asked you what states have the highest SAT scores on average. Someone suggested that two top states might be California and Massachusetts.\nIn the spirit of Rosler, let’s look at the data:\n\nSAT |&gt; \n  select(state, sat) |&gt;\n  arrange(desc(sat)) |&gt;\n  head(10)\n\n          state  sat\n1  North Dakota 1107\n2          Iowa 1099\n3     Minnesota 1085\n4          Utah 1076\n5     Wisconsin 1073\n6  South Dakota 1068\n7        Kansas 1060\n8      Nebraska 1050\n9      Illinois 1048\n10     Missouri 1045\n\n\nLook at the worst states. How should I modify the command?\n\nSAT |&gt;\n  select(state, sat) |&gt;\n  filter(state %in% c(\"California\", \"Massachusetts\"))\n\n          state sat\n1    California 902\n2 Massachusetts 907\n\n\nLet’s look at some explanatory variables that might account for SAT score: expenditures, class size, teacher salaries, ???\n\nSAT |&gt; \n  model_train(sat ~ expend) |&gt;\n  conf_interval()\n\n# A tibble: 2 × 4\n  term          .lwr  .coef    .upr\n  &lt;chr&gt;        &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;\n1 (Intercept) 1000.  1089.  1179.  \n2 expend       -35.6  -20.9   -6.16\n\n\nInterpret the coefficient. “Effect size” but don’t necessarily put a causal interpretation on it.\nA covariate: class size?\n\nSAT |&gt; \n  model_train(sat ~ expend + ratio) |&gt;\n  conf_interval()\n\n# A tibble: 3 × 4\n  term         .lwr   .coef    .upr\n  &lt;chr&gt;       &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n1 (Intercept) 919.  1136.   1353.  \n2 expend      -38.3  -22.3    -6.30\n3 ratio       -11.9   -2.29    7.33\n\n\nTwo graphs:\n\nSAT |&gt;\n  point_plot(sat ~ expend, annot = \"model\")\n\n\n\n\n\n\n\nSAT |&gt;\n  point_plot(sat ~ expend + frac, annot = \"model\")\n\n\n\n\n\n\n\n\nThe second plot looks at the relationship between expenditure and SAT scores, adjusting for the fraction of students who take the SAT.\n\nMOVE THIS TO HYPOTHESIS TESTING when those note files have been created.\nAn article about reporting baseline characteristics in clinical trials, arguing that to avoid the trade-off between a data-mining kind of choice of covariates and not including them as needed, best just to plan for adjustment and do it without looking at the baseline balance. Maybe something for hypothesis-testing section: https://onlinelibrary.wiley.com/doi/abs/10.1002/sim.4780131703"
  },
  {
    "objectID": "Kaplan_posts/2025-01-31-notes/2025-01-31-notes.html",
    "href": "Kaplan_posts/2025-01-31-notes/2025-01-31-notes.html",
    "title": "Kaplan’s in-class notes for 2025-01-31",
    "section": "",
    "text": "Interpolation and extrapolation. Why extrapolation is hard. Why linear interpolation is good.\nWhat’s wrong with the figure on p. 90"
  },
  {
    "objectID": "Kaplan_posts/2025-01-31-notes/2025-01-31-notes.html#rosling-ch.-3-the-straight-line-instinct",
    "href": "Kaplan_posts/2025-01-31-notes/2025-01-31-notes.html#rosling-ch.-3-the-straight-line-instinct",
    "title": "Kaplan’s in-class notes for 2025-01-31",
    "section": "",
    "text": "Interpolation and extrapolation. Why extrapolation is hard. Why linear interpolation is good.\nWhat’s wrong with the figure on p. 90"
  },
  {
    "objectID": "Kaplan_posts/2025-01-31-notes/2025-01-31-notes.html#regression-techniques",
    "href": "Kaplan_posts/2025-01-31-notes/2025-01-31-notes.html#regression-techniques",
    "title": "Kaplan’s in-class notes for 2025-01-31",
    "section": "Regression techniques",
    "text": "Regression techniques"
  },
  {
    "objectID": "Kaplan_posts/2025-01-31-notes/2025-01-31-notes.html#computing",
    "href": "Kaplan_posts/2025-01-31-notes/2025-01-31-notes.html#computing",
    "title": "Kaplan’s in-class notes for 2025-01-31",
    "section": "Computing",
    "text": "Computing\nContrast prediction with regression: Same model training, but different goal: to understand what the connections are in a system.\n\nDifferent criteria apply to the selection of explanatory variables.\nWe are interested in details of the model, not just the prediction. But the form will look much like he prediction interval for quantitative response variables.\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nShow also “confidence band”\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nEvaluation of model formula\nCompare by-hand evaluation at given inputs\n\n\nWhy do statistics book defer prediction intervals?\nWhy statistics books teach confidence intervals before prediction intervals: Prediction intervals look bad.\n\nThe residuals set the size (mainly) of the prediction intervals.\nThe prediction bands cover about 95% of the data. Drawing them by hand from a point plot.\n\nConfidence intervals get smaller/tighter the more data you have.\nSimulation: We’ll simulate the height data so that we can generate as much (made-up) data as we like.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nPrediction intervals don’t change so much with sample size:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nNow change the interval to \"confidence\" and see how nice the graph looks."
  },
  {
    "objectID": "Kaplan_posts/2025-01-31-notes/2025-01-31-notes.html#r2",
    "href": "Kaplan_posts/2025-01-31-notes/2025-01-31-notes.html#r2",
    "title": "Kaplan’s in-class notes for 2025-01-31",
    "section": "R2",
    "text": "R2\nAnother model summary: How much of the variation in the response variable has been explained.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page."
  },
  {
    "objectID": "Kaplan_posts/2025-01-31-notes/2025-01-31-notes.html#covariates",
    "href": "Kaplan_posts/2025-01-31-notes/2025-01-31-notes.html#covariates",
    "title": "Kaplan’s in-class notes for 2025-01-31",
    "section": "Covariates",
    "text": "Covariates\nShow coefficients change as we add in covariates:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nThe coefficient tells the contribution of its variable in the context of the other explanatory variable.\nA more dramatic example:\nIn class I asked you what states have the highest SAT scores on average. Someone suggested that two top states might be California and Massachusetts.\nIn the spirit of Rosler, let’s look at the data:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nLook at the worst states. How should I modify the command?\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nLet’s look at some explanatory variables that might account for SAT score: expenditures, class size, teacher salaries, ???\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nInterpret the coefficient. “Effect size” but don’t necessarily put a causal interpretation on it.\nA covariate: class size?\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nTwo graphs:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nThe second plot looks at the relationship between expenditure and SAT scores, adjusting for the fraction of students who take the SAT.\n\nMOVE THIS TO HYPOTHESIS TESTING when those note files have been created.\nAn article about reporting baseline characteristics in clinical trials, arguing that to avoid the trade-off between a data-mining kind of choice of covariates and not including them as needed, best just to plan for adjustment and do it without looking at the baseline balance. Maybe something for hypothesis-testing section: https://onlinelibrary.wiley.com/doi/abs/10.1002/sim.4780131703"
  },
  {
    "objectID": "Kaplan_posts/2025-02-25-notes/2025-02-25-notes.html",
    "href": "Kaplan_posts/2025-02-25-notes/2025-02-25-notes.html",
    "title": "Kaplan’s in-class notes for 2025-02-25",
    "section": "",
    "text": "Interpolation and extrapolation. Why extrapolation is hard. Why linear interpolation is good."
  },
  {
    "objectID": "Kaplan_posts/2025-02-25-notes/2025-02-25-notes.html#rosling-ch.-3-the-straight-line-instinct",
    "href": "Kaplan_posts/2025-02-25-notes/2025-02-25-notes.html#rosling-ch.-3-the-straight-line-instinct",
    "title": "Kaplan’s in-class notes for 2025-02-25",
    "section": "",
    "text": "Interpolation and extrapolation. Why extrapolation is hard. Why linear interpolation is good."
  },
  {
    "objectID": "Kaplan_posts/2025-02-25-notes/2025-02-25-notes.html#regression-techniques",
    "href": "Kaplan_posts/2025-02-25-notes/2025-02-25-notes.html#regression-techniques",
    "title": "Kaplan’s in-class notes for 2025-02-25",
    "section": "Regression techniques",
    "text": "Regression techniques"
  },
  {
    "objectID": "Kaplan_posts/2025-02-25-notes/2025-02-25-notes.html#computing",
    "href": "Kaplan_posts/2025-02-25-notes/2025-02-25-notes.html#computing",
    "title": "Kaplan’s in-class notes for 2025-02-25",
    "section": "Computing",
    "text": "Computing\nContrast prediction with regression: Same model training, but different goal: to understand what the connections are in a system.\n\nDifferent criteria apply to the selection of explanatory variables.\nWe are interested in details of the model, not just the prediction. But the form will look much like he prediction interval for quantitative response variables.\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nShow also “confidence band”\n\n\nHmod1 &lt;- Galton |&gt;\n  model_train(height ~ mother + father + sex) \nHmod1 |&gt; conf_interval()\n\n# A tibble: 4 × 4\n  term         .lwr  .coef   .upr\n  &lt;chr&gt;       &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 (Intercept) 9.95  15.3   20.7  \n2 mother      0.260  0.321  0.383\n3 father      0.349  0.406  0.463\n4 sexM        4.94   5.23   5.51 \n\nHmod1 |&gt; \n  model_eval(Galton) |&gt;\n  summarize(sd(.resid))\n\n  sd(.resid)\n1   2.150721\n\n\n\nEvaluation of model formula\nCompare by-hand evaluation at given inputs\n\n\nWhy do statistics book defer prediction intervals?\nWhy statistics books teach confidence intervals before prediction intervals: Prediction intervals look bad.\n\nThe residuals set the size (mainly) of the prediction intervals.\nThe prediction bands cover about 95% of the data. Drawing them by hand from a point plot.\n\nConfidence intervals get smaller/tighter the more data you have.\nSimulation: We’ll simulate the height data so that we can generate as much (made-up) data as we like.\n\nGalton |&gt; \n  summarize(mean(mother), sd(mother), \n            mean(father), sd(father))\n\n  mean(mother) sd(mother) mean(father) sd(father)\n1     64.08441   2.307025     69.23285   2.470256\n\n\n\nHsim &lt;- datasim_make(\n  mom &lt;- rnorm(n, mean=64, sd = 2.3),\n  dad &lt;- rnorm(n, mean=69, sd = 2.5),\n  sex &lt;- categorical(n, \"b\", \"g\"),\n  height &lt;- 15.34 + 0.32*mom + 0.41*dad + \n    cat2value(sex, b=5.22, g = 0) +\n    2.15 * rnorm(n)\n)\nHsim |&gt; take_sample(n = 5)\n\n# A tibble: 5 × 4\n    mom   dad sex   height\n  &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;  &lt;dbl&gt;\n1  60.3  69.1 g       66.9\n2  66.5  65.4 b       68.7\n3  64.2  67.6 g       66.7\n4  66.0  67.0 b       72.1\n5  60.9  67.2 b       63.5\n\n\nPrediction intervals don’t change so much with sample size:\n\nHsim |&gt; \n  take_sample(n = 100) |&gt;\n  point_plot(height ~ mom, annot = \"model\", interval = \"prediction\")\n\n\n\n\n\n\n\nHsim |&gt; \n  take_sample(n = 10000) |&gt;\n  point_plot(height ~ mom, annot = \"model\", interval = \"prediction\")\n\n\n\n\n\n\n\n\nNow change the interval to \"confidence\" and see how nice the graph looks."
  },
  {
    "objectID": "Kaplan_posts/2025-02-25-notes/2025-02-25-notes.html#r2",
    "href": "Kaplan_posts/2025-02-25-notes/2025-02-25-notes.html#r2",
    "title": "Kaplan’s in-class notes for 2025-02-25",
    "section": "R2",
    "text": "R2\nAnother model summary: How much of the variation in the response variable has been explained.\n\nHmod1 |&gt; R2()\n\n    n k  Rsquared        F     adjR2 p df.num df.denom\n1 898 3 0.6396752 529.0317 0.6384661 0      3      894"
  },
  {
    "objectID": "Kaplan_posts/2025-02-25-notes/2025-02-25-notes.html#covariates",
    "href": "Kaplan_posts/2025-02-25-notes/2025-02-25-notes.html#covariates",
    "title": "Kaplan’s in-class notes for 2025-02-25",
    "section": "Covariates",
    "text": "Covariates\nShow coefficients change as we add in covariates:\n\nGalton |&gt; \n  model_train(height ~ mother) |&gt;\n  conf_interval()\n\n# A tibble: 2 × 4\n  term          .lwr  .coef   .upr\n  &lt;chr&gt;        &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 (Intercept) 40.3   46.7   53.1  \n2 mother       0.213  0.313  0.413\n\nGalton |&gt; \n  model_train(height ~ mother + father + sex) |&gt;\n  conf_interval()\n\n# A tibble: 4 × 4\n  term         .lwr  .coef   .upr\n  &lt;chr&gt;       &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 (Intercept) 9.95  15.3   20.7  \n2 mother      0.260  0.321  0.383\n3 father      0.349  0.406  0.463\n4 sexM        4.94   5.23   5.51 \n\n\nThe coefficient tells the contribution of its variable in the context of the other explanatory variable.\nA more dramatic example:\nIn class I asked you what states have the highest SAT scores on average. Someone suggested that two top states might be California and Massachusetts.\nIn the spirit of Rosler, let’s look at the data:\n\nSAT |&gt; \n  select(state, sat) |&gt;\n  arrange(desc(sat)) |&gt;\n  head(10)\n\n          state  sat\n1  North Dakota 1107\n2          Iowa 1099\n3     Minnesota 1085\n4          Utah 1076\n5     Wisconsin 1073\n6  South Dakota 1068\n7        Kansas 1060\n8      Nebraska 1050\n9      Illinois 1048\n10     Missouri 1045\n\n\nLook at the worst states. How should I modify the command?\n\nSAT |&gt;\n  select(state, sat) |&gt;\n  filter(state %in% c(\"California\", \"Massachusetts\"))\n\n          state sat\n1    California 902\n2 Massachusetts 907\n\n\nLet’s look at some explanatory variables that might account for SAT score: expenditures, class size, teacher salaries, ???\n\nSAT |&gt; \n  model_train(sat ~ expend) |&gt;\n  conf_interval()\n\n# A tibble: 2 × 4\n  term          .lwr  .coef    .upr\n  &lt;chr&gt;        &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;\n1 (Intercept) 1000.  1089.  1179.  \n2 expend       -35.6  -20.9   -6.16\n\n\nInterpret the coefficient. “Effect size” but don’t necessarily put a causal interpretation on it.\nA covariate: class size?\n\nSAT |&gt; \n  model_train(sat ~ expend + ratio) |&gt;\n  conf_interval()\n\n# A tibble: 3 × 4\n  term         .lwr   .coef    .upr\n  &lt;chr&gt;       &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n1 (Intercept) 919.  1136.   1353.  \n2 expend      -38.3  -22.3    -6.30\n3 ratio       -11.9   -2.29    7.33\n\n\nTwo graphs:\n\nSAT |&gt;\n  point_plot(sat ~ expend, annot = \"model\")\n\n\n\n\n\n\n\nSAT |&gt;\n  point_plot(sat ~ expend + frac, annot = \"model\")\n\n\n\n\n\n\n\n\nThe second plot looks at the relationship between expenditure and SAT scores, adjusting for the fraction of students who take the SAT.\n\nMOVE THIS TO HYPOTHESIS TESTING when those note files have been created.\nAn article about reporting baseline characteristics in clinical trials, arguing that to avoid the trade-off between a data-mining kind of choice of covariates and not including them as needed, best just to plan for adjustment and do it without looking at the baseline balance. Maybe something for hypothesis-testing section: https://onlinelibrary.wiley.com/doi/abs/10.1002/sim.4780131703"
  },
  {
    "objectID": "Kaplan_posts/2025-02-21-notes/2025-02-21-notes.html",
    "href": "Kaplan_posts/2025-02-21-notes/2025-02-21-notes.html",
    "title": "Kaplan’s in-class notes for 2025-02-21",
    "section": "",
    "text": "QUESTION 1: The following lines of code\n\nModel_1 &lt;- Galton |&gt;\n  model_train(height ~ mother + father + sex)\nModel_1 |&gt;\n  model_eval(sex = \"M\", mother = 70, father = 66)\n\nlead to this printed output\nsex  mother   father    .lwr   .output    .upr\nM        70       66    65.6    69.9      74.1\n\nPrediction interval bounds are not hard limits. They are intended to indicate an interval that contains 95% of the actual inputs.\n\n\nIn the output, there are some labels, a categorical level, and five numbers. Briefly explain what each of the five numbers means.\nYou can’t see the model coefficients from this output. Which R function should you pipe Model_1 into in order to read the coefficients?\n\n\nI’m seeing summary(), coef() and such which are not in the vocabulary we are using in the tutorial. Where are you hearing about these? A web search?\n\n\nShow whether the following coefficients are roughly consistent with the above output from model_eval(sex = \"M\", mother = 70, father = 66). Explain your reasoning.\n\n  term         .lwr  .coef   .upr\n  &lt;chr&gt;       &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 (Intercept) 9.95    15.0   20.7  \n2 mother      0.260    0.6  0.683\n3 father      0.349    0.5  0.863\n4 sexM        0.523    2.0   5.51 \nQUESTION 2: Bmod is the name of a model trained on a data frame named Buildings. You have never seen the Buildings data, nor do you know what response and explanatory variables were used in Bmod. Even so, you should be able to say which if any of the outputs A, B, and C below is a possible result from the following command. If the output is not possible, briefly explain why.\nBMod |&gt; \n  model_eval(data = Buildings) |&gt;\n  summarize(mean(.resid), \n            sd(.resid), \n            var(.resid))\nOutput A\n   mean(.resid) sd(.resid) var(.resid)\n1       1.4e-14         12         144\n\n“A is not possible because of the very low mean considering its large variance” - The mean does not constrain the variance. - Mean of residuals will always be zero (in-sample)\n\nOutput B\n   mean(.resid) sd(.resid) var(.resid)\n1             0         12         189\nOutput C\n   mean(.resid) sd(.resid) var(.resid)\n1             1         12         144\nQUESTION 3: The data (from Birdkeepers) comes from a study of birdkeeping and lung cancer. Here is a simple tabulation of the number of people in Birdkeepers who have lung cancer (LC) and who are birdkeepers (BK).\n\nBirdkeepers |&gt; summarize(n(), .by = c(LC, BK))\n\n          LC     BK n()\n1 LungCancer   Bird  33\n2 LungCancer NoBird  16\n3   NoCancer NoBird  64\n4   NoCancer   Bird  34\n\n\n\nUsing birdkeeping as a test for lung cancer, say what level of BK should correspond to a positive test result, then calculate the sensitivity, specificity, and prevalence, risk ratio, and odds ratio for LungCancer.\n\n\nPeople were good at this, but sometimes got mixed up between the actual state and the outcome of the do-you-own-a-bird test.\n\n\nThe prevalence of cancer in the study group is very high: one-third. This is intentional: one third (49) of the subjects were being treated for lung cancer, each was matched up with two controls with similar ages. How does the overly high prevalence change the interpretation of the sensitivity and specificity of the test.\n\n\nSome people suggested that high prevalence indicates that study is misleading (external validity) and somehow more random or untrustworthy than usual. But the high prevalence is due to the design of the study, not the result of sampling variation."
  },
  {
    "objectID": "Kaplan_posts/2025-02-21-notes/2025-02-21-notes.html#rosling-ch.-3-the-straight-line-instinct",
    "href": "Kaplan_posts/2025-02-21-notes/2025-02-21-notes.html#rosling-ch.-3-the-straight-line-instinct",
    "title": "Kaplan’s in-class notes for 2025-02-21",
    "section": "",
    "text": "Interpolation and extrapolation. Why extrapolation is hard. Why linear interpolation is good."
  },
  {
    "objectID": "Kaplan_posts/2025-02-21-notes/2025-02-21-notes.html#regression-techniques",
    "href": "Kaplan_posts/2025-02-21-notes/2025-02-21-notes.html#regression-techniques",
    "title": "Kaplan’s in-class notes for 2025-02-21",
    "section": "Regression techniques",
    "text": "Regression techniques"
  },
  {
    "objectID": "Kaplan_posts/2025-02-21-notes/2025-02-21-notes.html#computing",
    "href": "Kaplan_posts/2025-02-21-notes/2025-02-21-notes.html#computing",
    "title": "Kaplan’s in-class notes for 2025-02-21",
    "section": "Computing",
    "text": "Computing\nContrast prediction with regression: Same model training, but different goal: to understand what the connections are in a system.\n\nDifferent criteria apply to the selection of explanatory variables.\nWe are interested in details of the model, not just the prediction. But the form will look much like he prediction interval for quantitative response variables.\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nShow also “confidence band”\n\n\nHmod1 &lt;- Galton |&gt;\n  model_train(height ~ mother + father + sex) \nHmod1 |&gt; conf_interval()\n\n# A tibble: 4 × 4\n  term         .lwr  .coef   .upr\n  &lt;chr&gt;       &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 (Intercept) 9.95  15.3   20.7  \n2 mother      0.260  0.321  0.383\n3 father      0.349  0.406  0.463\n4 sexM        4.94   5.23   5.51 \n\nHmod1 |&gt; \n  model_eval(Galton) |&gt;\n  summarize(sd(.resid))\n\n  sd(.resid)\n1   2.150721\n\n\n\nEvaluation of model formula\nCompare by-hand evaluation at given inputs\n\n\nWhy do statistics book defer prediction intervals?\nWhy statistics books teach confidence intervals before prediction intervals: Prediction intervals look bad.\n\nThe residuals set the size (mainly) of the prediction intervals.\nThe prediction bands cover about 95% of the data. Drawing them by hand from a point plot.\n\nConfidence intervals get smaller/tighter the more data you have.\nSimulation: We’ll simulate the height data so that we can generate as much (made-up) data as we like.\n\nGalton |&gt; \n  summarize(mean(mother), sd(mother), \n            mean(father), sd(father))\n\n  mean(mother) sd(mother) mean(father) sd(father)\n1     64.08441   2.307025     69.23285   2.470256\n\n\n\nHsim &lt;- datasim_make(\n  mom &lt;- rnorm(n, mean=64, sd = 2.3),\n  dad &lt;- rnorm(n, mean=69, sd = 2.5),\n  sex &lt;- categorical(n, \"b\", \"g\"),\n  height &lt;- 15.34 + 0.32*mom + 0.41*dad + \n    cat2value(sex, b=5.22, g = 0) +\n    2.15 * rnorm(n)\n)\nHsim |&gt; take_sample(n = 5)\n\n# A tibble: 5 × 4\n    mom   dad sex   height\n  &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;  &lt;dbl&gt;\n1  65.3  66.6 g       64.8\n2  63.8  65.2 b       64.9\n3  68.2  68.5 b       66.6\n4  59.1  71.3 b       69.9\n5  64.9  69.5 g       65.9\n\n\nPrediction intervals don’t change so much with sample size:\n\nHsim |&gt; \n  take_sample(n = 100) |&gt;\n  point_plot(height ~ mom, annot = \"model\", interval = \"prediction\")\n\n\n\n\n\n\n\nHsim |&gt; \n  take_sample(n = 10000) |&gt;\n  point_plot(height ~ mom, annot = \"model\", interval = \"prediction\")\n\n\n\n\n\n\n\n\nNow change the interval to \"confidence\" and see how nice the graph looks."
  },
  {
    "objectID": "Kaplan_posts/2025-02-21-notes/2025-02-21-notes.html#r2",
    "href": "Kaplan_posts/2025-02-21-notes/2025-02-21-notes.html#r2",
    "title": "Kaplan’s in-class notes for 2025-02-21",
    "section": "R2",
    "text": "R2\nAnother model summary: How much of the variation in the response variable has been explained.\n\nHmod1 |&gt; R2()\n\n    n k  Rsquared        F     adjR2 p df.num df.denom\n1 898 3 0.6396752 529.0317 0.6384661 0      3      894"
  },
  {
    "objectID": "Kaplan_posts/2025-02-21-notes/2025-02-21-notes.html#covariates",
    "href": "Kaplan_posts/2025-02-21-notes/2025-02-21-notes.html#covariates",
    "title": "Kaplan’s in-class notes for 2025-02-21",
    "section": "Covariates",
    "text": "Covariates\nShow coefficients change as we add in covariates:\n\nGalton |&gt; \n  model_train(height ~ mother) |&gt;\n  conf_interval()\n\n# A tibble: 2 × 4\n  term          .lwr  .coef   .upr\n  &lt;chr&gt;        &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 (Intercept) 40.3   46.7   53.1  \n2 mother       0.213  0.313  0.413\n\nGalton |&gt; \n  model_train(height ~ mother + father + sex) |&gt;\n  conf_interval()\n\n# A tibble: 4 × 4\n  term         .lwr  .coef   .upr\n  &lt;chr&gt;       &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 (Intercept) 9.95  15.3   20.7  \n2 mother      0.260  0.321  0.383\n3 father      0.349  0.406  0.463\n4 sexM        4.94   5.23   5.51 \n\n\nThe coefficient tells the contribution of its variable in the context of the other explanatory variable.\nA more dramatic example:\nIn class I asked you what states have the highest SAT scores on average. Someone suggested that two top states might be California and Massachusetts.\nIn the spirit of Rosler, let’s look at the data:\n\nSAT |&gt; \n  select(state, sat) |&gt;\n  arrange(desc(sat)) |&gt;\n  head(10)\n\n          state  sat\n1  North Dakota 1107\n2          Iowa 1099\n3     Minnesota 1085\n4          Utah 1076\n5     Wisconsin 1073\n6  South Dakota 1068\n7        Kansas 1060\n8      Nebraska 1050\n9      Illinois 1048\n10     Missouri 1045\n\n\nLook at the worst states. How should I modify the command?\n\nSAT |&gt;\n  select(state, sat) |&gt;\n  filter(state %in% c(\"California\", \"Massachusetts\"))\n\n          state sat\n1    California 902\n2 Massachusetts 907\n\n\nLet’s look at some explanatory variables that might account for SAT score: expenditures, class size, teacher salaries, ???\n\nSAT |&gt; \n  model_train(sat ~ expend) |&gt;\n  conf_interval()\n\n# A tibble: 2 × 4\n  term          .lwr  .coef    .upr\n  &lt;chr&gt;        &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;\n1 (Intercept) 1000.  1089.  1179.  \n2 expend       -35.6  -20.9   -6.16\n\n\nInterpret the coefficient. “Effect size” but don’t necessarily put a causal interpretation on it.\nA covariate: class size?\n\nSAT |&gt; \n  model_train(sat ~ expend + ratio) |&gt;\n  conf_interval()\n\n# A tibble: 3 × 4\n  term         .lwr   .coef    .upr\n  &lt;chr&gt;       &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n1 (Intercept) 919.  1136.   1353.  \n2 expend      -38.3  -22.3    -6.30\n3 ratio       -11.9   -2.29    7.33\n\n\nTwo graphs:\n\nSAT |&gt;\n  point_plot(sat ~ expend, annot = \"model\")\n\n\n\n\n\n\n\nSAT |&gt;\n  point_plot(sat ~ expend + frac, annot = \"model\")\n\n\n\n\n\n\n\n\nThe second plot looks at the relationship between expenditure and SAT scores, adjusting for the fraction of students who take the SAT.\n\nMOVE THIS TO HYPOTHESIS TESTING when those note files have been created.\nAn article about reporting baseline characteristics in clinical trials, arguing that to avoid the trade-off between a data-mining kind of choice of covariates and not including them as needed, best just to plan for adjustment and do it without looking at the baseline balance. Maybe something for hypothesis-testing section: https://onlinelibrary.wiley.com/doi/abs/10.1002/sim.4780131703"
  },
  {
    "objectID": "Kaplan_posts/2025-01-14-notes/2025-01-14-notes.html",
    "href": "Kaplan_posts/2025-01-14-notes/2025-01-14-notes.html",
    "title": "Kaplan’s in-class notes for 2025-01-14",
    "section": "",
    "text": "Tell me what types of statistical graphics you’ve encountered. Draw some (imagined) examples of different types on the board.\n\nWe will mainly work with one type: the “annotated point plot.”\nGeneral statistical graphics words:\n\ngraphics frame\nglyphs\ngraphical property (“aesthetic”): color, transparency, angle, length, size, …\nscale: transformation from data values to graphical property, E.G. or y or color or size or whatever\n“guide” the tick marks or legend.\nfacet\n\npoint_plot() system:\n\npoints and annotations\na point_plot() is fundamentally about a data frame and the trends encountered therein.\n\neach point corresponds to one specimen from the data frame.\npoint plots work with either quantitative or categorical variables\na point is a simple glyph with location (x, y, facet) and color, transparency (point_ink).\n\nan “annotation” is another kind of glyph, typically with somewhat complex structure\n\nwe will use two basic types of annotations, \"model\" and \"violin\". They serve different purposes.\nannotations reflect collective properties of multiple specimens (often all the specimens)\n\n\nTry out different variables.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nShow different scales: categorical and quantitative\nAdd violin annotations.\nAdd model annotations.\nThe meaning of var ~ 1. We’ll call this the Null model.\npoint_ink, model_ink annotations. (Explain the use of _ instead of - within a name.)"
  },
  {
    "objectID": "Kaplan_posts/2025-01-14-notes/2025-01-14-notes.html#graphics-background",
    "href": "Kaplan_posts/2025-01-14-notes/2025-01-14-notes.html#graphics-background",
    "title": "Kaplan’s in-class notes for 2025-01-14",
    "section": "",
    "text": "Tell me what types of statistical graphics you’ve encountered. Draw some (imagined) examples of different types on the board.\n\nWe will mainly work with one type: the “annotated point plot.”\nGeneral statistical graphics words:\n\ngraphics frame\nglyphs\ngraphical property (“aesthetic”): color, transparency, angle, length, size, …\nscale: transformation from data values to graphical property, E.G. or y or color or size or whatever\n“guide” the tick marks or legend.\nfacet\n\npoint_plot() system:\n\npoints and annotations\na point_plot() is fundamentally about a data frame and the trends encountered therein.\n\neach point corresponds to one specimen from the data frame.\npoint plots work with either quantitative or categorical variables\na point is a simple glyph with location (x, y, facet) and color, transparency (point_ink).\n\nan “annotation” is another kind of glyph, typically with somewhat complex structure\n\nwe will use two basic types of annotations, \"model\" and \"violin\". They serve different purposes.\nannotations reflect collective properties of multiple specimens (often all the specimens)\n\n\nTry out different variables.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nShow different scales: categorical and quantitative\nAdd violin annotations.\nAdd model annotations.\nThe meaning of var ~ 1. We’ll call this the Null model.\npoint_ink, model_ink annotations. (Explain the use of _ instead of - within a name.)"
  },
  {
    "objectID": "Kaplan_posts/2025-01-14-notes/2025-01-14-notes.html#rossling-chapter-2",
    "href": "Kaplan_posts/2025-01-14-notes/2025-01-14-notes.html#rossling-chapter-2",
    "title": "Kaplan’s in-class notes for 2025-01-14",
    "section": "Rossling Chapter 2",
    "text": "Rossling Chapter 2\n\nReading questions\nBad and better\n\nPeople feel like just because things aren’t the best, they cant simply be better. They feel that there is still bad in the world (which there is) but they don’t stop to think how much it has improved.\n\n\nDistinguishing between “better” and “bad” means to distinguish between a level and a trend. For example, something can be bad overall (like poverty being high) but can be trending in a direction indicating that it’s getting better. I think the component of high school curriculum that makes a framework for this distinction is the SAT.\n\n\nRoom for improvement. Not satisfactory but developed from the last time it was measured. Sounds kind of like the growth mindset.\n\nDistinguishing between the current state and how the state is changing is in the domain of calculus.\nGive examples: (We’ll leave out “middling” and “staying the same.”)\n\nGood and getting better\nGood and getting worse\nBad and getting worse\nBad and getting better\n\nFeeling, not thinking &gt; The idea that your outlook remains negative even after seeing positive data because you know problems still exist (even if they’re getting better). ## Wrangling\nYou are expected to learn the names of the basic operations but not to be able to create complex “queries” with them. Such mastery comes with experience.\nTalk about units of observation. Show how summarize changes the unit of observation, but the other wrangling functions leave it alone.\nThe basic operations.\nThe most advanced wrangling operation is a “join” or “merge”. This is the only wrangling operation that takes two data frames as input. [Try Grades, Sessions, Gradepoint database.]\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nYou do not need to master joins. But it is essential to work with databases. As of 10 years ago, few people other than database professionals knew about it: I taught it to hundreds of faculty, most of whom were shocked that something so simple accomplished what they had been doing by hand and calling “cross referencing.”"
  },
  {
    "objectID": "Kaplan_posts/2025-01-14-notes/2025-01-14-notes.html#interpreting-trends",
    "href": "Kaplan_posts/2025-01-14-notes/2025-01-14-notes.html#interpreting-trends",
    "title": "Kaplan’s in-class notes for 2025-01-14",
    "section": "Interpreting trends",
    "text": "Interpreting trends\nBirths78 shows a pattern, a kind of trend\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nForm hypotheses about the two bands. Then go back to data and check them.\nWrangle out the anomolies, e.g. births ~ day_of_week\n\nBirths78 |&gt;\n  point_plot(births ~ wday)\n\n\n\n\n\n\n\nBirths78 |&gt;\n  select(date, births, wday) |&gt;\n  mutate(r = rank(births), .by = wday) |&gt;\n  arrange(r) |&gt;\n  head(14)\n\n         date births wday r\n1  1978-01-02   7527  Mon 1\n2  1978-04-12   8606  Wed 1\n3  1978-04-15   7527  Sat 1\n4  1978-04-21   8892  Fri 1\n5  1978-04-30   7135  Sun 1\n6  1978-07-04   8433  Tue 1\n7  1978-11-23   7915  Thu 1\n8  1978-01-03   8825  Tue 2\n9  1978-04-16   7193  Sun 2\n10 1978-04-19   8720  Wed 2\n11 1978-04-20   8582  Thu 2\n12 1978-05-06   7718  Sat 2\n13 1978-05-12   8975  Fri 2\n14 1978-05-29   7780  Mon 2"
  },
  {
    "objectID": "Kaplan_posts/2025-02-04-notes/2025-02-04-notes.html",
    "href": "Kaplan_posts/2025-02-04-notes/2025-02-04-notes.html",
    "title": "Kaplan’s in-class notes for 2025-02-04",
    "section": "",
    "text": "We know how to “fit” models: model_train()\nModels are themselves complex computational objects:\n\nmod &lt;- Galton |&gt;\n  model_train(height ~ mother + father + sex)\nstr(mod)\n\nList of 13\n $ coefficients : Named num [1:4] 15.345 0.321 0.406 5.226\n  ..- attr(*, \"names\")= chr [1:4] \"(Intercept)\" \"mother\" \"father\" \"sexM\"\n $ residuals    : Named num [1:898] -0.78 0.446 0.246 0.246 0.899 ...\n  ..- attr(*, \"names\")= chr [1:898] \"1\" \"2\" \"3\" \"4\" ...\n $ effects      : Named num [1:898] -2000.6 -21.6 -28 78.2 0.9 ...\n  ..- attr(*, \"names\")= chr [1:898] \"(Intercept)\" \"mother\" \"father\" \"sexM\" ...\n $ rank         : int 4\n $ fitted.values: Named num [1:898] 74 68.8 68.8 68.8 72.6 ...\n  ..- attr(*, \"names\")= chr [1:898] \"1\" \"2\" \"3\" \"4\" ...\n $ assign       : int [1:4] 0 1 2 3\n $ qr           :List of 5\n  ..$ qr   : num [1:898, 1:4] -29.9666 0.0334 0.0334 0.0334 0.0334 ...\n  .. ..- attr(*, \"dimnames\")=List of 2\n  .. .. ..$ : chr [1:898] \"1\" \"2\" \"3\" \"4\" ...\n  .. .. ..$ : chr [1:4] \"(Intercept)\" \"mother\" \"father\" \"sexM\"\n  .. ..- attr(*, \"assign\")= int [1:4] 0 1 2 3\n  .. ..- attr(*, \"contrasts\")=List of 1\n  .. .. ..$ sex: chr \"contr.treatment\"\n  ..$ qraux: num [1:4] 1.03 1.04 1.11 1.03\n  ..$ pivot: int [1:4] 1 2 3 4\n  ..$ tol  : num 1e-07\n  ..$ rank : int 4\n  ..- attr(*, \"class\")= chr \"qr\"\n $ df.residual  : int 894\n $ contrasts    :List of 1\n  ..$ sex: chr \"contr.treatment\"\n $ xlevels      :List of 1\n  ..$ sex: chr [1:2] \"F\" \"M\"\n $ call         : language stats::lm(formula = tilde, data = data)\n $ terms        :Classes 'terms', 'formula'  language height ~ mother + father + sex\n  .. ..- attr(*, \"variables\")= language list(height, mother, father, sex)\n  .. ..- attr(*, \"factors\")= int [1:4, 1:3] 0 1 0 0 0 0 1 0 0 0 ...\n  .. .. ..- attr(*, \"dimnames\")=List of 2\n  .. .. .. ..$ : chr [1:4] \"height\" \"mother\" \"father\" \"sex\"\n  .. .. .. ..$ : chr [1:3] \"mother\" \"father\" \"sex\"\n  .. ..- attr(*, \"term.labels\")= chr [1:3] \"mother\" \"father\" \"sex\"\n  .. ..- attr(*, \"order\")= int [1:3] 1 1 1\n  .. ..- attr(*, \"intercept\")= int 1\n  .. ..- attr(*, \"response\")= int 1\n  .. ..- attr(*, \".Environment\")=&lt;environment: R_GlobalEnv&gt; \n  .. ..- attr(*, \"predvars\")= language list(height, mother, father, sex)\n  .. ..- attr(*, \"dataClasses\")= Named chr [1:4] \"numeric\" \"numeric\" \"numeric\" \"factor\"\n  .. .. ..- attr(*, \"names\")= chr [1:4] \"height\" \"mother\" \"father\" \"sex\"\n $ model        :'data.frame':  898 obs. of  4 variables:\n  ..$ height: num [1:898] 73.2 69.2 69 69 73.5 72.5 65.5 65.5 71 68 ...\n  ..$ mother: num [1:898] 67 67 67 67 66.5 66.5 66.5 66.5 64 64 ...\n  ..$ father: num [1:898] 78.5 78.5 78.5 78.5 75.5 75.5 75.5 75.5 75 75 ...\n  ..$ sex   : Factor w/ 2 levels \"F\",\"M\": 2 1 1 1 2 2 1 1 2 1 ...\n  ..- attr(*, \"terms\")=Classes 'terms', 'formula'  language height ~ mother + father + sex\n  .. .. ..- attr(*, \"variables\")= language list(height, mother, father, sex)\n  .. .. ..- attr(*, \"factors\")= int [1:4, 1:3] 0 1 0 0 0 0 1 0 0 0 ...\n  .. .. .. ..- attr(*, \"dimnames\")=List of 2\n  .. .. .. .. ..$ : chr [1:4] \"height\" \"mother\" \"father\" \"sex\"\n  .. .. .. .. ..$ : chr [1:3] \"mother\" \"father\" \"sex\"\n  .. .. ..- attr(*, \"term.labels\")= chr [1:3] \"mother\" \"father\" \"sex\"\n  .. .. ..- attr(*, \"order\")= int [1:3] 1 1 1\n  .. .. ..- attr(*, \"intercept\")= int 1\n  .. .. ..- attr(*, \"response\")= int 1\n  .. .. ..- attr(*, \".Environment\")=&lt;environment: R_GlobalEnv&gt; \n  .. .. ..- attr(*, \"predvars\")= language list(height, mother, father, sex)\n  .. .. ..- attr(*, \"dataClasses\")= Named chr [1:4] \"numeric\" \"numeric\" \"numeric\" \"factor\"\n  .. .. .. ..- attr(*, \"names\")= chr [1:4] \"height\" \"mother\" \"father\" \"sex\"\n - attr(*, \"class\")= chr [1:2] \"model_object\" \"lm\"\n - attr(*, \"training_data\")='data.frame':   898 obs. of  6 variables:\n  ..$ family: Factor w/ 197 levels \"1\",\"10\",\"100\",..: 1 1 1 1 108 108 108 108 123 123 ...\n  ..$ father: num [1:898] 78.5 78.5 78.5 78.5 75.5 75.5 75.5 75.5 75 75 ...\n  ..$ mother: num [1:898] 67 67 67 67 66.5 66.5 66.5 66.5 64 64 ...\n  ..$ sex   : Factor w/ 2 levels \"F\",\"M\": 2 1 1 1 2 2 1 1 2 1 ...\n  ..$ height: num [1:898] 73.2 69.2 69 69 73.5 72.5 65.5 65.5 71 68 ...\n  ..$ nkids : int [1:898] 4 4 4 4 4 4 4 4 2 2 ...\n\n\nWe have met three ways of summarizing models:\n\nPlot them. point_plot() does this. We can also use model_plot() directly on the model. (But best to display data along with the models.)\nCoefficients with the conf_interval() summary.\nR2 with the R2() summary.\n\nIn the previous block, we discussed prediction and prediction intervals. We used the same regression models to quantify prediction. But here, our aim is different, to characterize the relationship between the explanatory and response variables.\n\nIs there any relationship?\nIf there is a relationship, is it positive or negative. This corresponds to the sign of the coefficient.\n\nADJUSTMENT:\n\nAre there other explanatory variables that might contribute to or “color” the relationship indicated by (ii)?"
  },
  {
    "objectID": "Kaplan_posts/2025-02-04-notes/2025-02-04-notes.html#where-we-are",
    "href": "Kaplan_posts/2025-02-04-notes/2025-02-04-notes.html#where-we-are",
    "title": "Kaplan’s in-class notes for 2025-02-04",
    "section": "",
    "text": "We know how to “fit” models: model_train()\nModels are themselves complex computational objects:\n\nmod &lt;- Galton |&gt;\n  model_train(height ~ mother + father + sex)\nstr(mod)\n\nList of 13\n $ coefficients : Named num [1:4] 15.345 0.321 0.406 5.226\n  ..- attr(*, \"names\")= chr [1:4] \"(Intercept)\" \"mother\" \"father\" \"sexM\"\n $ residuals    : Named num [1:898] -0.78 0.446 0.246 0.246 0.899 ...\n  ..- attr(*, \"names\")= chr [1:898] \"1\" \"2\" \"3\" \"4\" ...\n $ effects      : Named num [1:898] -2000.6 -21.6 -28 78.2 0.9 ...\n  ..- attr(*, \"names\")= chr [1:898] \"(Intercept)\" \"mother\" \"father\" \"sexM\" ...\n $ rank         : int 4\n $ fitted.values: Named num [1:898] 74 68.8 68.8 68.8 72.6 ...\n  ..- attr(*, \"names\")= chr [1:898] \"1\" \"2\" \"3\" \"4\" ...\n $ assign       : int [1:4] 0 1 2 3\n $ qr           :List of 5\n  ..$ qr   : num [1:898, 1:4] -29.9666 0.0334 0.0334 0.0334 0.0334 ...\n  .. ..- attr(*, \"dimnames\")=List of 2\n  .. .. ..$ : chr [1:898] \"1\" \"2\" \"3\" \"4\" ...\n  .. .. ..$ : chr [1:4] \"(Intercept)\" \"mother\" \"father\" \"sexM\"\n  .. ..- attr(*, \"assign\")= int [1:4] 0 1 2 3\n  .. ..- attr(*, \"contrasts\")=List of 1\n  .. .. ..$ sex: chr \"contr.treatment\"\n  ..$ qraux: num [1:4] 1.03 1.04 1.11 1.03\n  ..$ pivot: int [1:4] 1 2 3 4\n  ..$ tol  : num 1e-07\n  ..$ rank : int 4\n  ..- attr(*, \"class\")= chr \"qr\"\n $ df.residual  : int 894\n $ contrasts    :List of 1\n  ..$ sex: chr \"contr.treatment\"\n $ xlevels      :List of 1\n  ..$ sex: chr [1:2] \"F\" \"M\"\n $ call         : language stats::lm(formula = tilde, data = data)\n $ terms        :Classes 'terms', 'formula'  language height ~ mother + father + sex\n  .. ..- attr(*, \"variables\")= language list(height, mother, father, sex)\n  .. ..- attr(*, \"factors\")= int [1:4, 1:3] 0 1 0 0 0 0 1 0 0 0 ...\n  .. .. ..- attr(*, \"dimnames\")=List of 2\n  .. .. .. ..$ : chr [1:4] \"height\" \"mother\" \"father\" \"sex\"\n  .. .. .. ..$ : chr [1:3] \"mother\" \"father\" \"sex\"\n  .. ..- attr(*, \"term.labels\")= chr [1:3] \"mother\" \"father\" \"sex\"\n  .. ..- attr(*, \"order\")= int [1:3] 1 1 1\n  .. ..- attr(*, \"intercept\")= int 1\n  .. ..- attr(*, \"response\")= int 1\n  .. ..- attr(*, \".Environment\")=&lt;environment: R_GlobalEnv&gt; \n  .. ..- attr(*, \"predvars\")= language list(height, mother, father, sex)\n  .. ..- attr(*, \"dataClasses\")= Named chr [1:4] \"numeric\" \"numeric\" \"numeric\" \"factor\"\n  .. .. ..- attr(*, \"names\")= chr [1:4] \"height\" \"mother\" \"father\" \"sex\"\n $ model        :'data.frame':  898 obs. of  4 variables:\n  ..$ height: num [1:898] 73.2 69.2 69 69 73.5 72.5 65.5 65.5 71 68 ...\n  ..$ mother: num [1:898] 67 67 67 67 66.5 66.5 66.5 66.5 64 64 ...\n  ..$ father: num [1:898] 78.5 78.5 78.5 78.5 75.5 75.5 75.5 75.5 75 75 ...\n  ..$ sex   : Factor w/ 2 levels \"F\",\"M\": 2 1 1 1 2 2 1 1 2 1 ...\n  ..- attr(*, \"terms\")=Classes 'terms', 'formula'  language height ~ mother + father + sex\n  .. .. ..- attr(*, \"variables\")= language list(height, mother, father, sex)\n  .. .. ..- attr(*, \"factors\")= int [1:4, 1:3] 0 1 0 0 0 0 1 0 0 0 ...\n  .. .. .. ..- attr(*, \"dimnames\")=List of 2\n  .. .. .. .. ..$ : chr [1:4] \"height\" \"mother\" \"father\" \"sex\"\n  .. .. .. .. ..$ : chr [1:3] \"mother\" \"father\" \"sex\"\n  .. .. ..- attr(*, \"term.labels\")= chr [1:3] \"mother\" \"father\" \"sex\"\n  .. .. ..- attr(*, \"order\")= int [1:3] 1 1 1\n  .. .. ..- attr(*, \"intercept\")= int 1\n  .. .. ..- attr(*, \"response\")= int 1\n  .. .. ..- attr(*, \".Environment\")=&lt;environment: R_GlobalEnv&gt; \n  .. .. ..- attr(*, \"predvars\")= language list(height, mother, father, sex)\n  .. .. ..- attr(*, \"dataClasses\")= Named chr [1:4] \"numeric\" \"numeric\" \"numeric\" \"factor\"\n  .. .. .. ..- attr(*, \"names\")= chr [1:4] \"height\" \"mother\" \"father\" \"sex\"\n - attr(*, \"class\")= chr [1:2] \"model_object\" \"lm\"\n - attr(*, \"training_data\")='data.frame':   898 obs. of  6 variables:\n  ..$ family: Factor w/ 197 levels \"1\",\"10\",\"100\",..: 1 1 1 1 108 108 108 108 123 123 ...\n  ..$ father: num [1:898] 78.5 78.5 78.5 78.5 75.5 75.5 75.5 75.5 75 75 ...\n  ..$ mother: num [1:898] 67 67 67 67 66.5 66.5 66.5 66.5 64 64 ...\n  ..$ sex   : Factor w/ 2 levels \"F\",\"M\": 2 1 1 1 2 2 1 1 2 1 ...\n  ..$ height: num [1:898] 73.2 69.2 69 69 73.5 72.5 65.5 65.5 71 68 ...\n  ..$ nkids : int [1:898] 4 4 4 4 4 4 4 4 2 2 ...\n\n\nWe have met three ways of summarizing models:\n\nPlot them. point_plot() does this. We can also use model_plot() directly on the model. (But best to display data along with the models.)\nCoefficients with the conf_interval() summary.\nR2 with the R2() summary.\n\nIn the previous block, we discussed prediction and prediction intervals. We used the same regression models to quantify prediction. But here, our aim is different, to characterize the relationship between the explanatory and response variables.\n\nIs there any relationship?\nIf there is a relationship, is it positive or negative. This corresponds to the sign of the coefficient.\n\nADJUSTMENT:\n\nAre there other explanatory variables that might contribute to or “color” the relationship indicated by (ii)?"
  },
  {
    "objectID": "Kaplan_posts/2025-02-04-notes/2025-02-04-notes.html#adjustment",
    "href": "Kaplan_posts/2025-02-04-notes/2025-02-04-notes.html#adjustment",
    "title": "Kaplan’s in-class notes for 2025-02-04",
    "section": "Adjustment",
    "text": "Adjustment\nAdjustment generally: keeping other things the same when examining the relationship between two variables of interest. Classical experimental design, keeping conditions and materials constant while varying an input of interest, is an example of setting things up to avoid having to adjust. But with observational data, we can’t intervene and instead have to use modeling to try to approximate the experimental conditions.\nInterpretting the Galton model when adjusting for other potential explanatory variables. Example:\nA negative relationship! Bigger families have shorter kids.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nWhat if we “adjust” for the child’s sex?\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nWhat if we get rid of the one family with 15 kids? Maybe they are pulling down the average.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nTrivial example of adjustment: school expenditures and student performance (as indicated by standardized tests).\n\nThe very idea of a “standardized” test is an experiment-like attempt to hold conditions constant. If states or school districts designed their own evaluations, it would be difficult to compare different states or districts. I wouldn’t call this adjustment, since it’s an intervention, but an intervention with the same aim as adjustment.\nStudent performance: average across individuals in order to avoid reports that cherry-pick the best students. (School districts have indeed been found out both altering student test forms and telling poor students to stay away during standardized testing days.)\nExpenditures. There are big states that spend large amounts on education and small states that spend relatively small amounts. To avoid confusing (a word we will see later is “confounding”) state size with the intensity of spending, a sensible (and common-sense) form of adjustment is per capita adjustment: divide spending by the number of people. For education, per pupil spending is appropriate.\n\nShow the SAT data:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nWhat do you think are the best and worst states (on average) for education? Let’s see how this lines up with expenditures (per pupil).\n\n\nSAT |&gt; \n  select(state, expend) |&gt;\n  arrange(desc(expend)) |&gt;\n  DT::datatable()\n\n\n\n\n\nWARNING: If you have heard from other students or read this example previously, don’t be a spoiler! Keep it to yourself.\nSAT performance versus per-capita school expenditures. This adjusts for state size.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nTry also ratio (something like class size) and salary (how well they pay teachers).\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nAdjusting for who takes the SAT\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page."
  },
  {
    "objectID": "Kaplan_posts/2025-01-10-notes/2025-01-10-notes.html",
    "href": "Kaplan_posts/2025-01-10-notes/2025-01-10-notes.html",
    "title": "Kaplan’s in-class notes for 2025-01-10",
    "section": "",
    "text": "Recent article about morning coffee and wellness\n\n“The study found that people who drank coffee in the morning had a lower risk of dying from cardiovascular disease and had a lower mortality risk than all-day coffee consumers—but the research could not prove whether coffee was the sole cause.”\n\n\nWhy look at both “dying from cardiovascular disease” and “mortality.”-\n“Lower risk.” How do you measure risk?\nWhat does “prove” mean in this context? Is proof an appropriate criterion for interpreting the study? What’s its purpose in this paragraph?\n\n\n“Dr Qi said further studies are needed to see if their findings could also be observed in other populations, adding: ‘We need clinical trials to test the potential impact of changing the time of day when people drink coffee.’”\n\n\nWhat is meant by “clinical trials?”\nWhy do they refer to the potential impact of changing the time of day when people drink coffee?\n\n\n“The researchers from Tulane University in New Orleans, looked at 40,725 adults who had taken part in the National Health and Nutrition Examination Survey in the US between 1999 and 2018.”\n\n\nWhat does 40,725—the sample size—tell you here?\n\n\n“The researchers found that morning coffee drinkers were 16% less likely to have died compared to those who did not drink coffee, and 31% less likely to have died from heart disease.”\n\n\nSuggest some mechanisms that might be consistent with this result but not imply that having more people drink coffee in the morning would reduce risk for those people."
  },
  {
    "objectID": "Kaplan_posts/2025-01-10-notes/2025-01-10-notes.html#critical-thinking-and-qr2",
    "href": "Kaplan_posts/2025-01-10-notes/2025-01-10-notes.html#critical-thinking-and-qr2",
    "title": "Kaplan’s in-class notes for 2025-01-10",
    "section": "",
    "text": "Recent article about morning coffee and wellness\n\n“The study found that people who drank coffee in the morning had a lower risk of dying from cardiovascular disease and had a lower mortality risk than all-day coffee consumers—but the research could not prove whether coffee was the sole cause.”\n\n\nWhy look at both “dying from cardiovascular disease” and “mortality.”-\n“Lower risk.” How do you measure risk?\nWhat does “prove” mean in this context? Is proof an appropriate criterion for interpreting the study? What’s its purpose in this paragraph?\n\n\n“Dr Qi said further studies are needed to see if their findings could also be observed in other populations, adding: ‘We need clinical trials to test the potential impact of changing the time of day when people drink coffee.’”\n\n\nWhat is meant by “clinical trials?”\nWhy do they refer to the potential impact of changing the time of day when people drink coffee?\n\n\n“The researchers from Tulane University in New Orleans, looked at 40,725 adults who had taken part in the National Health and Nutrition Examination Survey in the US between 1999 and 2018.”\n\n\nWhat does 40,725—the sample size—tell you here?\n\n\n“The researchers found that morning coffee drinkers were 16% less likely to have died compared to those who did not drink coffee, and 31% less likely to have died from heart disease.”\n\n\nSuggest some mechanisms that might be consistent with this result but not imply that having more people drink coffee in the morning would reduce risk for those people."
  },
  {
    "objectID": "Kaplan_posts/2025-01-10-notes/2025-01-10-notes.html#discussion-rosling-ch.-1",
    "href": "Kaplan_posts/2025-01-10-notes/2025-01-10-notes.html#discussion-rosling-ch.-1",
    "title": "Kaplan’s in-class notes for 2025-01-10",
    "section": "Discussion: Rosling Ch. 1",
    "text": "Discussion: Rosling Ch. 1\nReading questions\nAvoiding the “gap instinct”:\n\ndon’t look at averages outside of the context of variation."
  },
  {
    "objectID": "Kaplan_posts/2025-01-10-notes/2025-01-10-notes.html#discussion-spiegelhalter-ch.-1",
    "href": "Kaplan_posts/2025-01-10-notes/2025-01-10-notes.html#discussion-spiegelhalter-ch.-1",
    "title": "Kaplan’s in-class notes for 2025-01-10",
    "section": "Discussion: Spiegelhalter Ch. 1",
    "text": "Discussion: Spiegelhalter Ch. 1\nTable 1.1: Tabular format of data\nRelative vs absolute risk: bacon sandwich eating. 20% increase in risk. Compare to Fig 1.4\n\nRisk ratio: risk of cancer with bacon divided by without bacon\nCan’t add together relative risks. Instead, multiply them.\n\nExplain (briefly) what odds is about, why we need it.\nReading questions"
  },
  {
    "objectID": "Kaplan_posts/2025-01-10-notes/2025-01-10-notes.html#introduction-to-computing",
    "href": "Kaplan_posts/2025-01-10-notes/2025-01-10-notes.html#introduction-to-computing",
    "title": "Kaplan’s in-class notes for 2025-01-10",
    "section": "Introduction to computing",
    "text": "Introduction to computing\nYour window to R computing will be through R-chunks embedded in documents. Each box provides full access to all the capabilities in R, although ours have been set up to provide seamless access only to the tools we’ll need in QR2.\nIn writing documents, I use a different system.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nOur data will be stored in data frames.\n\nEach frame has a name, e.g. Galton and is already available to you in\nOccasionally we will load in data from other sources. You’ll be shown how to do this, but you don’t need to master the methods.\n\n\n\n\nExample: Galton\n\nUsually we don’t want to look at the whole thing in print.\nPipe syntax, function names, parentheses\nnames(), head(), nrow(). Specimens and variables.\nInputs (from pipe) and arguments (in parentheses)\n\nRandom samples. We discussed this on Tuesday. Can take a random sample from a data frame with take_sample(n=...).\nUse wrangling operations to construct new data frames from an existing one. Vocabulary: filter(), mutate(), summarize(), arrange(), select(), grouping (with .by=).\n\nWhat is a database?\nWhat is a “relational” database.\n\nWhat names/words might you have heard that relate to relational databases.\nWhen were they invented.\nHow big a sector of the economy? How does one measure “how big” when talking about the economy.\n\n\nCalculate the amount of variation in a quantitative variable. Vocabulary: variance, var(), “standard deviation.”\nConstruct and interpret annotated point plots from a data frame. Vocabulary: point_plot(), tilde expression, response variable, explanatory variable, covariate, facet, trend, model, mapping, violin.\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page."
  },
  {
    "objectID": "Kaplan_posts/2025-02-18-notes/2025-02-18-notes.html",
    "href": "Kaplan_posts/2025-02-18-notes/2025-02-18-notes.html",
    "title": "Kaplan’s in-class notes for 2025-02-18",
    "section": "",
    "text": "Look at Quiz Question 4 with an eye toward the confidence interval rather than just the coefficient.\nQUIZ QUESTION 4: We’re going to make some models of lung cancer in the Birdkeepers, looking at smoking and age as risk factors for cancer. CD is the number of cigarettes smoked per day (0 for a non-smoker). YR is the number of years that the person has smoked (0 for a non-smoker), and AG is the persons age. cancer is a binary variable with value 1 indicating cancer and 0 indicating otherwise. datawith the same response variable.\n\nBirdkeepers |&gt; model_train(cancer ~ CD) |&gt; \n  conf_interval()\n\n# A tibble: 2 × 4\n  term           .lwr   .coef    .upr\n  &lt;chr&gt;         &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n1 (Intercept) -2.32   -1.54   -0.829 \n2 CD           0.0143  0.0511  0.0909\n\n\n\nInterpret the output. Does the CD coefficient of 0.05 mean that smoking an additional cigarette per day increases or decreases or doesn’t affect the odds of getting lung cancer?\n\nSomeone claims that the coefficient 0.05 is misleading and that what really matters is how many years the person has been smoking.\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\n\nBirdkeepers |&gt; model_train(cancer ~ CD + YR) |&gt; \n  conf_interval()\n\n# A tibble: 3 × 4\n  term           .lwr   .coef    .upr\n  &lt;chr&gt;         &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n1 (Intercept) -3.70   -2.46   -1.43  \n2 CD          -0.0211  0.0238  0.0689\n3 YR           0.0129  0.0460  0.0816\n\n\n\nInterpret the coefficients 0.023 and 0.046 from the model. Does taking into account the years of smoking increase, decrease, or leave alone the effect of smoking an additional cigarette per day.\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\n\nAccording to the cancer ~ CD + YR model, if a person could reduce their smoking consumption by 20 cigarettes per day, how many more years could they keep on smoking to arrive at the same risk of lung cancer?\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\nYou realize that anyone who smokes an extra year must also be an extra year older. Age increases the risk of illness so you theorize that some of the credit being given to years of smoking is really about “years of living.” So you try a third model …\n\nBirdkeepers |&gt; \n  model_train(cancer ~ CD + YR + AG) |&gt; \n  conf_interval()\n\n# A tibble: 4 × 4\n  term           .lwr   .coef     .upr\n  &lt;chr&gt;         &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept) -2.73    0.373  3.45    \n2 CD          -0.0306  0.0163 0.0630  \n3 YR           0.0276  0.0719 0.126   \n4 AG          -0.131  -0.0619 0.000984\n\n\n\nWe have a technical word for an explanatory variable introduced to a model to place another explanatory variable in context. What is that word?\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\n\nDo the coefficients of the cancer ~ CD + YR + AG model supportt the hypothesis that some of the effect of years of smoking is really attributable to ordinary aging? Explain briefly.\n\n\n\n\n\n\n\nAnswer"
  },
  {
    "objectID": "Kaplan_posts/2025-02-18-notes/2025-02-18-notes.html#rosling-ch.-3-the-straight-line-instinct",
    "href": "Kaplan_posts/2025-02-18-notes/2025-02-18-notes.html#rosling-ch.-3-the-straight-line-instinct",
    "title": "Kaplan’s in-class notes for 2025-02-18",
    "section": "",
    "text": "Interpolation and extrapolation. Why extrapolation is hard. Why linear interpolation is good."
  },
  {
    "objectID": "Kaplan_posts/2025-02-18-notes/2025-02-18-notes.html#regression-techniques",
    "href": "Kaplan_posts/2025-02-18-notes/2025-02-18-notes.html#regression-techniques",
    "title": "Kaplan’s in-class notes for 2025-02-18",
    "section": "Regression techniques",
    "text": "Regression techniques"
  },
  {
    "objectID": "Kaplan_posts/2025-02-18-notes/2025-02-18-notes.html#computing",
    "href": "Kaplan_posts/2025-02-18-notes/2025-02-18-notes.html#computing",
    "title": "Kaplan’s in-class notes for 2025-02-18",
    "section": "Computing",
    "text": "Computing\nContrast prediction with regression: Same model training, but different goal: to understand what the connections are in a system.\n\nDifferent criteria apply to the selection of explanatory variables.\nWe are interested in details of the model, not just the prediction. But the form will look much like he prediction interval for quantitative response variables.\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nShow also “confidence band”\n\n\nHmod1 &lt;- Galton |&gt;\n  model_train(height ~ mother + father + sex) \nHmod1 |&gt; conf_interval()\n\n# A tibble: 4 × 4\n  term         .lwr  .coef   .upr\n  &lt;chr&gt;       &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 (Intercept) 9.95  15.3   20.7  \n2 mother      0.260  0.321  0.383\n3 father      0.349  0.406  0.463\n4 sexM        4.94   5.23   5.51 \n\nHmod1 |&gt; \n  model_eval(Galton) |&gt;\n  summarize(sd(.resid))\n\n  sd(.resid)\n1   2.150721\n\n\n\nEvaluation of model formula\nCompare by-hand evaluation at given inputs\n\n\nWhy do statistics book defer prediction intervals?\nWhy statistics books teach confidence intervals before prediction intervals: Prediction intervals look bad.\n\nThe residuals set the size (mainly) of the prediction intervals.\nThe prediction bands cover about 95% of the data. Drawing them by hand from a point plot.\n\nConfidence intervals get smaller/tighter the more data you have.\nSimulation: We’ll simulate the height data so that we can generate as much (made-up) data as we like.\n\nGalton |&gt; \n  summarize(mean(mother), sd(mother), \n            mean(father), sd(father))\n\n  mean(mother) sd(mother) mean(father) sd(father)\n1     64.08441   2.307025     69.23285   2.470256\n\n\n\nHsim &lt;- datasim_make(\n  mom &lt;- rnorm(n, mean=64, sd = 2.3),\n  dad &lt;- rnorm(n, mean=69, sd = 2.5),\n  sex &lt;- categorical(n, \"b\", \"g\"),\n  height &lt;- 15.34 + 0.32*mom + 0.41*dad + \n    cat2value(sex, b=5.22, g = 0) +\n    2.15 * rnorm(n)\n)\nHsim |&gt; take_sample(n = 5)\n\n# A tibble: 5 × 4\n    mom   dad sex   height\n  &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;  &lt;dbl&gt;\n1  63.4  67.0 g       64.2\n2  63.9  68.8 b       71.1\n3  60.9  69.8 b       67.2\n4  62.7  67.8 b       69.4\n5  64.8  69.9 g       63.1\n\n\nPrediction intervals don’t change so much with sample size:\n\nHsim |&gt; \n  take_sample(n = 100) |&gt;\n  point_plot(height ~ mom, annot = \"model\", interval = \"prediction\")\n\n\n\n\n\n\n\nHsim |&gt; \n  take_sample(n = 10000) |&gt;\n  point_plot(height ~ mom, annot = \"model\", interval = \"prediction\")\n\n\n\n\n\n\n\n\nNow change the interval to \"confidence\" and see how nice the graph looks."
  },
  {
    "objectID": "Kaplan_posts/2025-02-18-notes/2025-02-18-notes.html#r2",
    "href": "Kaplan_posts/2025-02-18-notes/2025-02-18-notes.html#r2",
    "title": "Kaplan’s in-class notes for 2025-02-18",
    "section": "R2",
    "text": "R2\nAnother model summary: How much of the variation in the response variable has been explained.\n\nHmod1 |&gt; R2()\n\n    n k  Rsquared        F     adjR2 p df.num df.denom\n1 898 3 0.6396752 529.0317 0.6384661 0      3      894"
  },
  {
    "objectID": "Kaplan_posts/2025-02-18-notes/2025-02-18-notes.html#covariates",
    "href": "Kaplan_posts/2025-02-18-notes/2025-02-18-notes.html#covariates",
    "title": "Kaplan’s in-class notes for 2025-02-18",
    "section": "Covariates",
    "text": "Covariates\nShow coefficients change as we add in covariates:\n\nGalton |&gt; \n  model_train(height ~ mother) |&gt;\n  conf_interval()\n\n# A tibble: 2 × 4\n  term          .lwr  .coef   .upr\n  &lt;chr&gt;        &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 (Intercept) 40.3   46.7   53.1  \n2 mother       0.213  0.313  0.413\n\nGalton |&gt; \n  model_train(height ~ mother + father + sex) |&gt;\n  conf_interval()\n\n# A tibble: 4 × 4\n  term         .lwr  .coef   .upr\n  &lt;chr&gt;       &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 (Intercept) 9.95  15.3   20.7  \n2 mother      0.260  0.321  0.383\n3 father      0.349  0.406  0.463\n4 sexM        4.94   5.23   5.51 \n\n\nThe coefficient tells the contribution of its variable in the context of the other explanatory variable.\nA more dramatic example:\nIn class I asked you what states have the highest SAT scores on average. Someone suggested that two top states might be California and Massachusetts.\nIn the spirit of Rosler, let’s look at the data:\n\nSAT |&gt; \n  select(state, sat) |&gt;\n  arrange(desc(sat)) |&gt;\n  head(10)\n\n          state  sat\n1  North Dakota 1107\n2          Iowa 1099\n3     Minnesota 1085\n4          Utah 1076\n5     Wisconsin 1073\n6  South Dakota 1068\n7        Kansas 1060\n8      Nebraska 1050\n9      Illinois 1048\n10     Missouri 1045\n\n\nLook at the worst states. How should I modify the command?\n\nSAT |&gt;\n  select(state, sat) |&gt;\n  filter(state %in% c(\"California\", \"Massachusetts\"))\n\n          state sat\n1    California 902\n2 Massachusetts 907\n\n\nLet’s look at some explanatory variables that might account for SAT score: expenditures, class size, teacher salaries, ???\n\nSAT |&gt; \n  model_train(sat ~ expend) |&gt;\n  conf_interval()\n\n# A tibble: 2 × 4\n  term          .lwr  .coef    .upr\n  &lt;chr&gt;        &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;\n1 (Intercept) 1000.  1089.  1179.  \n2 expend       -35.6  -20.9   -6.16\n\n\nInterpret the coefficient. “Effect size” but don’t necessarily put a causal interpretation on it.\nA covariate: class size?\n\nSAT |&gt; \n  model_train(sat ~ expend + ratio) |&gt;\n  conf_interval()\n\n# A tibble: 3 × 4\n  term         .lwr   .coef    .upr\n  &lt;chr&gt;       &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n1 (Intercept) 919.  1136.   1353.  \n2 expend      -38.3  -22.3    -6.30\n3 ratio       -11.9   -2.29    7.33\n\n\nTwo graphs:\n\nSAT |&gt;\n  point_plot(sat ~ expend, annot = \"model\")\n\n\n\n\n\n\n\nSAT |&gt;\n  point_plot(sat ~ expend + frac, annot = \"model\")\n\n\n\n\n\n\n\n\nThe second plot looks at the relationship between expenditure and SAT scores, adjusting for the fraction of students who take the SAT.\n\nMOVE THIS TO HYPOTHESIS TESTING when those note files have been created.\nAn article about reporting baseline characteristics in clinical trials, arguing that to avoid the trade-off between a data-mining kind of choice of covariates and not including them as needed, best just to plan for adjustment and do it without looking at the baseline balance. Maybe something for hypothesis-testing section: https://onlinelibrary.wiley.com/doi/abs/10.1002/sim.4780131703"
  },
  {
    "objectID": "Kaplan_posts/2025-01-17-notes/2025-01-17-notes.html",
    "href": "Kaplan_posts/2025-01-17-notes/2025-01-17-notes.html",
    "title": "Kaplan’s in-class notes for 2025-01-17",
    "section": "",
    "text": "I’ve posted a schedule for the whole term. It’s a schedule, not a contract.\nI’m going to start looking at reading questions, exercises, and such on a regular schedule.\n\nIf you are still shakey about submitting your work, let’s fix it!\nI want to see submissions from all of you on every document from now on.\n\nGrading. The STEM faculty have set a rule for instructors about grading.\n\n\nHalf or more of graded content should be presented and done in a classroom setting.\n\n\nI’m thinking to make this about 50%. Split more or less evenly between class participation and short (15 min) inclass quizzes roughly once a week.\nI won’t necessarily tell you in advance about such a quiz. If you need to make one up, we can do it in my office.\n\nWe’ll have a Quiz on Tuesday. No need to worry about it."
  },
  {
    "objectID": "Kaplan_posts/2025-01-17-notes/2025-01-17-notes.html#administration",
    "href": "Kaplan_posts/2025-01-17-notes/2025-01-17-notes.html#administration",
    "title": "Kaplan’s in-class notes for 2025-01-17",
    "section": "",
    "text": "I’ve posted a schedule for the whole term. It’s a schedule, not a contract.\nI’m going to start looking at reading questions, exercises, and such on a regular schedule.\n\nIf you are still shakey about submitting your work, let’s fix it!\nI want to see submissions from all of you on every document from now on.\n\nGrading. The STEM faculty have set a rule for instructors about grading.\n\n\nHalf or more of graded content should be presented and done in a classroom setting.\n\n\nI’m thinking to make this about 50%. Split more or less evenly between class participation and short (15 min) inclass quizzes roughly once a week.\nI won’t necessarily tell you in advance about such a quiz. If you need to make one up, we can do it in my office.\n\nWe’ll have a Quiz on Tuesday. No need to worry about it."
  },
  {
    "objectID": "Kaplan_posts/2025-01-17-notes/2025-01-17-notes.html#spiegelhalter-ch.-3",
    "href": "Kaplan_posts/2025-01-17-notes/2025-01-17-notes.html#spiegelhalter-ch.-3",
    "title": "Kaplan’s in-class notes for 2025-01-17",
    "section": "Spiegelhalter Ch. 3",
    "text": "Spiegelhalter Ch. 3\nDeduction versus induction\n\ninduction and “proof”\n\nMeaning of “prove”. We’re hampered by the mathematical use of proof as a “correct deduction.”\n\nAre all deductions either correct or incorrect? Can all mathematical truths be proved.\n\nInternal and external validity\n\n“Internal validity is the quality of the study based on how truthful and unbiased the study is. External validity is testing the effects of chemicals on mice and relating that to humans to have the same impact. The distinguishing factor is that internal validity has to do with the study process, and external validity has to do with who or what is applicable based on the data.”\n\n\nLatin probare ‘to test, prove’. Current senses of the verb date from the late 19th century.\n\nDavid Hume: “The problem of induction” Hume didn’t even know what science is.\n\nThe problem of induction is a philosophical problem that questions the rationality of predictions about unobserved things based on previous observations. These inferences from the observed to the unobserved are known as “inductive inferences”. David Hume, who first formulated the problem in 1739, argued that there is no non-circular way to justify inductive inferences, while he acknowledged that everyone does and must make such inferences. — Wikipedia\n\n\nbell-shaped curve, aka “normal distribution”. In scientific circles: the “gaussian function”\n\nThere is nothing normal about it.\nIt’s relevant when there are lots of things contributing to each observation. E.g. height, means, ….\n\ndon’t mistake it for a law of Nature.\n\nAge, BP and BMI\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n“as diabetes is a categorical. Categorical is discrete, and “normal” is something applied only to continuous data.”"
  },
  {
    "objectID": "Kaplan_posts/2025-01-17-notes/2025-01-17-notes.html#rossling-ch.-4",
    "href": "Kaplan_posts/2025-01-17-notes/2025-01-17-notes.html#rossling-ch.-4",
    "title": "Kaplan’s in-class notes for 2025-01-17",
    "section": "Rossling Ch. 4",
    "text": "Rossling Ch. 4\nBe aware of biases in thinking: what the economists sometimes call “irrationality.”\n\nIMHO, all that you can expect from an educated person is that they are aware of such biases, and can exert mental effort to compensate for them.\n\n\n“The ”Attention Filter” refers to how humans are naturally inclined to focus more on shocking information, easily forgetting the less shocking news, leaving individuals with an unbalanced perspective of events. Selective reporting by media acts on this human instinct by choosing what the audience remembers.”\n\n\nFear instinct\n\n\n“Fear Instinct is that natural fear that human beings possess whether it be heights, spiders, snakes, ect. These are instinctive because when everyone was at level 1, they were constant threats to daily survival. To those who are still on level 1, the fear instinct is helpful, but to those on level 4, it can be considered a phobia. Phobias can often highjack ones life and make them think they are in danger when really there is no danger at all.”\n\n\nWhy have natural disasters become less dangerous?\n\n\n“Rosling says that the number of casualties from natural disasters decreased over the last century because of advancements in technology.”\n\n\nWhat are some examples?\n\n\nFear factors\nAnchoring (from Kahnemann and Tverskey)"
  },
  {
    "objectID": "Kaplan_posts/2025-01-17-notes/2025-01-17-notes.html#what-is-a-prediction",
    "href": "Kaplan_posts/2025-01-17-notes/2025-01-17-notes.html#what-is-a-prediction",
    "title": "Kaplan’s in-class notes for 2025-01-17",
    "section": "What is a prediction?",
    "text": "What is a prediction?\n\nWe extend the everyday meaning: Saying something about the as-yet-unknown value of a variable based on known values for other variables.\n\nNot just “Will you get sick?” but “Are you sick now?”\nClassifiers versus regression models.\n\nThe proper form for a prediction.\nPrediction and risk\nRisk and “risk factors”\nFraming for the statistical prediction problem\n\ntraining data\nperformance\ntesting data and related techniques, e.g. cross-validation\n\nStatistical prediction is a compromise between what we already know and what the data tell us.\n\nImportant problem in critical thinking generally: How to combine our beliefs with our observations?"
  },
  {
    "objectID": "Kaplan_posts/2025-01-17-notes/2025-01-17-notes.html#building-a-prediction-model",
    "href": "Kaplan_posts/2025-01-17-notes/2025-01-17-notes.html#building-a-prediction-model",
    "title": "Kaplan’s in-class notes for 2025-01-17",
    "section": "Building a prediction model",
    "text": "Building a prediction model\nQuestion: I want to predict a person’s age. What inputs can I use?\nQuestion: I want to predict whether an airline passenger is a terrorist? What inputs can I use?\n\nSecurity theater.\n\nExample in computing tutorials (which have not been assigned): Predicting a person’s height.\n\nStep 1: Choose the response variable whose value you want to predict for a given specimen.\nStep 2: Choose explanatory variables that will be available at the time you need to make the prediction.\nStep 3: Use the training data to develop an association between\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page."
  },
  {
    "objectID": "Kaplan_posts/2025-01-28-notes/2025-01-28-notes.html",
    "href": "Kaplan_posts/2025-01-28-notes/2025-01-28-notes.html",
    "title": "Kaplan’s in-class notes for 2025-01-28",
    "section": "",
    "text": "Question 3: What is a “unit of observation” in a data frame?\nQuestion 4: Distinction between response and explanatory variable.\n\nIt is the human modeler’s choice. That choice depends on the goals motivating your analysis of data.\nThere need not be an causal relationship between the explanatory variables and the response variable.\n\nIn prediction, often association is all that’s needed, and causality, if any, might even go in the opposite direction.\nIn Block 5, we’ll take on the issue of causality directly. For now, just understand that causality is a goal chiefly when we want to anticipate what change in output will result from a given change in input.\n\n\nQuestion 5. Make this plot …\n\n\n\n\n\n\n\n\n\nQuestion 6: In what ways is this table not a data frame:\n\n\n\nbrightness\ndistance\nred shift\ntype\n\n\n\n\n3\n2 parsec\n5%\nM\n\n\n3\n6 light-year\n0.07\nM\n\n\n2\n8\n.09\nS\n\n\n1.3\n19.2\n10%\nS\n\n\ntotal\n9.3\n35.2\n2M 2S\n\n\n\nA real-world example of this: people in the DOD Source\n\nHow to format this into a proper data frame. (Hint: Many ways.)\n\nStart with: What is the unit of observation?\n\nImagine what it would look like if the unit of observation were a single person.\n\nQuestion 7:\n\nresponse is always on vertical axis\nthe first explanatory variable is always on the x axis.\nannot = \"violin\" sketches a nice picture of the distribution.\n\nShow what happens as \\(n\\) gets bigger.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page."
  },
  {
    "objectID": "Kaplan_posts/2025-01-28-notes/2025-01-28-notes.html#go-over-quiz.",
    "href": "Kaplan_posts/2025-01-28-notes/2025-01-28-notes.html#go-over-quiz.",
    "title": "Kaplan’s in-class notes for 2025-01-28",
    "section": "",
    "text": "Question 3: What is a “unit of observation” in a data frame?\nQuestion 4: Distinction between response and explanatory variable.\n\nIt is the human modeler’s choice. That choice depends on the goals motivating your analysis of data.\nThere need not be an causal relationship between the explanatory variables and the response variable.\n\nIn prediction, often association is all that’s needed, and causality, if any, might even go in the opposite direction.\nIn Block 5, we’ll take on the issue of causality directly. For now, just understand that causality is a goal chiefly when we want to anticipate what change in output will result from a given change in input.\n\n\nQuestion 5. Make this plot …\n\n\n\n\n\n\n\n\n\nQuestion 6: In what ways is this table not a data frame:\n\n\n\nbrightness\ndistance\nred shift\ntype\n\n\n\n\n3\n2 parsec\n5%\nM\n\n\n3\n6 light-year\n0.07\nM\n\n\n2\n8\n.09\nS\n\n\n1.3\n19.2\n10%\nS\n\n\ntotal\n9.3\n35.2\n2M 2S\n\n\n\nA real-world example of this: people in the DOD Source\n\nHow to format this into a proper data frame. (Hint: Many ways.)\n\nStart with: What is the unit of observation?\n\nImagine what it would look like if the unit of observation were a single person.\n\nQuestion 7:\n\nresponse is always on vertical axis\nthe first explanatory variable is always on the x axis.\nannot = \"violin\" sketches a nice picture of the distribution.\n\nShow what happens as \\(n\\) gets bigger.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page."
  },
  {
    "objectID": "Kaplan_posts/2025-01-28-notes/2025-01-28-notes.html#census-project",
    "href": "Kaplan_posts/2025-01-28-notes/2025-01-28-notes.html#census-project",
    "title": "Kaplan’s in-class notes for 2025-01-28",
    "section": "Census project",
    "text": "Census project\n1940 Population schedule form and codebook\nDocuments for Travis County, Texas\nA population schedule from Austin\nOptaining enumeration districts: https://stevemorse.org/census/\nhttps://stevemorse.org/census/schedules.php\n\nTravis; Travis (Austin) (227-1 to 227-7, 227-8 to 227-26 )\nTravis (Austin) (227-27 to 227-47 )\nTravis (Austin) (227-48 to 227-71 )"
  },
  {
    "objectID": "Kaplan_posts/2025-01-28-notes/2025-01-28-notes.html#spiegelhalter-chaps-5-and-6",
    "href": "Kaplan_posts/2025-01-28-notes/2025-01-28-notes.html#spiegelhalter-chaps-5-and-6",
    "title": "Kaplan’s in-class notes for 2025-01-28",
    "section": "Spiegelhalter Chaps 5 and 6",
    "text": "Spiegelhalter Chaps 5 and 6\n\nSpeed cameras\n\nNo, he does not think speed cameras are effective. He thinks that accidents would have returned back to normal anyways, without intervention. Since a high accident rate is abnormal, “regression to the mean” will lead the rate to return to normal naturally and cannot be attributed to the installment of speed cameras. []\nSpeigelhalter says that speed cameras are less effective than they seem because of regression to the mean. Regression to the mean describes the fact that when something is unusual (such as higher than mean crashes) it is usually just because of luck, therefore the luck is bound to run out and the expected result is that it return closer to mean levels. If speed cameras are put in in response to unusually high crash rates, and then there are fewer crashes it’s probably not due to the camera but just because the bad luck has run out.\n\n[The question is how to separate out the effects of “regression to the mean” from the actual effect of the intervention. HOW CAN WE DO THIS?]\n3.Likely not because of their tendency to be put in places after wrecks. The Camera was placed after an extreme event and it reports normal traffic which makes it seem like the rode got safer when in reality the rode was just returning to normal after the wreck, a regression to the mean.\n\nSpeed cameras are only effect about a third of the time, most of the time when speed cameras are put in place it is to reduce traffic violations in that area, but regression to the mean suggest that the speed cameras weren’t the reason at all. Regression to the mean is when things gradually shift towards the mean, regardless of outside factors.” They can be effective, but it is important to perform a regression to determine what variable is causing the change.\nAccording to Spiegelhalter, speed cameras are not practical all the time. A history of bad luck in accidents resulting in the implementation of the speed camera does not mean the lower rate of accidents now is due to the camera. It could be due to the accident rates dropping back to the average rate when they occur. Spiegelhalter points out that we give intervention the credit when it could be a reversion to the average (regression to the mean).”\nYes, speed cameras are two-thirds effective, this was determined by modeling a regression that shows a tendency for them to work.\nThere is evidence that speed cameras can improve safety, but Spiegelhalter acknowledges that there is evidence suggesting that the “effectiveness” of speed cameras is often overstated due to statistical principles. Regarding “regression to the mean”, speed cameras are installed at locations that have recently experienced a high number of accidents but this decision is often an assumption. As a result, the reduction in accidents after installing speed cameras might not be entirely due to the cameras but instead due to regression to the mean.\npossibly, though if installed because of a recent high accident rate, may not be the cause of the reduction (or have their effect overestimated) as the accident rate would have likely regressed to the mean regardless of the intervention\nSpiegelhalter acknowledges that speed cameras are often associated with reducing accidents at locations where they are installed. However, he points out that this observed decrease can partly be explained by regression to the mean. RTTM is important as places where cameras are located usually have higher than average accident rates and would have regressed back to the average rate without intervention.\n\n\n\nPlacebo effect\n\nThe placebo effect is a perceived and then actual benefit from people in the control group who think they are receiving treatment. This improvement can be seen as “regression to the mean” because these patients may have just had conditions that resolved themselves anyways.\n\n\n\nLogistic regression\nIt makes a difference where you enter an answer!\n\nSubjects are often selected for their extreme symptoms, which may naturally regress to the mean due to random variation and not the treatment. regression to the mean and the psychosomatic placebo effect could both confound the effect of treatment and make the illusion of efficacy.\n\nGo through 5-before questions and 6-follow-up.\n[We got to here on Tuesday.]"
  },
  {
    "objectID": "Kaplan_posts/2025-01-28-notes/2025-01-28-notes.html#computing",
    "href": "Kaplan_posts/2025-01-28-notes/2025-01-28-notes.html#computing",
    "title": "Kaplan’s in-class notes for 2025-01-28",
    "section": "Computing",
    "text": "Computing\nContrast prediction with regression: Same model training, but different goal: to understand what the connections are in a system.\n\nDifferent criteria apply to the selection of explanatory variables.\nWe are interested in details of the model, not just the prediction. But the form will look much like he prediction interval for quantitative response variables.\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nShow also “confidence band”\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page."
  },
  {
    "objectID": "Kaplan_posts/2025-01-24-notes/2025-01-24-notes.html",
    "href": "Kaplan_posts/2025-01-24-notes/2025-01-24-notes.html",
    "title": "Kaplan’s in-class notes for 2025-01-24",
    "section": "",
    "text": "Odds, log odds, logistic regression\nPerhaps drivers of mortality a la USAFA example\n\nQuantitative outcome variable\nPrediction interval and how it corresponds to the “normal” curve.\nMean square error"
  },
  {
    "objectID": "Kaplan_posts/2025-02-10-notes/2025-02-10-notes.html",
    "href": "Kaplan_posts/2025-02-10-notes/2025-02-10-notes.html",
    "title": "Kaplan’s in-class notes for 2025-02-10",
    "section": "",
    "text": "Interpolation and extrapolation. Why extrapolation is hard. Why linear interpolation is good."
  },
  {
    "objectID": "Kaplan_posts/2025-02-10-notes/2025-02-10-notes.html#rosling-ch.-3-the-straight-line-instinct",
    "href": "Kaplan_posts/2025-02-10-notes/2025-02-10-notes.html#rosling-ch.-3-the-straight-line-instinct",
    "title": "Kaplan’s in-class notes for 2025-02-10",
    "section": "",
    "text": "Interpolation and extrapolation. Why extrapolation is hard. Why linear interpolation is good."
  },
  {
    "objectID": "Kaplan_posts/2025-02-10-notes/2025-02-10-notes.html#regression-techniques",
    "href": "Kaplan_posts/2025-02-10-notes/2025-02-10-notes.html#regression-techniques",
    "title": "Kaplan’s in-class notes for 2025-02-10",
    "section": "Regression techniques",
    "text": "Regression techniques"
  },
  {
    "objectID": "Kaplan_posts/2025-02-10-notes/2025-02-10-notes.html#computing",
    "href": "Kaplan_posts/2025-02-10-notes/2025-02-10-notes.html#computing",
    "title": "Kaplan’s in-class notes for 2025-02-10",
    "section": "Computing",
    "text": "Computing\nContrast prediction with regression: Same model training, but different goal: to understand what the connections are in a system.\n\nDifferent criteria apply to the selection of explanatory variables.\nWe are interested in details of the model, not just the prediction. But the form will look much like he prediction interval for quantitative response variables.\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nShow also “confidence band”\n\n\nHmod1 &lt;- Galton |&gt;\n  model_train(height ~ mother + father + sex) \nHmod1 |&gt; conf_interval()\n\n# A tibble: 4 × 4\n  term         .lwr  .coef   .upr\n  &lt;chr&gt;       &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 (Intercept) 9.95  15.3   20.7  \n2 mother      0.260  0.321  0.383\n3 father      0.349  0.406  0.463\n4 sexM        4.94   5.23   5.51 \n\nHmod1 |&gt; \n  model_eval(Galton) |&gt;\n  summarize(sd(.resid))\n\n  sd(.resid)\n1   2.150721\n\n\n\nEvaluation of model formula\nCompare by-hand evaluation at given inputs\n\n\nWhy do statistics book defer prediction intervals?\nWhy statistics books teach confidence intervals before prediction intervals: Prediction intervals look bad.\n\nThe residuals set the size (mainly) of the prediction intervals.\nThe prediction bands cover about 95% of the data. Drawing them by hand from a point plot.\n\nConfidence intervals get smaller/tighter the more data you have.\nSimulation: We’ll simulate the height data so that we can generate as much (made-up) data as we like.\n\nGalton |&gt; \n  summarize(mean(mother), sd(mother), \n            mean(father), sd(father))\n\n  mean(mother) sd(mother) mean(father) sd(father)\n1     64.08441   2.307025     69.23285   2.470256\n\n\n\nHsim &lt;- datasim_make(\n  mom &lt;- rnorm(n, mean=64, sd = 2.3),\n  dad &lt;- rnorm(n, mean=69, sd = 2.5),\n  sex &lt;- categorical(n, \"b\", \"g\"),\n  height &lt;- 15.34 + 0.32*mom + 0.41*dad + \n    cat2value(sex, b=5.22, g = 0) +\n    2.15 * rnorm(n)\n)\nHsim |&gt; take_sample(n = 5)\n\n# A tibble: 5 × 4\n    mom   dad sex   height\n  &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;  &lt;dbl&gt;\n1  65.3  69.0 g       63.6\n2  62.1  67.3 b       68.3\n3  63.3  65.3 b       71.5\n4  64.3  71.5 b       70.3\n5  63.9  66.7 g       61.1\n\n\nPrediction intervals don’t change so much with sample size:\n\nHsim |&gt; \n  take_sample(n = 100) |&gt;\n  point_plot(height ~ mom, annot = \"model\", interval = \"prediction\")\n\n\n\n\n\n\n\nHsim |&gt; \n  take_sample(n = 10000) |&gt;\n  point_plot(height ~ mom, annot = \"model\", interval = \"prediction\")\n\n\n\n\n\n\n\n\nNow change the interval to \"confidence\" and see how nice the graph looks."
  },
  {
    "objectID": "Kaplan_posts/2025-02-10-notes/2025-02-10-notes.html#r2",
    "href": "Kaplan_posts/2025-02-10-notes/2025-02-10-notes.html#r2",
    "title": "Kaplan’s in-class notes for 2025-02-10",
    "section": "R2",
    "text": "R2\nAnother model summary: How much of the variation in the response variable has been explained.\n\nHmod1 |&gt; R2()\n\n    n k  Rsquared        F     adjR2 p df.num df.denom\n1 898 3 0.6396752 529.0317 0.6384661 0      3      894"
  },
  {
    "objectID": "Kaplan_posts/2025-02-10-notes/2025-02-10-notes.html#covariates",
    "href": "Kaplan_posts/2025-02-10-notes/2025-02-10-notes.html#covariates",
    "title": "Kaplan’s in-class notes for 2025-02-10",
    "section": "Covariates",
    "text": "Covariates\nShow coefficients change as we add in covariates:\n\nGalton |&gt; \n  model_train(height ~ mother) |&gt;\n  conf_interval()\n\n# A tibble: 2 × 4\n  term          .lwr  .coef   .upr\n  &lt;chr&gt;        &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 (Intercept) 40.3   46.7   53.1  \n2 mother       0.213  0.313  0.413\n\nGalton |&gt; \n  model_train(height ~ mother + father + sex) |&gt;\n  conf_interval()\n\n# A tibble: 4 × 4\n  term         .lwr  .coef   .upr\n  &lt;chr&gt;       &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 (Intercept) 9.95  15.3   20.7  \n2 mother      0.260  0.321  0.383\n3 father      0.349  0.406  0.463\n4 sexM        4.94   5.23   5.51 \n\n\nThe coefficient tells the contribution of its variable in the context of the other explanatory variable.\nA more dramatic example:\nIn class I asked you what states have the highest SAT scores on average. Someone suggested that two top states might be California and Massachusetts.\nIn the spirit of Rosler, let’s look at the data:\n\nSAT |&gt; \n  select(state, sat) |&gt;\n  arrange(desc(sat)) |&gt;\n  head(10)\n\n          state  sat\n1  North Dakota 1107\n2          Iowa 1099\n3     Minnesota 1085\n4          Utah 1076\n5     Wisconsin 1073\n6  South Dakota 1068\n7        Kansas 1060\n8      Nebraska 1050\n9      Illinois 1048\n10     Missouri 1045\n\n\nLook at the worst states. How should I modify the command?\n\nSAT |&gt;\n  select(state, sat) |&gt;\n  filter(state %in% c(\"California\", \"Massachusetts\"))\n\n          state sat\n1    California 902\n2 Massachusetts 907\n\n\nLet’s look at some explanatory variables that might account for SAT score: expenditures, class size, teacher salaries, ???\n\nSAT |&gt; \n  model_train(sat ~ expend) |&gt;\n  conf_interval()\n\n# A tibble: 2 × 4\n  term          .lwr  .coef    .upr\n  &lt;chr&gt;        &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;\n1 (Intercept) 1000.  1089.  1179.  \n2 expend       -35.6  -20.9   -6.16\n\n\nInterpret the coefficient. “Effect size” but don’t necessarily put a causal interpretation on it.\nA covariate: class size?\n\nSAT |&gt; \n  model_train(sat ~ expend + ratio) |&gt;\n  conf_interval()\n\n# A tibble: 3 × 4\n  term         .lwr   .coef    .upr\n  &lt;chr&gt;       &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n1 (Intercept) 919.  1136.   1353.  \n2 expend      -38.3  -22.3    -6.30\n3 ratio       -11.9   -2.29    7.33\n\n\nTwo graphs:\n\nSAT |&gt;\n  point_plot(sat ~ expend, annot = \"model\")\n\n\n\n\n\n\n\nSAT |&gt;\n  point_plot(sat ~ expend + frac, annot = \"model\")\n\n\n\n\n\n\n\n\nThe second plot looks at the relationship between expenditure and SAT scores, adjusting for the fraction of students who take the SAT.\n\nMOVE THIS TO HYPOTHESIS TESTING when those note files have been created.\nAn article about reporting baseline characteristics in clinical trials, arguing that to avoid the trade-off between a data-mining kind of choice of covariates and not including them as needed, best just to plan for adjustment and do it without looking at the baseline balance. Maybe something for hypothesis-testing section: https://onlinelibrary.wiley.com/doi/abs/10.1002/sim.4780131703"
  },
  {
    "objectID": "Kaplan_posts/2025-01-21-notes/2025-01-21-notes.html",
    "href": "Kaplan_posts/2025-01-21-notes/2025-01-21-notes.html",
    "title": "Kaplan’s in-class notes for 2025-01-21",
    "section": "",
    "text": "Please enable JavaScript to experience the dynamic code cell content on this page.\nRisk and risk ratio\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nRisk ratio: 0.50 / 0.20 = 2.5. Birdkeeping is a “risk factor” for lung cancer.\nConstruct a one-node classification tree of lung cancer versus bird-keeping.\nConstruct the confusion matrix. False-positives and negatives, sensitivity, specificity, accuracy\n\nBuild a one-node classification tree based on YR. Meaning of the threshold. Receiver operating curve.\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page."
  },
  {
    "objectID": "Kaplan_posts/2025-01-21-notes/2025-01-21-notes.html#demonstrate-construction-of-a-classification-tree",
    "href": "Kaplan_posts/2025-01-21-notes/2025-01-21-notes.html#demonstrate-construction-of-a-classification-tree",
    "title": "Kaplan’s in-class notes for 2025-01-21",
    "section": "",
    "text": "Please enable JavaScript to experience the dynamic code cell content on this page.\nRisk and risk ratio\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nRisk ratio: 0.50 / 0.20 = 2.5. Birdkeeping is a “risk factor” for lung cancer.\nConstruct a one-node classification tree of lung cancer versus bird-keeping.\nConstruct the confusion matrix. False-positives and negatives, sensitivity, specificity, accuracy\n\nBuild a one-node classification tree based on YR. Meaning of the threshold. Receiver operating curve.\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page."
  },
  {
    "objectID": "Kaplan_posts/2025-01-21-notes/2025-01-21-notes.html#spiegelhalter-ch-6",
    "href": "Kaplan_posts/2025-01-21-notes/2025-01-21-notes.html#spiegelhalter-ch-6",
    "title": "Kaplan’s in-class notes for 2025-01-21",
    "section": "Spiegelhalter Ch 6",
    "text": "Spiegelhalter Ch 6\nLet’s be careful about language, especially “true”\n\nThe confusion matrix is essentially the data your algorithm incorrectly predicts.\n\n\nA confusion matrix is a table that classifies identification functions in machine learning.\n\n\nSensitivity is what is true that your algorithm also gets right, while the specificity is what is not true that your algorithm gets right."
  },
  {
    "objectID": "Kaplan_posts/2025-01-21-notes/2025-01-21-notes.html#work-with-the-start-of-tutorial-2",
    "href": "Kaplan_posts/2025-01-21-notes/2025-01-21-notes.html#work-with-the-start-of-tutorial-2",
    "title": "Kaplan’s in-class notes for 2025-01-21",
    "section": "Work with the start of Tutorial 2",
    "text": "Work with the start of Tutorial 2\nmodel_train() and model_eval().\nPrediction interval\nUse an example with a binary response variable."
  },
  {
    "objectID": "Kaplan_posts/2025-01-21-notes/2025-01-21-notes.html#quiz",
    "href": "Kaplan_posts/2025-01-21-notes/2025-01-21-notes.html#quiz",
    "title": "Kaplan’s in-class notes for 2025-01-21",
    "section": "Quiz",
    "text": "Quiz"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "QR2 Winter 2025",
    "section": "",
    "text": "Shared homepage\n\n\n\nThere are six sections of QR2 offered in the Winter 2025 term, taught by five different instructors. This page is a shared homepage for the course. To explain … the QR2 course has been constructed by the instructors and reflects our consensus vision of how best to achieve institution-wide objectives of core courses.\nIt’s impractical (and probably not optimal) for all the sections to be exactly identical. Different sections meet on different days and instructors are most effective when teaching in their individual styles.\nOn a day-to-day basis, you should refer to your instructor’s schedule linked to below."
  },
  {
    "objectID": "index.html#day-by-day-schedule",
    "href": "index.html#day-by-day-schedule",
    "title": "QR2 Winter 2025",
    "section": "Day-by-day schedule",
    "text": "Day-by-day schedule\nThere are six class sections for QR2. These follow a similar schedule, but naturally the viscisitudes and opportunities of classroom discussion lead to variation on a section-by-section basis. Accordingly, you should bookmark your section in your browser for easy access to the most up-to-date information.\n\nSection 1 (Prof. Ruth)\nSection 2 (Prof. Kolpakov)\nSection 3 (Prof. Kolpakov)\nSection 4 (Prof. Puelz)\nSection 5 (Prof. Overbey)\nSection 6 (Prof. Kaplan)"
  },
  {
    "objectID": "index.html#general-information",
    "href": "index.html#general-information",
    "title": "QR2 Winter 2025",
    "section": "General information",
    "text": "General information\nTextbooks:\n\nDavid Spiegelhalter, The Art of Statistics: Learning from Data\nHans Rossling, Factfulness\nQR 2 Computing Tutorials free and online\n\nComputing:\nOur data-science computing platform is embedded into each course document that involves computing. You do not need to install any special software; just an up-to-date web browser will do. No previous experience with technical software is required. For those interested in such things, the underlying platform is R along with the dplyr data-wrangling system, products that are extensively used professionally in data science.\nPolicies: See the syllabus page\nMuch of the required course work—reading questions, computing activities and exercises, and so on—is arranged for easy electronic submission. The uploading file uses your @uaustin.org email address."
  },
  {
    "objectID": "day-by-day/Kaplan-schedule.html",
    "href": "day-by-day/Kaplan-schedule.html",
    "title": "QR2 Prof. Kaplan’s Section",
    "section": "",
    "text": "Class Day 11 Tuesday 18 February\n\n\n\nBlock 4: Precision (cont.)\n\nNew readings\n\nSpiegelhalter Ch. 9 & reading questions\nComputing tutorial 4. Read before class session.\n\nStart work\n\nComputing Activities for Tutorial 4 due 25 Feb\n\nAssignments due:\n\nComputing Activities for Tutorial 3\n\nIn-class notes\n\n\n\n\n\n\n\n\n\nClass Day 12 Friday 21 February\n\n\n\nBlock 4: Precision (cont.)\n\nNew readings\n\nRosling Ch. 7 & reading questions\nSpiegelhalter Ch. 7 & reading questions (This was originally listed for Feb 4. The reading questions are now available.)\n\nStart work\nAssignments due:\n\nSpiegelhalter Ch. 9 post-discussion questions\nCensus project through Phase I Step 5. Your report will consist of starting to transcribe a couple of “person” and “household” rows into the two spreadsheets you created in Step 3. These do not need to be perfect; you may well want to reconfigure them later.\n\nIn-class notes\n\n\n\n\n\n\n\n\n\nClass Day 13 Tuesday 25 February\n\n\n\nBlock 5: Confounding & Causality\n\nNew readings\n\nSpiegelhalter Ch 4 & reading questions\nStart Computing tutorial 5. Read before class session.\n\nStart work\nAssignments due:\n\nComputing Activities for Tutorial 4\n\nIn-class notes\n\n\n\n\n\n\n\n\n\nClass Day 14 Friday 28 February\n\n\n\nBlock 5: Confounding & Causality (cont.)\n\nNew readings\n\nRosling Ch. 8 & reading questions\nComplete Computing tutorial 5. Read before class session.\n\nAssignment due:\n\nCensus project Complete transcription of your population schedule into your household and persons spreadsheets.\n\nStart work:\n\nCensus project Confer with two or three of your classmates to settle on a common format for your household and persons team spreadsheets. For Tuesday 4 March you have those team spreadsheets available (via link) to your instructor.\n\nIn-class notes\n\n\n\n\n\n\n\n\n\nClass Day 15 Tuesday 4 March\n\n\n\nBlock 5: Confounding & Causality (cont.)\n\nNew readings\n\nRosling Ch. 9 & reading questions\n\nStart work\nAssignments due:\n\nRespond to any feedback from the instructor combining the team spreadsheets into data frames that contain all-of-QR2 data. Diligence is required on your part.\n\nIn-class notes\n\n\n\n\n\n\n\n\n\nClass Day 16 Friday 7 March\n\n\n\nBlock 5: Confounding & Causality (cont.)\n\nNew readings\nStart work\n\nComputing Activities for Tutorial 5 (due 11 March)\n\nAssignments due:\n\nContinue to respond to any feedback from the instructor combining the team spreadsheets into data frames that contain all-of-QR2 data. Diligence is required on your part.\n\nIn-class notes\n\n\n\n\n\n\n\n\n\nClass Day 17 Tuesday 11 March\n\n\n\nBlock 6: Hypothetical Reasoning\n\nNew readings\n\nSpiegelhalter Ch. 11 & reading questions\nStart Computing tutorial 3. Read before class session.\n\nStart work\n\nYou should receive at this point the comprehensive, QR2-wide census data. Look through it and decide on the aspect of the data you are going to model.\n\nAssignments due:\n\nComputing Activities for Tutorial 5.\n\nIn-class notes\n\n\n\n\n\n\n\n\n\nClass Day 18 Friday 14 March\n\n\n\nBlock 6: Hypothetical Reasoning (cont.)\n\nNew readings\n\nRosling Ch. 10 & reading questions\nComplete Computing tutorial 6. Read before class session.\n\nAssignments due:\n\nSimple summaries of the all-of-QR2 Census data\nStatement of your “research topic” for your Census modeling project. This will include picking response and explanatory variables, including covariates. DO NOT AGONIZE about this. You are not going to be writing a PhD dissertation.\nRemember that your project presentation is due 21 March.\n\nIn-class notes\n\n\n\n\n\n\n\n\n\nClass Day 19 Tuesday 18 March\n\n\n\nBlock 6: Hypothetical Reasoning (cont.)\n\nNew readings\n\nSpiegelhalter Ch. 13 & reading questions\nSpiegelhalter Ch. 14 & reading questions\n\nStart work\n\nComputing Activities for Tutorial 6. Will be due March 24.\n\nAssignments due:\n\nRosling Ch. 10 post discussion questions\nPreliminary Census written report uploaded to Populi. Basic model summaries and graphics for your Census “research topic.” This will be the basis for your class presentation due on March 21. Formatting can be extremely rough; you will merely be cutting-and-pasting statistical summaries and graphics into an online document that you can reference fro your presentation.\n\nIn-class notes\n\n\n\n\n\n\n\n\n\nClass Day 20 Friday 21 March\n\n\n\n\nNew readings\n\n538 article about p-values\n\nAssignment due:\n\nPhase 4 Census Project presentation\nFor March 24, final Census Project report uploaded to Populi. This will be essentially your preliminary report along with some narrative description of your conclusions and identifying shortcomings that you would have worked on had this been a much more extensive project. It should be neat, but doesn’t need to be fancy! Take perhaps one hour to put it together.\nFor March 24, Computing Activities for Tutorial 3."
  },
  {
    "objectID": "day-by-day/Kaplan-schedule.html#near-future",
    "href": "day-by-day/Kaplan-schedule.html#near-future",
    "title": "QR2 Prof. Kaplan’s Section",
    "section": "",
    "text": "Class Day 11 Tuesday 18 February\n\n\n\nBlock 4: Precision (cont.)\n\nNew readings\n\nSpiegelhalter Ch. 9 & reading questions\nComputing tutorial 4. Read before class session.\n\nStart work\n\nComputing Activities for Tutorial 4 due 25 Feb\n\nAssignments due:\n\nComputing Activities for Tutorial 3\n\nIn-class notes\n\n\n\n\n\n\n\n\n\nClass Day 12 Friday 21 February\n\n\n\nBlock 4: Precision (cont.)\n\nNew readings\n\nRosling Ch. 7 & reading questions\nSpiegelhalter Ch. 7 & reading questions (This was originally listed for Feb 4. The reading questions are now available.)\n\nStart work\nAssignments due:\n\nSpiegelhalter Ch. 9 post-discussion questions\nCensus project through Phase I Step 5. Your report will consist of starting to transcribe a couple of “person” and “household” rows into the two spreadsheets you created in Step 3. These do not need to be perfect; you may well want to reconfigure them later.\n\nIn-class notes\n\n\n\n\n\n\n\n\n\nClass Day 13 Tuesday 25 February\n\n\n\nBlock 5: Confounding & Causality\n\nNew readings\n\nSpiegelhalter Ch 4 & reading questions\nStart Computing tutorial 5. Read before class session.\n\nStart work\nAssignments due:\n\nComputing Activities for Tutorial 4\n\nIn-class notes\n\n\n\n\n\n\n\n\n\nClass Day 14 Friday 28 February\n\n\n\nBlock 5: Confounding & Causality (cont.)\n\nNew readings\n\nRosling Ch. 8 & reading questions\nComplete Computing tutorial 5. Read before class session.\n\nAssignment due:\n\nCensus project Complete transcription of your population schedule into your household and persons spreadsheets.\n\nStart work:\n\nCensus project Confer with two or three of your classmates to settle on a common format for your household and persons team spreadsheets. For Tuesday 4 March you have those team spreadsheets available (via link) to your instructor.\n\nIn-class notes\n\n\n\n\n\n\n\n\n\nClass Day 15 Tuesday 4 March\n\n\n\nBlock 5: Confounding & Causality (cont.)\n\nNew readings\n\nRosling Ch. 9 & reading questions\n\nStart work\nAssignments due:\n\nRespond to any feedback from the instructor combining the team spreadsheets into data frames that contain all-of-QR2 data. Diligence is required on your part.\n\nIn-class notes\n\n\n\n\n\n\n\n\n\nClass Day 16 Friday 7 March\n\n\n\nBlock 5: Confounding & Causality (cont.)\n\nNew readings\nStart work\n\nComputing Activities for Tutorial 5 (due 11 March)\n\nAssignments due:\n\nContinue to respond to any feedback from the instructor combining the team spreadsheets into data frames that contain all-of-QR2 data. Diligence is required on your part.\n\nIn-class notes\n\n\n\n\n\n\n\n\n\nClass Day 17 Tuesday 11 March\n\n\n\nBlock 6: Hypothetical Reasoning\n\nNew readings\n\nSpiegelhalter Ch. 11 & reading questions\nStart Computing tutorial 3. Read before class session.\n\nStart work\n\nYou should receive at this point the comprehensive, QR2-wide census data. Look through it and decide on the aspect of the data you are going to model.\n\nAssignments due:\n\nComputing Activities for Tutorial 5.\n\nIn-class notes\n\n\n\n\n\n\n\n\n\nClass Day 18 Friday 14 March\n\n\n\nBlock 6: Hypothetical Reasoning (cont.)\n\nNew readings\n\nRosling Ch. 10 & reading questions\nComplete Computing tutorial 6. Read before class session.\n\nAssignments due:\n\nSimple summaries of the all-of-QR2 Census data\nStatement of your “research topic” for your Census modeling project. This will include picking response and explanatory variables, including covariates. DO NOT AGONIZE about this. You are not going to be writing a PhD dissertation.\nRemember that your project presentation is due 21 March.\n\nIn-class notes\n\n\n\n\n\n\n\n\n\nClass Day 19 Tuesday 18 March\n\n\n\nBlock 6: Hypothetical Reasoning (cont.)\n\nNew readings\n\nSpiegelhalter Ch. 13 & reading questions\nSpiegelhalter Ch. 14 & reading questions\n\nStart work\n\nComputing Activities for Tutorial 6. Will be due March 24.\n\nAssignments due:\n\nRosling Ch. 10 post discussion questions\nPreliminary Census written report uploaded to Populi. Basic model summaries and graphics for your Census “research topic.” This will be the basis for your class presentation due on March 21. Formatting can be extremely rough; you will merely be cutting-and-pasting statistical summaries and graphics into an online document that you can reference fro your presentation.\n\nIn-class notes\n\n\n\n\n\n\n\n\n\nClass Day 20 Friday 21 March\n\n\n\n\nNew readings\n\n538 article about p-values\n\nAssignment due:\n\nPhase 4 Census Project presentation\nFor March 24, final Census Project report uploaded to Populi. This will be essentially your preliminary report along with some narrative description of your conclusions and identifying shortcomings that you would have worked on had this been a much more extensive project. It should be neat, but doesn’t need to be fancy! Take perhaps one hour to put it together.\nFor March 24, Computing Activities for Tutorial 3."
  },
  {
    "objectID": "day-by-day/Kaplan-schedule.html#archive-of-past-class-days.",
    "href": "day-by-day/Kaplan-schedule.html#archive-of-past-class-days.",
    "title": "QR2 Prof. Kaplan’s Section",
    "section": "Archive of past class days.",
    "text": "Archive of past class days.\nJanuary: 7, 10, 14, 17, 21, 24, 31\n\nClass Day 1. Tuesday 7 January, 2025\nOrientation to QR2\n\nIn-class notes\nReadings:\n\nSpiegelhalter Introduction Link to a randomly selected seller\nRosling Introduction Link to a randomly selected seller of the book\nNote: As a rule, you should have completed a first pass of the day’s readings before the class meets. This being the first day, I can only ask to to try to do so.\n\nDiscussion:\n\nLearning about data and statistics\nSpiegelhalter Introduction reading questions\nRosling Introduction reading questions\nNote: The items under the “Discussion” header in this document are just FYI. The readings and the reading questions are the appropriate preparation.\n\n\n\n\nClass Day 2. Friday 10 January\nBlock 1: Data, visualization, trends\n\nNew readings:\n\nSpiegelhalter Chap 1 questions\nRosling Chap 1 questions\n\nAssignments due before class time:\n\nBefore-class reading questions as linked above.\nSpiegelhalter introduction follow-up reading questions\n\nDiscussions:\n\nSorting out any start-up problems\nSpiegelhalter and Rosling readings\nIn-class orientation to computing\n\nIn-class notes\n\n\n\nClass Day 3. Tuesday 14 January, 2025\nBlock 1 (cont.)\n\nNew readings:\n\nRosling Chap 2 & reading questions\nComputing tutorial 1. Read before class session.\n\nDiscussions and Activities\n\nRossling reading\nReview of Computing Tutorial 1\nIn-class group work on computing activities (TBA)\n\nAssignments due before class time:\n\nRemember that the “New readings” section has links to the relevant reading questions.\nAs regards the computing tutorials … There are some “reading tasks” with questions that you should answer and submit in the usual way. Equally important, however, are the many R Chunks. You’re expected to play around a bit as you work through the tutorial, trying out different commands, changing names, etc. Make mistakes! That’s a great way to learn. Each time you run an R chunk, that activity is stored in your browser until you close or refresh the page. Submit your work using the button and link at the bottom of the tutorial document. This will let me see what you’ve done, and will be a component of class participation.\nIf you stop part way through a document with questions, chunks, etc., submit your work up to that point. When you return to the document, you can do additional work and submit that. You don’t have to worry about submitting multiple times.\nNote that when you close a document tab, your previous entries are forgotten. But any submissions that you made previously are still safely stored at the submission collection site.\n\nIn-class notes"
  },
  {
    "objectID": "day-by-day/Kaplan-schedule.html#sec-17Jan",
    "href": "day-by-day/Kaplan-schedule.html#sec-17Jan",
    "title": "QR2 Prof. Kaplan’s Section",
    "section": "4. Class Day 4 Friday 17 January",
    "text": "4. Class Day 4 Friday 17 January\nBlock 2: Prediction\n\nNew readings\n\nRosling Chap 4 & reading questions. We’re skipping Rosling 3, saving that for another day.\nSpiegelhalter Chap 3 & reading questions\n\nStart work\n\ncomputing activities for Tutorial 1. Will be due before the next class meeting. You don’t have to do the ones labelled “In-class”. But do as many of the others as you can.\n\nAssignments due:\n\nAs always, reading questions from new readings are due before class.\nThe “computing activities” are due before the class day after the class day where they are listed under “Start work.” The activities for Tutorial 1 are listed for today, so they will be due by class time on Tuesday.\nI’ll stop mentioning this explicitly for future class days.\n\nIn-class notes"
  },
  {
    "objectID": "day-by-day/Kaplan-schedule.html#sec-21Jan",
    "href": "day-by-day/Kaplan-schedule.html#sec-21Jan",
    "title": "QR2 Prof. Kaplan’s Section",
    "section": "Class Day 5 Tuesday 21 January",
    "text": "Class Day 5 Tuesday 21 January\nBlock 2 Prediction (cont.)\n\nNew readings:\n\nSpiegelhalter Ch 6 and reading questions\nStart on Computing tutorial 2. Read before class session.\n\nAssignments due before class:\n\nToday’s new reading questions (as always)\nComputing Activities for Tutorial 1.\n\nIn-class notes"
  },
  {
    "objectID": "day-by-day/Kaplan-schedule.html#sec-24Jan",
    "href": "day-by-day/Kaplan-schedule.html#sec-24Jan",
    "title": "QR2 Prof. Kaplan’s Section",
    "section": "Class Day 6 Friday 24 January",
    "text": "Class Day 6 Friday 24 January\nBlock 2 Prediction (cont.)\n\nNew readings\n\nRosling Ch. 5 & reading questions\nComplete Computing tutorial 2. Read before class session.\n\nStart work\n\nComputing Activities for Tutorial 2. Will be due before class on Day 8 (Tuesday 31 Jan.)\n\nIn-class notes"
  },
  {
    "objectID": "day-by-day/Kaplan-schedule.html#sec-28Jan",
    "href": "day-by-day/Kaplan-schedule.html#sec-28Jan",
    "title": "QR2 Prof. Kaplan’s Section",
    "section": "Class Day 7 Tuesday 28 January",
    "text": "Class Day 7 Tuesday 28 January\nBlock 3 Regression & Adjustment\n\nNew readings\n\nSpiegelhalter Ch. 5 & reading questions\n\nAssignment due: Follow-up reading questions for Spiegelhalter Ch 6\nIn-class notes"
  },
  {
    "objectID": "day-by-day/Kaplan-schedule.html#sec-31Jan",
    "href": "day-by-day/Kaplan-schedule.html#sec-31Jan",
    "title": "QR2 Prof. Kaplan’s Section",
    "section": "Class Day 8 Friday 31 January",
    "text": "Class Day 8 Friday 31 January\nBlock 3 Regression & Adjustment (cont.)\n\nNew readings\n\nRosling Ch. 3 & reading questions\nStart Computing tutorial 3. Read before class session.\n\nAssignments due:\n\nComputing Activities for Tutorial 2\n\nStart work\n\nCensus project (to be announced)\n\nIn-class notes"
  },
  {
    "objectID": "day-by-day/Dailies-Kaplan/2025-01-10-notes.html",
    "href": "day-by-day/Dailies-Kaplan/2025-01-10-notes.html",
    "title": "Kaplan’s in-class notes for 2025-01-10",
    "section": "",
    "text": "Recent article about morning coffee and wellness\n\n“The study found that people who drank coffee in the morning had a lower risk of dying from cardiovascular disease and had a lower mortality risk than all-day coffee consumers—but the research could not prove whether coffee was the sole cause.”\n\n\nWhy look at both “dying from cardiovascular disease” and “mortality.”-\n“Lower risk.” How do you measure risk?\nWhat does “prove” mean in this context? Is proof an appropriate criterion for interpreting the study? What’s its purpose in this paragraph?\n\n\n“Dr Qi said further studies are needed to see if their findings could also be observed in other populations, adding: ‘We need clinical trials to test the potential impact of changing the time of day when people drink coffee.’”\n\n\nWhat is meant by “clinical trials?”\nWhy do they refer to the potential impact of changing the time of day when people drink coffee?\n\n\n“The researchers from Tulane University in New Orleans, looked at 40,725 adults who had taken part in the National Health and Nutrition Examination Survey in the US between 1999 and 2018.”\n\n\nWhat does 40,725—the sample size—tell you here?\n\n\n“The researchers found that morning coffee drinkers were 16% less likely to have died compared to those who did not drink coffee, and 31% less likely to have died from heart disease.”\n\n\nSuggest some mechanisms that might be consistent with this result but not imply that having more people drink coffee in the morning would reduce risk for those people."
  },
  {
    "objectID": "day-by-day/Dailies-Kaplan/2025-01-10-notes.html#critical-thinking-and-qr2",
    "href": "day-by-day/Dailies-Kaplan/2025-01-10-notes.html#critical-thinking-and-qr2",
    "title": "Kaplan’s in-class notes for 2025-01-10",
    "section": "",
    "text": "Recent article about morning coffee and wellness\n\n“The study found that people who drank coffee in the morning had a lower risk of dying from cardiovascular disease and had a lower mortality risk than all-day coffee consumers—but the research could not prove whether coffee was the sole cause.”\n\n\nWhy look at both “dying from cardiovascular disease” and “mortality.”-\n“Lower risk.” How do you measure risk?\nWhat does “prove” mean in this context? Is proof an appropriate criterion for interpreting the study? What’s its purpose in this paragraph?\n\n\n“Dr Qi said further studies are needed to see if their findings could also be observed in other populations, adding: ‘We need clinical trials to test the potential impact of changing the time of day when people drink coffee.’”\n\n\nWhat is meant by “clinical trials?”\nWhy do they refer to the potential impact of changing the time of day when people drink coffee?\n\n\n“The researchers from Tulane University in New Orleans, looked at 40,725 adults who had taken part in the National Health and Nutrition Examination Survey in the US between 1999 and 2018.”\n\n\nWhat does 40,725—the sample size—tell you here?\n\n\n“The researchers found that morning coffee drinkers were 16% less likely to have died compared to those who did not drink coffee, and 31% less likely to have died from heart disease.”\n\n\nSuggest some mechanisms that might be consistent with this result but not imply that having more people drink coffee in the morning would reduce risk for those people."
  },
  {
    "objectID": "day-by-day/Dailies-Kaplan/2025-01-10-notes.html#discussion-rosling-ch.-1",
    "href": "day-by-day/Dailies-Kaplan/2025-01-10-notes.html#discussion-rosling-ch.-1",
    "title": "Kaplan’s in-class notes for 2025-01-10",
    "section": "Discussion: Rosling Ch. 1",
    "text": "Discussion: Rosling Ch. 1\nReading questions\nAvoiding the “gap instinct”:\n\ndon’t look at averages outside of the context of variation."
  },
  {
    "objectID": "day-by-day/Dailies-Kaplan/2025-01-10-notes.html#discussion-spiegelhalter-ch.-1",
    "href": "day-by-day/Dailies-Kaplan/2025-01-10-notes.html#discussion-spiegelhalter-ch.-1",
    "title": "Kaplan’s in-class notes for 2025-01-10",
    "section": "Discussion: Spiegelhalter Ch. 1",
    "text": "Discussion: Spiegelhalter Ch. 1\nTable 1.1: Tabular format of data\nRelative vs absolute risk: bacon sandwich eating. 20% increase in risk. Compare to Fig 1.4\n\nRisk ratio: risk of cancer with bacon divided by without bacon\nCan’t add together relative risks. Instead, multiply them.\n\nExplain (briefly) what odds is about, why we need it.\nReading questions"
  },
  {
    "objectID": "day-by-day/Dailies-Kaplan/2025-01-10-notes.html#introduction-to-computing",
    "href": "day-by-day/Dailies-Kaplan/2025-01-10-notes.html#introduction-to-computing",
    "title": "Kaplan’s in-class notes for 2025-01-10",
    "section": "Introduction to computing",
    "text": "Introduction to computing\nYour window to R computing will be through R-chunks embedded in documents. Each box provides full access to all the capabilities in R, although ours have been set up to provide seamless access only to the tools we’ll need in QR2.\nIn writing documents, I use a different system.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nOur data will be stored in data frames.\n\nEach frame has a name, e.g. Galton and is already available to you in\nOccasionally we will load in data from other sources. You’ll be shown how to do this, but you don’t need to master the methods.\n\n\n\n\nExample: Galton\n\nUsually we don’t want to look at the whole thing in print.\nPipe syntax, function names, parentheses\nnames(), head(), nrow(). Specimens and variables.\nInputs (from pipe) and arguments (in parentheses)\n\nRandom samples. We discussed this on Tuesday. Can take a random sample from a data frame with take_sample(n=...).\nUse wrangling operations to construct new data frames from an existing one. Vocabulary: filter(), mutate(), summarize(), arrange(), select(), grouping (with .by=).\n\nWhat is a database?\nWhat is a “relational” database.\n\nWhat names/words might you have heard that relate to relational databases.\nWhen were they invented.\nHow big a sector of the economy? How does one measure “how big” when talking about the economy.\n\n\nCalculate the amount of variation in a quantitative variable. Vocabulary: variance, var(), “standard deviation.”\nConstruct and interpret annotated point plots from a data frame. Vocabulary: point_plot(), tilde expression, response variable, explanatory variable, covariate, facet, trend, model, mapping, violin.\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page."
  },
  {
    "objectID": "Reading-questions.html",
    "href": "Reading-questions.html",
    "title": "Reading Questions",
    "section": "",
    "text": "For instructors only\n\n\n\nThis document is intended for instructors as they create their day-to-day schedules. Those schedules will provide the student-facing link to reading-question documents.\nEach of the six blocks in QR2 has assigned readings from Spiegelhalter, Rossling, and the computing tutorials. There are corresponding reading questions. These are organized by book chapter. Your instructor will indicate which questions are due and when.\nSome of the reading questions should be completed and submitted before the date scheduled for the classroom discussion. Others are intended as follow up after the discussion. Due dates for the follow-up questions will be specified by individual instructors.\nNote that some documents may be empty."
  },
  {
    "objectID": "Reading-questions.html#block-1-data-visualization-trends",
    "href": "Reading-questions.html#block-1-data-visualization-trends",
    "title": "Reading Questions",
    "section": "Block 1: Data, visualization, trends",
    "text": "Block 1: Data, visualization, trends\n\nSpiegelhalter Introduction: before & follow-up\nSpiegelhalter Chapter 1: before & follow-up\nRossling Chapter 1: before & follow-up\nRossling Chapter 2: before & follow-up\nComputing tutorial 1. Read before class session."
  },
  {
    "objectID": "Reading-questions.html#block-2-prediction",
    "href": "Reading-questions.html#block-2-prediction",
    "title": "Reading Questions",
    "section": "Block 2: Prediction",
    "text": "Block 2: Prediction\n\nSpiegelhalter Chapter 3: before & follow-up\nSpiegelhalter Chapter 6: before & follow-up\nRossling Chapter 4: before & follow-up\nRossling Chapter 5: before & follow-up\nComputing tutorial 2. Read before class session."
  },
  {
    "objectID": "Reading-questions.html#block-3-regression-and-adjustment",
    "href": "Reading-questions.html#block-3-regression-and-adjustment",
    "title": "Reading Questions",
    "section": "Block 3: Regression and Adjustment",
    "text": "Block 3: Regression and Adjustment\n\nSpiegelhalter Chapter 5: before & follow-up\nRossling Chapter 3: before & follow-up\nComputing tutorial 3. Read before class session."
  },
  {
    "objectID": "Reading-questions.html#block-4-precision",
    "href": "Reading-questions.html#block-4-precision",
    "title": "Reading Questions",
    "section": "Block 4: Precision",
    "text": "Block 4: Precision\n\nSpiegelhalter Chapter 8: before & follow-up\nSpiegelhalter Chapter 9: before & follow-up\nRossling Chapter 6: before & follow-up\nRossling Chapter 7: before & follow-up\nComputing tutorial 4. Read before class session."
  },
  {
    "objectID": "Reading-questions.html#block-5-confounding-accuracy-and-adjustment",
    "href": "Reading-questions.html#block-5-confounding-accuracy-and-adjustment",
    "title": "Reading Questions",
    "section": "Block 5: Confounding, accuracy, and adjustment",
    "text": "Block 5: Confounding, accuracy, and adjustment\n\nSpiegelhalter Chapter 4: before & follow-up\nRossling Chapter 8: before & follow-up\nRossling Chapter 9: before & follow-up\nComputing tutorial 5. Read before class session."
  },
  {
    "objectID": "Reading-questions.html#block-6-hypothetical-reasoning",
    "href": "Reading-questions.html#block-6-hypothetical-reasoning",
    "title": "Reading Questions",
    "section": "Block 6: Hypothetical reasoning",
    "text": "Block 6: Hypothetical reasoning\n\nSpiegelhalter Chapter 11: before & follow-up\nSpiegelhalter Chapter 13: before & follow-up\nSpiegelhalter Chapter 14: before & follow-up\nRossling Chapter 10: before & follow-up\nComputing tutorial 6. Read before class session."
  },
  {
    "objectID": "syllabus.html",
    "href": "syllabus.html",
    "title": "QR2 Syllabus and Policies",
    "section": "",
    "text": "Under construction\n\n\n\nDue to last-minute issues, the QR2 Syllabus has not yet been posted on the official “SimpleSyllabus” site. This copy is provided for your convenience. In the case of any discrepancy between this copy and the SimpleSyllabus version, the SimpleSyllabus version has precedence."
  },
  {
    "objectID": "syllabus.html#required-texts",
    "href": "syllabus.html#required-texts",
    "title": "QR2 Syllabus and Policies",
    "section": "Required Texts",
    "text": "Required Texts\n\nDavid Spiegelhalter, “The Art of Statistics: How to Learn from Data” ISBN 978–1541675704\nHans Rosling et al., “Factfulness: Ten Reasons We’re Wrong About the World and Why Things Are Better Than You Think” ISBN 978–1250123824\n\nPre–requisite: INF 1130 (Quantitative Reasoning I)\nBrief description: The second of a two-course sequence in quantitative reasoning."
  },
  {
    "objectID": "syllabus.html#student-learning-outcomes",
    "href": "syllabus.html#student-learning-outcomes",
    "title": "QR2 Syllabus and Policies",
    "section": "Student learning outcomes",
    "text": "Student learning outcomes\nUpon successful completion of this course, students will have demonstrated basic achievement in quantitative reasoning. In particular, students will be able to:\n\nEmploy new data to reinforce or challenge existing beliefs.\nIdentify and describe patterns in data.\nDraw responsible conclusions about causal connections from observed data.\nAssess the utility of predictions.\nAvoiding common data analysis pitfalls and fallacies.\nUse appropriate computational tools to make sense of data.\nUnderstand and use elements of the data science paradigm.\nUse randomization and iteration appropriately in statistical summaries.\nGenerate simulated data in order to test claims about statistical methods.\nDevelop and use a small but creditable technical vocabulary of statistics and data science terms."
  },
  {
    "objectID": "syllabus.html#course-policiesrequirements",
    "href": "syllabus.html#course-policiesrequirements",
    "title": "QR2 Syllabus and Policies",
    "section": "Course Policies/Requirements",
    "text": "Course Policies/Requirements\n\nYour instructor will specify all homework assignments, quizzes, projects, and examinations, along with required completion dates. Late assignments will not be accepted.\nNo makeup exams will be given unless you have a prior excusal or are sick in quarters. It is your responsibility to arrange for a makeup exam if required.\nGrading for this course will be consistent with university policy; specifics will be provided by the course instructor.\nYou are expected to read supporting material before the class session in order to be able to participate actively in discussion.\nCollaboration on homework is encouraged. Document any such collaboration.\nYou may refer to outside sources to assist with homework. Document any such assistance.\nCheating will not be tolerated. In particular, exchanging electronic files containing work you have done for a graded assignment is prohibited unless stated otherwise. Cheating also includes but is not limited to looking at another student’s quiz or exam, copying someone else’s homework, and using unauthorized material during a quiz or exam.\nYou must have a laptop available for use outside of class. Laptops will be used periodically in class at the instructor’s discretion. The use of computing technology is encouraged in support of all work outside of class unless otherwise stated.\nPlease see the Student Handbook for policies regarding support for special needs or disabilities."
  },
  {
    "objectID": "syllabus.html#course-outline",
    "href": "syllabus.html#course-outline",
    "title": "QR2 Syllabus and Policies",
    "section": "Course Outline",
    "text": "Course Outline\nEach of these six topics will occupy a week or two of the course.\n\nData, variation, visualization, and trends\nPrediction and classification in a Bayesian context\nRegression modeling\nPrecision and the limits of data\nAccuracy, confounding and adjustment\nHypothetical reasoning and the scientific method\n\nEach instructor has his or her own daily schedule for the course. They are mainly similar but may differ in important ways such as the due-dates of assignments or the discussion topics for any particular class day. Please refer only to the daily schedule for your own class section. Links to these are on the shared QR2 homepage."
  },
  {
    "objectID": "syllabus.html#attendance-and-tardiness-policy",
    "href": "syllabus.html#attendance-and-tardiness-policy",
    "title": "QR2 Syllabus and Policies",
    "section": "Attendance and Tardiness Policy",
    "text": "Attendance and Tardiness Policy\n\nAttendance is mandatory.\nEach student may miss 10% of classes for any reason, with no excuse needed, and without penalty, i.e., 1, 2, or 3 classes, respectively, in a 1.5, 3, or 4.5 credit course.\nAfter that, each additional absence (for any reason) will result in a 4% or 6% or 12% final grade penalty for a 4.5 or 3.0 or 1.5 credit course, respectively.\nMissing more than 25% of the classes in a course (without medical excuse), including “free” absences, will result in failing the course.\nBeing more than 20 minutes late to a class counts as an unapproved absence."
  },
  {
    "objectID": "syllabus.html#accessibility-statement",
    "href": "syllabus.html#accessibility-statement",
    "title": "QR2 Syllabus and Policies",
    "section": "Accessibility Statement",
    "text": "Accessibility Statement\nPlease review the University Accessibility Statement in the student catalog. Students having special needs should contact the Polaris Center or email Accomodations@uaustin.org.\nDisability Support Services: The university will make reasonable accommodations for students with disabilities in compliance with Section 504 of the Rehabilitation Act and the Americans with Disabilities Act. The purpose of accommodations is to provide equal access to educational opportunities for eligible students with academic and/or physical disabilities."
  },
  {
    "objectID": "syllabus.html#academic-misconduct",
    "href": "syllabus.html#academic-misconduct",
    "title": "QR2 Syllabus and Policies",
    "section": "Academic Misconduct",
    "text": "Academic Misconduct\nInstructors at UATX have the authority to assess possible plagiarism, unauthorized use of artificial intelligence, and other forms of cheating in their courses. Normally, cheating will result in failing the assignment. Students may appeal such decisions to the Disciplinary Council, where they may exercise their right to a public hearing, by writing to the Dean of the Center responsible for the course."
  },
  {
    "objectID": "demonstrations/Narrow-and-wide.html",
    "href": "demonstrations/Narrow-and-wide.html",
    "title": "Wide and narrow data",
    "section": "",
    "text": "Wrangling demonstrations. Cases where the variables are really levels of a variable\n\nSpiegelhalter Fig 2.7\n\n\nAnother population dataset\nFor Fig 2.8 have them wrangle to get the proportion increase and then draw the graph using color for the relative size. Also, … what is the unit of observation? Are all the rows the same kind of unit of observation?"
  },
  {
    "objectID": "questions/S2-after.html",
    "href": "questions/S2-after.html",
    "title": "Spiegelhalter Chapter 2 Follow-up",
    "section": "",
    "text": "Question 1:\n\n\n\nWhich three graphics in Chapter 2 use jittering?\n  question id: S02A-1 \n\n\n\n\n\n\n\n\nQuestion 2:\n\n\n\nIn Figure 2.5, there are three variables presented. Say what they are, which are categorical and which quantitative, and what graphical feature—x, y, facet—each is mapped to. Do you think the choice of the y-mapped variable is sensible as the response variable in a model, that is, which variable can be explained in terms of the other two variables.\n  question id: S02A-2 \n\n\n\nS2-after.rmarkdown Collect your answers\n\nNo answers yet collected\n\n\n\n\n\n\n\nSubmit collected answers here"
  },
  {
    "objectID": "questions/R0-before.html",
    "href": "questions/R0-before.html",
    "title": "Rossling Introduction",
    "section": "",
    "text": "Question 1:\n\n\n\nWhat was your score on the 13-point performance questionnaire? Be honest?\n\n 0        1        2        3        4        5        6        7        8        9        10        11        12        13       \n\nquestion id: rosling-questionnaire\n\n\nWhich questions surprised you the most?\n  question id: SOOB-1 \n\n\n\n\n\n\n\n\nQuestion 2:\n\n\n\nIn what town did Rosling see a former UN Secretary General? Who was it? (You’ll have to do some web searching for the second question.)\n  question id: S0B2-1 \n\n\n\nR0-before.rmarkdown Collect your answers\n\nNo answers yet collected\n\n\n\n\n\n\n\nSubmit collected answers here"
  },
  {
    "objectID": "questions/S4-before.html",
    "href": "questions/S4-before.html",
    "title": "Spiegelhalter Chapter 4",
    "section": "",
    "text": "Question 1:\n\n\n\nIn discussing clinical trials, the book offers eight principles for a good study. List them by name and, for each, say whether or not you understand (or agree!) that the principle is important for drawing conclusions from data. Be honest! You don’t have to agree with David Spiegelhalter. And, insofar as you accept his authority on the matter, it is best to face head on any reasons you might have for skepticism.\n  question id: S04A-1 \n\n\n\n\n\n\n\n\nQuestion 2:\n\n\n\nTraditional statistics course avoid issues of causation, other to warn students not to draw causal conclusions from observational—as opposed to experimental—data. Many of the modern ideas and methods concerning causality stem from the needs of epidemiology. What do you think it is about epidemiology that encourages an emphasis on causality?\n  question id: S04B-2 \n\n\n\n\n\n\n\n\nQuestion 3:\n\n\n\nExplain in your own words what confounding variables, lurking variabiables, and covariates have in common.\n  question id: S04B-3 \n\n\n\n\n\n\n\n\nQuestion 4:\n\n\n\nExplain in your own words what confounding variables, lurking variabiables, and covariates have in common.\n  question id: S04B-4 \n\n\n\nS4-before.rmarkdown Collect your answers\n\nNo answers yet collected\n\n\n\n\n\n\n\nSubmit collected answers here"
  },
  {
    "objectID": "questions/S3-before.html",
    "href": "questions/S3-before.html",
    "title": "Spiegelhalter Chapter 3",
    "section": "",
    "text": "Question 1:\n\n\n\nExplain briefly the difference between inductive and deductive reasoning. Which did you mainly use in mathematics courses?\n  question id: S03-QB1-inductive-deductive \n\n\n\n\n\n\n\n\nQuestion 2:\n\n\n\nWhat distinguishes internal from external validity?\n  question id: S03-QB-internal-external \n\n\n\n\n\n\n\n\nQuestion 3:\n\n\n\nWhat does “proceed” mean in this summary statement from the end of the chapter?\n\nThe best way to proceed from sample to study population is to have drawn a random sample.\n\n  question id: S03-QB3-proceed \n\n\n\n\n\n\n\n\nQuestion 4:\n\n\n\nThe PIDD data frame contains records of 768 women from the Pima Indian population. You can look at the documentation with the command ?PIDD.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nWas random sampling used in collecting this data?\n\n  question id: S03-QB4-sampling \n\nThere are 8 quantitative variables in PIDD: age, pregnancies, glucose, BP, skin_thickness, insulin, bmi, pedigree. For some of these, a numerical value of 0 is used to record *missing data(), that is, values that were not measured or recorded. (This is a poor practice. You can ask why in class.)\n\nSpiegelhalter’s Figure 3.2 is an example of using traditional graphics for distributions. The equivalent we will use in QR2 is a violin plot, that is, a violin annotation on top of a point plot. To show the distribution for a single variable, use ~ 1 as the right-hand side of the tilde expression, as with the following R chunk.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nLook at each of the 8 quantitative variables in PIMA—one at a time—and say whether the distribution of that variable is roughly “normal.” (Ignore the zero values when they refer to missing data.)\n\n  question id: SO3-QB4-normal \n\nThe ninth variable in PIDD is diabetes. Could such a variable ever be normally distributed? Explain your reasoning.\n\n  question id: S03-QB4-diabetes \n\n\n\nS3-before.rmarkdown Collect your answers\n\nNo answers yet collected\n\n\n\n\n\n\n\nSubmit collected answers here"
  },
  {
    "objectID": "questions/R1-after.html",
    "href": "questions/R1-after.html",
    "title": "Rossling Chapter 1 Follow-up",
    "section": "",
    "text": "Just some filler.\n  question id: dove-sees-smoke \n\nR1-after.rmarkdown Collect your answers\n\nNo answers yet collected\n\n\n\n\n\n\n\nSubmit collected answers here"
  },
  {
    "objectID": "questions/S0-after.html",
    "href": "questions/S0-after.html",
    "title": "Spiegelhalter Introduction",
    "section": "",
    "text": "Question 1:\n\n\n\nOn page 11, the reading refers to a “basic bag of [statistical] tools,” which is the focus of what the author claims is an outdated way of teaching about data science. What previous contact have you had with statistical tools that might be in such a “basic bag?”\n  question id: S1-QB3 \n\n\n\nS0-after.rmarkdown Collect your answers\n\nNo answers yet collected\n\n\n\n\n\n\n\nSubmit collected answers here"
  },
  {
    "objectID": "questions/S5-before.html",
    "href": "questions/S5-before.html",
    "title": "Spiegelhalter Chapter 7",
    "section": "",
    "text": "Question 1:\n\n\n\nWhat is the difference between a sample mean and population mean?\n  question id: S07-QB1 \n\n\n\n\n\n\n\n\nQuestion 2:\n\n\n\nAcross different samples from the same population, does the sample mean vary? Does the population mean vary?\n  question id: S07-QB2 \n\n\n\n\n\n\n\n\nQuestion 3:\n\n\n\n\nBriefly explain what “bootstrapping” is and what it is useful for.\n\n  question id: S07-QB3a \n\nBootstrapping is a relatively new technique in statistics, certainly compared to the hundred-year old methods taught in most college introductions as well as AP stats. The method was introduced by Brad Efron in 1979, as an extension of an earlier method called the “jackknife.” (A “jackknife” is a pocket tool useful but not idea for many purposes, as in “Jack of all trades, master of none.”) Another paper by Efron in 1979, published in the journal of the Society of Industrial and Applied Mathematics and the very first statistics paper Prof. Kaplan (an engineer) ever read, aimed to introduce a more general STEM audience to the bookstrap. The title says a lot about how revolutionary the idea is: “Computers and the Theory of Statistics: Thinking the Unthinkable.”\n\nBootstrapping is conceptually simple. Why did it emerge only in the last 40-years as a mainstream statistics method?\n  question id: S07-QB3b \n\n\n\nS5-before.rmarkdown Collect your answers\n\nNo answers yet collected\n\n\n\n\n\n\n\nSubmit collected answers here"
  },
  {
    "objectID": "questions/S0-before.html",
    "href": "questions/S0-before.html",
    "title": "Spiegelhalter Introduction",
    "section": "",
    "text": "Question 1:\n\n\n\nOn page 9, the book doesn’t report the difference between the “low” happiness score for 50-54 year olds compared to the “high” score for the 70-74 age group. Make a guess for what the difference might be, based on your intuition. You don’t have to justify your guess.\n  question id: S1-QB1-1 \nThen, rereading the paragraph carefully to gather what facts are available, say what you think is the smallest difference between these groups that would be worth mentioning.\n  question id: S1-QB1-2 \n\n\n\n\n\n\n\n\nQuestion 2:\n\n\n\nThe reading lists some “necessary component[s]” of being a data scientists. List them, briefly.\n  question id: S1-QB2 \n\n\n\nS0-before.rmarkdown Collect your answers\n\nNo answers yet collected\n\n\n\n\n\n\n\nSubmit collected answers here"
  },
  {
    "objectID": "questions/S1-after.html",
    "href": "questions/S1-after.html",
    "title": "Spiegelhalter Chapter 1 Follow-up",
    "section": "",
    "text": "Just some filler.\n  question id: dove-sees-smoke \n\nS1-after.rmarkdown Collect your answers\n\nNo answers yet collected\n\n\n\n\n\n\n\nSubmit collected answers here"
  },
  {
    "objectID": "questions/R3-before.html",
    "href": "questions/R3-before.html",
    "title": "Rosling Chapter 3: The Straight-line Instinct",
    "section": "",
    "text": "Question 1:\n\n\n\nWhat is the Straight Line Instinct, and how does it influence people’s perception of global trends?\n  question id: R03-QB1 \n\n\n\n\n\n\n\n\nQuestion 2:\n\n\n\nWhat data do Rosling use to support the argument that global population will eventually stop growing?\n  question id: R03-QB2-pop-stabilize \n\n\n\n\n\n\n\n\nQuestion 3:\n\n\n\nIn addition to the Straight Line model, what other curves does Rosling suggest as useful models?\n  question id: R03-QB3-other-curves \n\n\n\n\n\n\n\n\nQuestion 4:\n\n\n\nWhat line or curve relationship in this reading most surprised you?\n  question id: RO3-QB4-surprised \n\n\n\n\n\n\n\n\nQuestion 5:\n\n\n\nWhat argument or assertion in this chapter most motivated you to think of counter-arguments?\n  question id: R03-QB5-counter-arguments \n\n\n\nR3-before.rmarkdown Collect your answers\n\nNo answers yet collected\n\n\n\n\n\n\n\nSubmit collected answers here"
  },
  {
    "objectID": "questions/R6-before.html",
    "href": "questions/R6-before.html",
    "title": "Rossling Chapter 6: The Generalization Instinct",
    "section": "",
    "text": "Question 1:\n\n\n\nWhat is the “generalization instinct,” and why can it be misleading?\n  question id: R06-QB1 \n\n\n\n\n\n\nAnswer\n\n\n\nIt’s our instinct to categorize people or things and assume everything or everyone in that category is similar. While grouping is necessary, it can distort reality by causing us to overlook important differences within a group—or similarities that exist across groups.\n\n\n\n\n\n\n\n\n\n\nQuestion 2:\n\n\n\nWhat mistake did public health authorities make regarding babies’ sleeping positions, and why? What does this reveal about the danger of sweeping generalizations?\n  question id: R06-QB2 \n\n\n\n\n\n\nAnswer\n\n\n\n\nThey incorrectly recommended that infants sleep on their stomachs based on the “recovery position” used to prevent unconscious soldiers from choking on vomit. This advice led to increased infant deaths.\nIt reveals that applying a rule from one group (unconscious soldiers) to a completely different group (sleeping babies) can have life-threatening consequences.\n\n\n\n\n\n\n\n\n\n\n\nQuestion 3:\n\n\n\nAccording to Rosling, what are two practical ways to counteract the generalization instinct?\n  question id: R06-QB3 \n\n\n\n\n\n\nAnswer\n\n\n\nHe advises (1) seeking differences within groups and similarities across groups rather than making blanket judgments, and (2) assuming people are smart rather than labeling their behavior as “strange.” Questioning categories and being aware of vivid but unrepresentative examples are also key strategies.\n\n\n\n\n\nR6-before.rmarkdown Collect your answers\n\nNo answers yet collected\n\n\n\n\n\n\n\nSubmit collected answers here"
  },
  {
    "objectID": "Kaplan_posts/2025-02-07-notes/2025-02-07-notes.html",
    "href": "Kaplan_posts/2025-02-07-notes/2025-02-07-notes.html",
    "title": "Kaplan’s in-class notes for 2025-02-07",
    "section": "",
    "text": "Quiz today (last 15 minutes)\nA famous question\n\n\nLinda is 31 years old, single, outspoken, and very bright. She majored in philosophy. As a student, she was deeply concerned with issues of discrimination and social justice and also participated in anti-nuclear demonstrations. Which is more probable?\n\n\nLinda is a bank teller.\nLinda is a bank teller and is active in the feminist movement."
  },
  {
    "objectID": "questions/S9-before.html",
    "href": "questions/S9-before.html",
    "title": "Spiegelhalter Chapter 9",
    "section": "",
    "text": "Question 0:\n\n\n\n\nComment on the accuracy of the BBC headline, “Threefold Variation in UK Bowel Cancer Rates” (described on p.233).\n\n  question id: S09-QB0a \n\nBriefly explain why such threefold variation would or would not be considered unusual.\n\n  question id: S09-QB0c \n\nBetween two UK cities with populations of 1M persons each, would it be surprising to see threefold variation in bowel cancer rates? Referring to Figure 9.2, briefly explain why or why not.\n\n  question id: S09-QB0c \n\n\n\n\n\n\n\n\nQuestion 1:\n\n\n\nSpiegelhalter describes this chapter (9) as “the most challenging chapter in this book.”\nAssuming that you did indeed find the reading challenging, or rather as a story whose plot line is hard to follow, identify one instance where a statement didn’t seem to you to follow the logic of the preceeding text.\n  question id: S09-QB1 \n\n\n\n\n\n\n\n\nQuestion 2:\n\n\n\nThe key to the chapter lies somewhat hidden in a two-sentence paragraph. I’ll re-write the first sentence of that paragraph, which I think is badly stated in the original. Then I’ll quote the second sentence.\n\nSuppose we draw a random sample of a given size from a known and easily accessible set of specimens. Spiegelhalter calls this a “known population.” We calculate a statistical summary of that sample in the form of a single number, say a model coefficient. We can easily repeat this process many times to see how that single-number summary differs from one sample to the next, thereby collecting the set of numbers needed to describe the “sampling distribution” of that summary.\n\n\n“Of course, this is the wrong way round—we want to use a [single] sample to learn about [an] unknown population—but we can only get to this conclusion by first exploring how a knnown population [can give] rise to [the differing summary numbers from ]different samples.”\n\nExplain how Fig 9.1 relates to thie “first exploring how a known population ….”\n  question id: S09-QB2 \n\n\n\n\n\n\n\n\nQuestion 3:\n\n\n\nThe “funnel plot” (Fig 9.2) is an point plot with more or less ordinary annotations. Think about what the data frame underlying the point plot looks like. What is the unit of observation? What are the variables?\n  question id: S09-QB3 .\n\n\n\n\n\n\n\n\nQuestion 4:\n\n\n\nEach of the dots in the funnel plot amounts to the result from a mini-study where all the death certificates from one year in a geographical district were examined to count how many list bowel cancer as the primary cause of death. Common sense says that there will tend to be more such death certificates in heavily populated districts. The variable actually plotted on the vertical axis is a version of the count adjusted for population size. Explain briefly the simple way the adjustment was done.\n  question id: S09-Q4a \nDid the adjustment for population size eliminate the dependence on the population size of the variable mapped to y? Explain based on what you see in the graph.\n  question id: S09-Q4b \n\n\n\nS9-before.rmarkdown Collect your answers\n\nNo answers yet collected\n\n\n\n\n\n\n\nSubmit collected answers here"
  },
  {
    "objectID": "questions/S10-after.html",
    "href": "questions/S10-after.html",
    "title": "Spiegelhalter Chapter 10",
    "section": "",
    "text": "Question 1:\n\n\n\n\nExplain why the data in Figure 10.1 lead convincingly to the conclusion that “more boys are born than girls.”\n\n  question id: S10-QA1a \n\nExplain why the data in Table 10.1 do not lead convincingly to the conclusion that “females place their right arm on top more than males” when folding their arms.\n\n  question id: S10-QA1b \n\n\n\n\n\n\n\n\nQuestion 2:\n\n\n\nExplain what a p-value is in one sentence.\n  question id: S10-QA2 \n\n\n\n\n\n\n\n\nQuestion 3:\n\n\n\nSummarize how Figure 10.5 addresses the question, “What could go wrong with p-values?”\n  question id: S10-QA3 \n\n\n\nS10-after.rmarkdown Collect your answers\n\nNo answers yet collected\n\n\n\n\n\n\n\nSubmit collected answers here"
  },
  {
    "objectID": "questions/S10-before.html",
    "href": "questions/S10-before.html",
    "title": "Spiegelhalter Chapter 10",
    "section": "",
    "text": "Question 1:\n\n\n\n\nWhat is a “null hypothesis?”\n\n  question id: S10-QB1a \n\nDo the facts gleaned from an experiment lead to proving a null hypothesis, disproving a null hypothesis, or either?\n\n  question id: S10-QB1b \n\n\n\n\n\n\n\n\nQuestion 2:\n\n\n\n\nSuppose 100 trials of an experiment, each with its own sample source, are run to determine the efficacy of some treatment. If the treatment actually has no effect, and each test is run at a 0.05 significance level, how many of the experiments are expected to conclude a “statistically significant” effect?\n\n  question id: S10-QB2a \n\nBriefly explain Type I and Type II error.\n\n  question id: S10-QB2b \n\n\n\nS10-before.rmarkdown Collect your answers\n\nNo answers yet collected\n\n\n\n\n\n\n\nSubmit collected answers here"
  },
  {
    "objectID": "questions/S9-after.html",
    "href": "questions/S9-after.html",
    "title": "Spiegelhalter Chapter 9",
    "section": "",
    "text": "Question 1:\n\n\n\nIn Figure 9.3, what is the expected poll margin of error? Give one reason why the apparent margin of error is different than expected.\n  question id: S09/QA1 \n\n\n\n\n\n\n\n\nQuestion 2:\n\n\n\nWhat is the difference between aleatory and epistemic uncertainty?\n  question id: S09-QA2 \n\n\n\n\n\n\n\n\nQuestion 3:\n\n\n\nRefer to Figure 9.4.\n\nBetween the almost year-long interval April-2014 through March-2015 (second-from-rightmost bar) and the following year (rightmost bar), did the count of homicides in England and Wales increase? Explain what you see in the graph that motivates your conclusion\n\n  question id: S09-QA3a .\n\nWhat can you say based on Fig 9.4 about whether the underlying risk of homicide increased between from the second-from-rightmost bar to the rightmost bar? Which feature of the graph helped you frame your answer?\n\n  question id: S09-QA3b .\n\n\n\nS9-after.rmarkdown Collect your answers\n\nNo answers yet collected\n\n\n\n\n\n\n\nSubmit collected answers here"
  },
  {
    "objectID": "Kaplan_posts/2025-02-18-notes/2025-02-18-notes.html#precision",
    "href": "Kaplan_posts/2025-02-18-notes/2025-02-18-notes.html#precision",
    "title": "Kaplan’s in-class notes for 2025-02-18",
    "section": "",
    "text": "Look at Quiz Question 4 with an eye toward the confidence interval rather than just the coefficient.\nQUIZ QUESTION 4: We’re going to make some models of lung cancer in the Birdkeepers, looking at smoking and age as risk factors for cancer. CD is the number of cigarettes smoked per day (0 for a non-smoker). YR is the number of years that the person has smoked (0 for a non-smoker), and AG is the persons age. cancer is a binary variable with value 1 indicating cancer and 0 indicating otherwise. datawith the same response variable.\n\nBirdkeepers |&gt; model_train(cancer ~ CD) |&gt; \n  conf_interval()\n\n# A tibble: 2 × 4\n  term           .lwr   .coef    .upr\n  &lt;chr&gt;         &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n1 (Intercept) -2.32   -1.54   -0.829 \n2 CD           0.0143  0.0511  0.0909\n\n\n\nInterpret the output. Does the CD coefficient of 0.05 mean that smoking an additional cigarette per day increases or decreases or doesn’t affect the odds of getting lung cancer?\n\nSomeone claims that the coefficient 0.05 is misleading and that what really matters is how many years the person has been smoking.\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\n\nBirdkeepers |&gt; model_train(cancer ~ CD + YR) |&gt; \n  conf_interval()\n\n# A tibble: 3 × 4\n  term           .lwr   .coef    .upr\n  &lt;chr&gt;         &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n1 (Intercept) -3.70   -2.46   -1.43  \n2 CD          -0.0211  0.0238  0.0689\n3 YR           0.0129  0.0460  0.0816\n\n\n\nInterpret the coefficients 0.023 and 0.046 from the model. Does taking into account the years of smoking increase, decrease, or leave alone the effect of smoking an additional cigarette per day.\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\n\nAccording to the cancer ~ CD + YR model, if a person could reduce their smoking consumption by 20 cigarettes per day, how many more years could they keep on smoking to arrive at the same risk of lung cancer?\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\nYou realize that anyone who smokes an extra year must also be an extra year older. Age increases the risk of illness so you theorize that some of the credit being given to years of smoking is really about “years of living.” So you try a third model …\n\nBirdkeepers |&gt; \n  model_train(cancer ~ CD + YR + AG) |&gt; \n  conf_interval()\n\n# A tibble: 4 × 4\n  term           .lwr   .coef     .upr\n  &lt;chr&gt;         &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept) -2.73    0.373  3.45    \n2 CD          -0.0306  0.0163 0.0630  \n3 YR           0.0276  0.0719 0.126   \n4 AG          -0.131  -0.0619 0.000984\n\n\n\nWe have a technical word for an explanatory variable introduced to a model to place another explanatory variable in context. What is that word?\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\n\nDo the coefficients of the cancer ~ CD + YR + AG model supportt the hypothesis that some of the effect of years of smoking is really attributable to ordinary aging? Explain briefly.\n\n\n\n\n\n\n\nAnswer"
  },
  {
    "objectID": "Kaplan_posts/2025-02-18-notes/2025-02-18-notes.html#a-look-at-the-census-project",
    "href": "Kaplan_posts/2025-02-18-notes/2025-02-18-notes.html#a-look-at-the-census-project",
    "title": "Kaplan’s in-class notes for 2025-02-18",
    "section": "A look at the Census project",
    "text": "A look at the Census project\nPopulation Schedule registration sheet\nBring up these in separate tabs:\n\nPopulation schedule thumbnail\n\nhow to download\nhow to display so that you can transcribe\n\nTranscribed “persons” sheet\n\nHow will we know what household a person belongs to?\nDon’t want to have any line depend on information in any other line. So the dashes must go, as must “Same Place”\nUnder relationship, this census taker has listed the relationship to the institution. Maybe we need an institution table as well. (This project is too small for that, I think.)\nNotice the pencilled-in notations “Head”. The sheet lists the profession but doesn’t make clear that the person is the head of a household. On the raw sheets, there is a pencilled-in * or + or x to indicate that the person is the head of a household.\n\nStub of a “household” sheet.\n\nHow can we provide an “anchor” so that the person can refer to a household?\nQuestion: Is family name an attribute of a household or a person? Can different people in the same household have different family names?\nDo we want to put the number of people in the household here, or calculate it from other data.\n\nPerhaps, Eitan Zarin’s persons and households."
  },
  {
    "objectID": "Kaplan_posts/2025-02-18-notes/2025-02-18-notes.html#precision-1",
    "href": "Kaplan_posts/2025-02-18-notes/2025-02-18-notes.html#precision-1",
    "title": "Kaplan’s in-class notes for 2025-02-18",
    "section": "Precision",
    "text": "Precision\n\nIn groups of 3 or 4 people, discuss for 3 minutes the distinction, if any, between “precision” and “accuracy”. Then we’ll report back to the class.\nDiscuss Figure 9.2 from Spiegelhalter, the funnel plot. Discuss what’s going on … we have some standard theories for why the colon-cancer death rate might be different from district to district: different diets, different age structures, environmental toxins, etc. In formulating an explanation for the district-to-district variation in death rate, we need to account for an inevitable factor that stems from mathematics and not biological pathology: the variation induced by sample size.\nConstruct the Linda simulation: Randomly picking 12 cases out of a population in which 92% of the cases turned out heads, with 8% turning out tails. Calculate variance\n\n\nsample_size &lt;- 12\ntibble::tibble(\n  answer = \n    take_sample(\n      c(rep(1,11),0), \n      n = sample_size, \n      replace=TRUE) # Change to TRUE \n) |&gt;\n  summarize(frac = mean(answer)) # |&gt; trials(50) \n\n# A tibble: 1 × 1\n   frac\n  &lt;dbl&gt;\n1 0.833\n\n# |&gt; summarize(center = mean(frac), samp_variance = var(frac))\n\n\nRedo the simulation but with a larger sample size."
  },
  {
    "objectID": "Kaplan_posts/2025-02-18-notes/2025-02-18-notes.html#quiz",
    "href": "Kaplan_posts/2025-02-18-notes/2025-02-18-notes.html#quiz",
    "title": "Kaplan’s in-class notes for 2025-02-18",
    "section": "Quiz",
    "text": "Quiz\nExample: Look at Quiz problems 3\nQUESTION 3: The data (from Birdkeepers) comes from a study of birdkeeping and lung cancer. Here is a simple tabulation of the number of people in Birdkeepers who have lung cancer (LC) and who are birdkeepers (BK).\n\nBirdkeepers |&gt; summarize(n(), .by = c(LC, BK))\n\n          LC     BK n()\n1 LungCancer   Bird  33\n2 LungCancer NoBird  16\n3   NoCancer NoBird  64\n4   NoCancer   Bird  34\n\n\n\nUsing birdkeeping as a test for lung cancer, say what level of BK should correspond to a positive test result, then calculate the sensitivity, specificity, and prevalence, risk ratio, and odds ratio for LungCancer.\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\n\nThe prevalence of cancer in the study group is very high: one-third. This is intentional: one third (49) of the subjects were being treated for lung cancer, each was matched up with two controls with similar ages. How does the overly high prevalence change the interpretation of the sensitivity and specificity of the test.\n\n\n\n\n\n\n\nAnswer"
  },
  {
    "objectID": "questions/R4-before.html",
    "href": "questions/R4-before.html",
    "title": "Rosling Chapter 3: The Straight-line Instinct",
    "section": "",
    "text": "Question 1:\n\n\n\nAccording to the text, why do news stories about airplane crashes receive so much attention? How does that attention compare with the global trend in aviation safety?\n  question id: R04-QB1 \n\n\n\n\n\n\nAnswer\n\n\n\n\nCrashes trigger strong fear instincts (physical harm, captivity), making them especially dramatic for audiences and thus favored by media outlets seeking attention.\nDespite the media focus, air travel has become extraordinarily safe, with crash-related deaths dropping dramatically over the past 70 years thanks to international collaborations and shared safety regulations.\n\n\n\n\n\n\n\n\n\n\n\nQuestion 2:\n\n\n\nWhat is the “fear instinct,” as described by Hans Rosling? Name one example from the chapter where fear distorted people’s perception of actual risk.\n  question id: R04-QB2 \n\n\n\n\n\n\nAnswer\n\n\n\n\nThe “fear instinct” is our hardwired tendency to notice and react to frightening information, which can overshadow rational decision-making.\nA key example is the Fukushima nuclear plant incident, where the fear of radiation led to the rapid evacuation of elderly residents. In fact, the evacuation stress killed many of them, yet no one died directly from the radioactive leak itself.\n\n\n\n\n\n\n\n\n\n\n\nQuestion 3:\n\n\n\nWhat advice does the author give for coping with the fear instinct and making more fact-based judgments?\n  question id: R04-QB3 \n\n\n\n\n\n\nAnswer\n\n\n\nHe advises recognizing when frightening information catches our attention and remembering that it isn’t always the most dangerous threat. He suggests measuring actual risks (danger & exposure) and pausing until fear subsides before making decisions—ensuring our resources and efforts focus on genuinely high-risk problems rather than merely scary ones.\n\n\n\n\n\nR4-before.rmarkdown Collect your answers\n\nNo answers yet collected\n\n\n\n\n\n\n\nSubmit collected answers here"
  },
  {
    "objectID": "questions/R7-before.html",
    "href": "questions/R7-before.html",
    "title": "Rossling Chapter 7: The Destiny Instinct",
    "section": "",
    "text": "Question 1:\n\n\n\nWhat is the “destiny instinct,” and how can it distort our view of global development?\n  question id: R07-QB1 \n\n\n\n\n\n\nTip\n\n\n\nThe destiny instinct is the notion that certain characteristics—cultural or otherwise—are fixed and unchanging, leading us to believe that some countries or groups will “always” stay poor or “always” behave a certain way. This obscures real progress taking place.\n\n\n\n\n\n\n\n\n\n\nQuestion 2:\n\n\n\nAccording to the text, how do data on children per woman in Iran challenge common Western assumptions about cultural “destiny”?\n  question id: R07-QB2 \n\n\n\n\n\n\nTip\n\n\n\nIran experienced the fastest drop in fertility rates ever recorded, down from over six babies per woman to fewer than three within 15 years. This drastic change contradicts the idea that Iran’s culture or religion fated it to large families indefinitely.\n\n\n\n\n\n\n\n\n\n\nQuestion 3:\n\n\n\nWhat practical steps does Rosling propose to counteract the destiny instinct?\n  question id: R07-QB3 \n\n\n\n\n\n\nTip\n\n\n\nHe recommends recognizing that slow changes are still changes; keeping knowledge updated by reviewing new data; reflecting on generational shifts (e.g., talking to older relatives); and collecting concrete examples of cultural transformation to challenge the idea of fixed destinies.\n\n\n\n\n\nR7-before.rmarkdown Collect your answers\n\nNo answers yet collected\n\n\n\n\n\n\n\nSubmit collected answers here"
  },
  {
    "objectID": "questions/R8-before.html",
    "href": "questions/R8-before.html",
    "title": "Rossling Chapter 8: The Single Perspective Instinct",
    "section": "",
    "text": "Question 1:\n\n\n\nWhat is the “single perspective instinct,” and why is it potentially dangerous?\n  question id: R08-QB1 \n\n\n\n\n\n\nAnswer\n\n\n\nIt’s the tendency to reduce complex issues to a single idea or explanation, believing that one cause or solution can address multiple diverse problems. This oversimplification blinds us to other valid perspectives and can lead to poor decisions.\n\n\n\n\n\n\n\n\n\n\nQuestion 2:\n\n\n\nHow does Cuba exemplify the dangers of relying on a single idea (government planning)? What contrasting problem does the United States highlight regarding single-perspective thinking (free markets)?\n  question id: R08-QB2 \n\n\n\n\n\n\nAnswer\n\n\n\n\nCuba has high life expectancy but remains economically poor and restrictive. It demonstrates that heavy government planning can provide good public health results while limiting other freedoms and wealth.\nThe United States, by contrast, spends more per capita on healthcare than any other country yet sees poorer health outcomes than other wealthy nations—showing that free markets alone can also fail to deliver holistic benefits.\n\n\n\n\n\n\n\n\n\n\n\nQuestion 3:\n\n\n\nWhat strategies does Rosling suggest to counteract the single perspective instinct?\n  question id: R08-QB3 \n\n\n\n\n\n\nAnswer\n\n\n\nHe recommends testing ideas with people who disagree, recognizing the limits of one’s expertise, remembering that no single solution fits every problem, and combining multiple approaches rather than clinging to just one. In short, keep a “toolbox,” not just a single “hammer.”\n\n\n\n\n\nR8-before.rmarkdown Collect your answers\n\nNo answers yet collected\n\n\n\n\n\n\n\nSubmit collected answers here"
  },
  {
    "objectID": "questions/S7-before.html",
    "href": "questions/S7-before.html",
    "title": "Spiegelhalter Chapter 7",
    "section": "",
    "text": "Question 1:\n\n\n\nWhat is the difference between a sample mean and population mean?\n  question id: S07-QB1 \n\n\n\n\n\n\n\n\nQuestion 2:\n\n\n\nAcross different samples from the same population, does the sample mean vary? Does the population mean vary?\n  question id: S07-QB2 \n\n\n\n\n\n\n\n\nQuestion 3:\n\n\n\n\nBriefly explain what “bootstrapping” is and what it is useful for.\n\n  question id: S07-QB3a \n\nBootstrapping is a relatively new technique in statistics, certainly compared to the hundred-year old methods taught in most college introductions as well as AP stats. The method was introduced by Brad Efron in 1979, as an extension of an earlier method called the “jackknife.” (A “jackknife” is a pocket tool useful but not idea for many purposes, as in “Jack of all trades, master of none.”) Another paper by Efron in 1979, published in the journal of the Society of Industrial and Applied Mathematics and the very first statistics paper Prof. Kaplan (an engineer) ever read, aimed to introduce a more general STEM audience to the bookstrap. The title says a lot about how revolutionary the idea is: “Computers and the Theory of Statistics: Thinking the Unthinkable.”\n\nBootstrapping is conceptually simple. Why did it emerge only in the last 40-years as a mainstream statistics method?\n  question id: S07-QB3b \n\n\n\nS7-before.rmarkdown Collect your answers\n\nNo answers yet collected\n\n\n\n\n\n\n\nSubmit collected answers here"
  },
  {
    "objectID": "day-by-day/Kaplan-schedule.html#class-day-10-friday-7-febuary",
    "href": "day-by-day/Kaplan-schedule.html#class-day-10-friday-7-febuary",
    "title": "QR2 Prof. Kaplan’s Section",
    "section": "Class Day 10 Friday 7 Febuary",
    "text": "Class Day 10 Friday 7 Febuary\nBlock 4: Precision\n\nNew readings\n\nSpiegelhalter Ch. 8 & reading questions\nRosling Ch. 6 & reading questions\n\nStart work\nAssignments due:\n\nCensus project report through Step 3. Your report takes the form of your entry into the Population Schedule Registry.\n\nIn-class notes"
  },
  {
    "objectID": "day-by-day/Kaplan-schedule.html#class-day-9-tuesday-4-february",
    "href": "day-by-day/Kaplan-schedule.html#class-day-9-tuesday-4-february",
    "title": "QR2 Prof. Kaplan’s Section",
    "section": "Class Day 9 Tuesday 4 February",
    "text": "Class Day 9 Tuesday 4 February\nBlock 3 Regression & Adjustment (cont)\n\nNew readings\n\nFinish Computing tutorial 3. Read before class session.\nSpiegelhalter Ch. 7 & reading questions\n\nStart work\n\nComputing Activities for Tutorial 3 due at end of reading week\nCensus project Phase 1\n\nIn-class notes\n\nReading break 10-14 February."
  },
  {
    "objectID": "Kaplan_posts/2025-02-21-notes/2025-02-21-notes.html#review-of-quiz-from-feb-7",
    "href": "Kaplan_posts/2025-02-21-notes/2025-02-21-notes.html#review-of-quiz-from-feb-7",
    "title": "Kaplan’s in-class notes for 2025-02-21",
    "section": "",
    "text": "QUESTION 1: The following lines of code\n\nModel_1 &lt;- Galton |&gt;\n  model_train(height ~ mother + father + sex)\nModel_1 |&gt;\n  model_eval(sex = \"M\", mother = 70, father = 66)\n\nlead to this printed output\nsex  mother   father    .lwr   .output    .upr\nM        70       66    65.6    69.9      74.1\n\nPrediction interval bounds are not hard limits. They are intended to indicate an interval that contains 95% of the actual inputs.\n\n\nIn the output, there are some labels, a categorical level, and five numbers. Briefly explain what each of the five numbers means.\nYou can’t see the model coefficients from this output. Which R function should you pipe Model_1 into in order to read the coefficients?\n\n\nI’m seeing summary(), coef() and such which are not in the vocabulary we are using in the tutorial. Where are you hearing about these? A web search?\n\n\nShow whether the following coefficients are roughly consistent with the above output from model_eval(sex = \"M\", mother = 70, father = 66). Explain your reasoning.\n\n  term         .lwr  .coef   .upr\n  &lt;chr&gt;       &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 (Intercept) 9.95    15.0   20.7  \n2 mother      0.260    0.6  0.683\n3 father      0.349    0.5  0.863\n4 sexM        0.523    2.0   5.51 \nQUESTION 2: Bmod is the name of a model trained on a data frame named Buildings. You have never seen the Buildings data, nor do you know what response and explanatory variables were used in Bmod. Even so, you should be able to say which if any of the outputs A, B, and C below is a possible result from the following command. If the output is not possible, briefly explain why.\nBMod |&gt; \n  model_eval(data = Buildings) |&gt;\n  summarize(mean(.resid), \n            sd(.resid), \n            var(.resid))\nOutput A\n   mean(.resid) sd(.resid) var(.resid)\n1       1.4e-14         12         144\n\n“A is not possible because of the very low mean considering its large variance” - The mean does not constrain the variance. - Mean of residuals will always be zero (in-sample)\n\nOutput B\n   mean(.resid) sd(.resid) var(.resid)\n1             0         12         189\nOutput C\n   mean(.resid) sd(.resid) var(.resid)\n1             1         12         144\nQUESTION 3: The data (from Birdkeepers) comes from a study of birdkeeping and lung cancer. Here is a simple tabulation of the number of people in Birdkeepers who have lung cancer (LC) and who are birdkeepers (BK).\n\nBirdkeepers |&gt; summarize(n(), .by = c(LC, BK))\n\n          LC     BK n()\n1 LungCancer   Bird  33\n2 LungCancer NoBird  16\n3   NoCancer NoBird  64\n4   NoCancer   Bird  34\n\n\n\nUsing birdkeeping as a test for lung cancer, say what level of BK should correspond to a positive test result, then calculate the sensitivity, specificity, and prevalence, risk ratio, and odds ratio for LungCancer.\n\n\nPeople were good at this, but sometimes got mixed up between the actual state and the outcome of the do-you-own-a-bird test.\n\n\nThe prevalence of cancer in the study group is very high: one-third. This is intentional: one third (49) of the subjects were being treated for lung cancer, each was matched up with two controls with similar ages. How does the overly high prevalence change the interpretation of the sensitivity and specificity of the test.\n\n\nSome people suggested that high prevalence indicates that study is misleading (external validity) and somehow more random or untrustworthy than usual. But the high prevalence is due to the design of the study, not the result of sampling variation."
  },
  {
    "objectID": "Kaplan_posts/2025-02-21-notes/2025-02-21-notes.html#precision-with-confidence-intervals",
    "href": "Kaplan_posts/2025-02-21-notes/2025-02-21-notes.html#precision-with-confidence-intervals",
    "title": "Kaplan’s in-class notes for 2025-02-21",
    "section": "Precision with confidence intervals",
    "text": "Precision with confidence intervals\nHere is the raw bowel-cancer data from Spiegelhalter Fig. 9.2\n\nBowel_cancer_UK &lt;- \n  suppressMessages(readr::read_csv(\"09-2-bowel-cancer-data-x.csv\", )) |&gt;\n  mutate(rate_per_100000 = (n/d)*100000)\nBowel_cancer_UK |&gt; nrow()\n\n[1] 379\n\nBowel_cancer_UK |&gt; head()\n\n# A tibble: 6 × 5\n  Country          District            n      d rate_per_100000\n  &lt;chr&gt;            &lt;chr&gt;           &lt;dbl&gt;  &lt;dbl&gt;           &lt;dbl&gt;\n1 Scotland         Aberdeen City      63 286755            22.0\n2 Scotland         Aberdeenshire      65 358324            18.1\n3 Scotland         Angus              35 205399            17.0\n4 Northern Ireland Antrim             12  56953            21.1\n5 Northern Ireland Ards               20 102145            19.6\n6 Scotland         Argyll and Bute    39 157449            24.8\n\nme &lt;- function(pop, multiplier = 1.96) { \n  multiplier* 100000*sqrt(0.000176*(1-0.000176)/pop)\n}\ncurves &lt;- tibble::tibble(\n  pop = 10^seq(4.5, 6.2, length=200),\n  t95 = 17.6 + me(pop, 1.96),\n  b95 = 17.6 - me(pop, 1.96),\n  t99 = 17.6 + me(pop, 2.58),\n  b99 = 17.6 - me(pop, 2.58),\n)\nBowel_cancer_UK |&gt;\n  gf_point(rate_per_100000 ~ d, size=.5) |&gt;\n  gf_ribbon(t99 + b99 ~ pop, data = curves, \n            fill = \"blue\", alpha = 0.1, inherit=FALSE) |&gt;\n  gf_ribbon(t95 + b95 ~ pop, data = curves, \n            fill = \"green\", alpha = 0.2, inherit=FALSE)"
  },
  {
    "objectID": "Kaplan_posts/2025-02-21-notes/2025-02-21-notes.html#simulation-of-funnel-plot",
    "href": "Kaplan_posts/2025-02-21-notes/2025-02-21-notes.html#simulation-of-funnel-plot",
    "title": "Kaplan’s in-class notes for 2025-02-21",
    "section": "Simulation of funnel plot",
    "text": "Simulation of funnel plot\n\nPlease enable JavaScript to experience the dynamic code cell content on this page."
  },
  {
    "objectID": "Kaplan_posts/2025-02-21-notes/2025-02-21-notes.html#simulation",
    "href": "Kaplan_posts/2025-02-21-notes/2025-02-21-notes.html#simulation",
    "title": "Kaplan’s in-class notes for 2025-02-21",
    "section": "Simulation",
    "text": "Simulation\nMake a system with known coefficients and different amounts of randomness.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page."
  },
  {
    "objectID": "Kaplan_posts/2025-02-21-notes/2025-02-21-notes.html#take-new-random-samples-from-a-big-set",
    "href": "Kaplan_posts/2025-02-21-notes/2025-02-21-notes.html#take-new-random-samples-from-a-big-set",
    "title": "Kaplan’s in-class notes for 2025-02-21",
    "section": "Take new random samples from a big set",
    "text": "Take new random samples from a big set\n\nSamples &lt;- Natality_2014 |&gt; \n  mutate(sex = zero_one(sex, one = \"F\")) |&gt; \n  take_sample(n=100) |&gt;\n  model_train(sex ~ mager) |&gt;\n  conf_interval() |&gt;\n  trials(100) \nSamples |&gt; \n  filter(term == \"mager\") |&gt;\n  summarize(mager = mean(.coef), sv = var(.coef), se = sqrt(sv))\n\n        mager         sv         se\n1 0.008401271 0.00114872 0.03389277\n\nSamples |&gt;\n  filter(term == \"mager\") |&gt;\n  gf_errorbar(.lwr + .upr ~ .trial) |&gt;\n  gf_point(.coef ~ .trial, color = \"red\")\n\n\n\n\n\n\n\n\nDemonstrate the \\(1/\\sqrt{n}\\) dependence of SE on sample size (or \\(1/n\\) for sampling variance).\nDerive standard error of the mean by algebra."
  },
  {
    "objectID": "Kaplan_posts/2025-02-21-notes/2025-02-21-notes.html#resample",
    "href": "Kaplan_posts/2025-02-21-notes/2025-02-21-notes.html#resample",
    "title": "Kaplan’s in-class notes for 2025-02-21",
    "section": "Resample",
    "text": "Resample\n\nOur_sample &lt;- Natality_2014 |&gt; \n  mutate(sex = zero_one(sex, one = \"F\")) |&gt; \n  take_sample(n=1000)\n\nResamples &lt;- Our_sample |&gt; resample() |&gt;\n  model_train(sex ~ mager) |&gt;\n  conf_interval() |&gt;\n  trials(100)\n\nResamples |&gt;\n  filter(term == \"mager\") |&gt;\n  summarize(mean(.coef), var(.coef), sd(.coef))\n\n  mean(.coef)   var(.coef) sd(.coef)\n1  0.01587188 0.0001346133 0.0116023\n\nResamples |&gt;\n  filter(term == \"mager\") |&gt;\n  gf_errorbar(.lwr + .upr ~ .trial) |&gt;\n  gf_point(.coef ~ .trial, color = \"red\")"
  },
  {
    "objectID": "Kaplan_posts/2025-02-21-notes/2025-02-21-notes.html#just-rely-on-model_train-and-conf_interval",
    "href": "Kaplan_posts/2025-02-21-notes/2025-02-21-notes.html#just-rely-on-model_train-and-conf_interval",
    "title": "Kaplan’s in-class notes for 2025-02-21",
    "section": "Just rely on model_train() and conf_interval()",
    "text": "Just rely on model_train() and conf_interval()\n\nNatality_2014 |&gt; \n  mutate(sex = zero_one(sex, one = \"F\")) |&gt; \n  take_sample(n = 100) |&gt;\n  model_train(sex ~ mager) |&gt;\n  conf_interval()\n\nWaiting for profiling to be done...\n\n\n# A tibble: 2 × 4\n  term           .lwr   .coef   .upr\n  &lt;chr&gt;         &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;\n1 (Intercept) -1.16    0.694  2.59  \n2 mager       -0.0953 -0.0270 0.0398"
  }
]